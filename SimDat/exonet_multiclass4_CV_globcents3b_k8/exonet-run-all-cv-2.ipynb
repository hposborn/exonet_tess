{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This run is with centroids as layer to local only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### python packages\n",
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "from random import random\n",
    "import pandas as pd\n",
    "\n",
    "### torch packages\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "### sklearn packages\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "### remove these later (for notebook version only)\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "'''\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show, export_png\n",
    "from bokeh.layouts import row\n",
    "output_notebook()\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set manual seed\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KeplerDataLoaderCrossVal(Dataset):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    PURPOSE: DATA LOADER FOR KERPLER LIGHT CURVES\n",
    "    INPUT: PATH TO DIRECTOR WITH LIGHT CURVES + INFO FILES\n",
    "    OUTPUT: LOCAL + GLOBAL VIEWS, LABELS\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, infofiles):\n",
    "        ### list of global, local, and info files (assumes certain names of files)\n",
    "        self.flist_global,self.flist_local=[],[]\n",
    "        for i,val in enumerate(infofiles):\n",
    "            self.flist_global.append(val.replace('_info2.npy','_glob.npy'))\n",
    "            self.flist_local.append(val.replace('_info2.npy','_loc.npy'))\n",
    "        self.flist_info = infofiles\n",
    "        \n",
    "        ### list of whitened centroid files\n",
    "        #self.flist_global_cen = np.sort(glob.glob(os.path.join(filepath, '*global_cen_w.npy')))\n",
    "        #self.flist_local_cen = np.sort(glob.glob(os.path.join(filepath, '*local_cen_w.npy')))\n",
    "        \n",
    "        ### ids = {TIC}_{TCE}\n",
    "        self.ids = np.sort(['_'.join(x.split('/')[-1].split('_')[1:3]) for x in self.flist_global])\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.ids.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        ### grab local and global views\n",
    "        data_global = np.nan_to_num(np.load(self.flist_global[idx],encoding='latin1'))\n",
    "        data_local = np.nan_to_num(np.load(self.flist_local[idx],encoding='latin1'))\n",
    "\n",
    "        ### grab centroid views\n",
    "        data_global_cen = data_global[:,1]\n",
    "        data_local_cen = data_local[:,1]\n",
    "        \n",
    "        data_global = data_global[:,0]\n",
    "        data_local = data_local[:,0]\n",
    "        \n",
    "        ### info file contains: [0]kic, [1]tce, [2]period, [3]epoch, [4]duration, [5]label)\n",
    "        data_info = np.load(self.flist_info[idx],encoding='latin1')\n",
    "        #np.load(self.flist_info[idx],encoding='latin1')\n",
    "        \n",
    "        if data_info[6]=='PL':\n",
    "            label=1\n",
    "        elif data_info[6]=='UNK':\n",
    "            label=0\n",
    "        elif data_info[6]=='EB':\n",
    "            label=2\n",
    "        else:\n",
    "            label=3\n",
    "        \n",
    "        #collist=['TPERIOD','TDUR','DRRATIO','NTRANS','TSNR','TDEPTH','INDUR',\n",
    "        #         'SESMES_LOG_RATIO','PRAD_LOG_RATIO','TDUR_LOG_RATIO','RADRATIO','IMPACT',\n",
    "        #         'TESSMAG','RADIUS','PMTOTAL','LOGG','MH','TEFF']#from bls search, derived from transit model, from starpars\n",
    "        stelpars=np.nan_to_num(np.hstack((data_info[7:13].astype(float),data_info[-18:-6].astype(float))))\n",
    "        \n",
    "        return (data_local.astype(float), data_global.astype(float), data_local_cen.astype(float), data_global_cen.astype(float), stelpars), label\n",
    "    \n",
    "class BalancedBatchSampler(Sampler):\n",
    "    \"\"\"Wraps another sampler to yield a class-balanced mini-batch of indices.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Size of mini-batch.\n",
    "        label_index (int): The index of the label in the dataset returned tuple\n",
    "        shuffle (bool): Whether to shuffle every iteration or not.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, dataset, n_classes, label_index=1, shuffle=True, stats_file=None):\n",
    "        self.n_classes = n_classes\n",
    "        self.dataset = dataset\n",
    "        self.label_index = label_index\n",
    "        self.batch_size = batch_size\n",
    "        self.class_indices = []\n",
    "        self.class_iteration_index = [0]*self.n_classes\n",
    "        self.max_class_length = 0\n",
    "        self.max_class = 0\n",
    "        self.per_class_batchsize = self.batch_size // self.n_classes\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            self.class_indices.append([])\n",
    "\n",
    "        if stats_file is not None and os.path.exists(stats_file):\n",
    "            with open(stats_file, 'rb') as stats_f:\n",
    "                self.class_indices = pickle.load(stats_f)\n",
    "            print('Loaded dataset class distribution from: {}'.format(stats_file))\n",
    "        else:\n",
    "            print('Accumulating dataset class distribution...')\n",
    "            for i in np.arange(len(self.dataset)):\n",
    "                label = int(self.dataset[i][self.label_index])\n",
    "                self.class_indices[label].append(i)\n",
    "            if stats_file is not None:\n",
    "                with open(stats_file, 'wb') as stats_f:\n",
    "                    pickle.dump(self.class_indices, stats_f)\n",
    "                print('Saved dataset class distribution to: {}'.format(stats_file))\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            if self.shuffle:\n",
    "                self.class_indices[i] = list(np.random.permutation(self.class_indices[i]))\n",
    "\n",
    "            if len(self.class_indices) > self.max_class_length:\n",
    "                self.max_class_length = len(self.class_indices[i])\n",
    "                self.max_class = i\n",
    "        \n",
    "        print('Balancing dataset with class distribution: {}'.format([len(class_ind) for class_ind in self.class_indices]))\n",
    "\n",
    "    def __iter__(self):\n",
    "        # only generate full batches\n",
    "        while self.class_iteration_index[self.max_class] + self.per_class_batchsize  < self.max_class_length:\n",
    "            batch = []\n",
    "\n",
    "            random_class_indices = []\n",
    "            for i in range(self.n_classes):\n",
    "                # if not majority class, and we ran out of samples, reset\n",
    "                if i!= self.max_class and len(self.class_indices[i]) < self.class_iteration_index[i] + self.per_class_batchsize:\n",
    "                    self.class_iteration_index[i] = 0\n",
    "                j = self.class_iteration_index[i]\n",
    "                sliced = self.class_indices[i][j:j+self.per_class_batchsize] \n",
    "                batch.extend(sliced)\n",
    "                self.class_iteration_index[i] += self.per_class_batchsize\n",
    "\n",
    "            yield batch\n",
    "            batch = []\n",
    "\n",
    "        # reset indices for next epoch\n",
    "        for i in range(self.n_classes):\n",
    "            self.class_iteration_index[i] = 0\n",
    "            if self.shuffle:\n",
    "                self.class_indices[i] = list(np.random.permutation(self.class_indices[i]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_class_length // self.per_class_batchsize\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WritePyFile(foldname, savename, savedicname, mod, aug, fpath, kcount, cont=False,batchsize=64,n_epochs=300,gpu=None):\n",
    "    #Writes python file which can be run seperately to produce easily multithreaded runs...\n",
    "    print(foldname, savename, savedicname, mod, aug, fpath)\n",
    "    #Loads all possible classes and models\n",
    "    with open(path.join(foldname,'exonet_basefile.py'),'r') as f_x:\n",
    "        file2write=f_x.read()\n",
    "    \n",
    "    file2write+='''\n",
    "#INPUTS:\n",
    "foldname    = \\\"{foldname}\\\"\n",
    "savename    = \\\"{savename}\\\"\n",
    "savedicname = \\\"{savedicname}\\\"\n",
    "mod         = \\\"{mod}\\\"\n",
    "aug         = \\\"{aug}\\\"\n",
    "fpath       = \\\"{fpath}\\\"\n",
    "kcount      = \\\"{kcount}\\\"\n",
    "#cont       = {cont}\n",
    "'''.format(foldname=foldname,savename=savename,savedicname=savedicname,mod=mod,aug=aug,fpath=fpath,kcount=kcount,cont=str(cont))\n",
    "    if gpu is None:\n",
    "        file2write+='''\n",
    "#Assigning a GPU:\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \\\"{count}\\\"\n",
    "'''.format(count=str(int(np.floor(kcount/2))))\n",
    "    else:\n",
    "        file2write+='''\n",
    "#Assigning a GPU:\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \\\"{count}\\\"\n",
    "'''.format(count=str(int(gpu)))\n",
    "\n",
    "    file2write+='''\n",
    "model=Model().cuda()\n",
    "lr = 2.05e-5\n",
    "'''.format(modsel=models[mod])\n",
    "    file2write+='''\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_size = {batchsize}\n",
    "n_epochs = {n_epochs}\n",
    "\n",
    "print(savename,\"Loading datasets\")\n",
    "kepler_val_data = KeplerDataLoaderCrossVal(infofiles=pickle.load(open(path.join(foldname,savename+'_valfiles.pickle'),'rb')))\n",
    "kepler_train_data = KeplerDataLoaderCrossVal(infofiles=pickle.load(open(path.join(foldname,savename+'_trainfiles.pickle'),'rb')))\n",
    "kepler_batch_sampler = pickle.load(open(path.join(foldname,savename+'_BBS.pickle'),'rb'))\n",
    "kepler_data_loader = DataLoader(kepler_train_data, batch_sampler = kepler_batch_sampler, num_workers=4)\n",
    "kepler_val_loader = DataLoader(kepler_val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "'''.format(foldname=foldname,savename=savename,batchsize=str(int(batchsize)),n_epochs=str(int(n_epochs)))\n",
    "\n",
    "    if cont:\n",
    "        #If continuing from saved state:\n",
    "        file2write+='''\n",
    "    \n",
    "try:\n",
    "    model.load_state_dict(torch.load(path.join(foldname,savename.replace('_comp','')+'_temp.pth')))\n",
    "except:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(path.join(foldname,savename.replace('_comp','')+'.pth')))\n",
    "        \n",
    "    except:\n",
    "        print(\"Cannot re-load .pth from file\")\n",
    "try:\n",
    "    dic=pickle.load(open(path.join(foldname,savename.replace('_comp','')+'_tempdic.pickle'),'rb'))\n",
    "except:\n",
    "    try:\n",
    "        dic=pickle.load(open(path.join(foldname,savename.replace('_comp','')+'_final_dic.pickle'),'rb'))\n",
    "    except:\n",
    "        print(\"Cannot re-load dic from file - neither\", )\n",
    "\n",
    "'''\n",
    "    else:\n",
    "        file2write+='''\n",
    "dic=None\n",
    "'''\n",
    "    print(augment,aug)\n",
    "    file2write+='''\n",
    "print(savename,\"starting training\")\n",
    "outputdic = train_model(n_epochs, kepler_data_loader, kepler_val_loader, model, criterion, optimizer,augment=\\\"{aug_str}\\\",savefile=path.join(foldname,savename+'_tempdic.pickle'),cont=dic)\n",
    "print(\"saving \"+savename)\n",
    "torch.save(model.state_dict(),path.join(foldname,savename+'.pth'))\n",
    "'''.format(aug_str=augment[aug])\n",
    "    file2write+='''\n",
    "outputdic['k']=kcount;outputdic['fpath']=fpath;outputdic['aug']=aug\n",
    "outputdic['mod']=mod\n",
    "outputdic['unqid']=fpath+'_'+aug+'_'+mod\n",
    "'''\n",
    "    file2write+='''\n",
    "pickle.dump(outputdic,open(path.join(foldname,savename+'_final_dic.pickle'),'wb'))\n",
    "if path.exists(path.join(foldname,\\\"{pickledic}\\\")):\n",
    "    outputvals=pickle.load(open(path.join(foldname,\\\"{pickledic}\\\"),'rb'))\n",
    "else:\n",
    "    outputvals={{}}\n",
    "outputvals[savename]=outputdic\n",
    "pickle.dump(outputvals,open(path.join(foldname,\\\"{pickledic}\\\"),'wb'))\n",
    "'''.format(pickledic=savedicname+'.pickle')\n",
    "    #print(file2write[23095:])\n",
    "    if not path.isdir(path.join(foldname,'run')):\n",
    "        os.system('mkdir '+path.join(foldname,'run'))\n",
    "    with open(path.join(foldname,'run',savename+'.py'),'w') as f:\n",
    "        f.write(file2write)\n",
    "    return path.join(foldname,'run',savename+'.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8 exonet_CV_0.8_101_all_Big exonet_multiclass4_CV_globcents3b_k8_dic Big all 101\n",
      "{'all': 'all'} all\n",
      "/home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8 exonet_CV_1.8_101_all_Big exonet_multiclass4_CV_globcents3b_k8_dic Big all 101\n",
      "{'all': 'all'} all\n",
      "/home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8 exonet_CV_2.8_101_all_Big exonet_multiclass4_CV_globcents3b_k8_dic Big all 101\n",
      "{'all': 'all'} all\n",
      "/home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8 exonet_CV_3.8_101_all_Big exonet_multiclass4_CV_globcents3b_k8_dic Big all 101\n",
      "{'all': 'all'} all\n",
      "/home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8 exonet_CV_4.8_101_all_Big exonet_multiclass4_CV_globcents3b_k8_dic Big all 101\n",
      "{'all': 'all'} all\n",
      "/home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8 exonet_CV_5.8_101_all_Big exonet_multiclass4_CV_globcents3b_k8_dic Big all 101\n",
      "{'all': 'all'} all\n",
      "/home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8 exonet_CV_6.8_101_all_Big exonet_multiclass4_CV_globcents3b_k8_dic Big all 101\n",
      "{'all': 'all'} all\n",
      "/home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8 exonet_CV_7.8_101_all_Big exonet_multiclass4_CV_globcents3b_k8_dic Big all 101\n",
      "{'all': 'all'} all\n",
      "\n",
      "#Running all files in /home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8 \n",
      "python3 /home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8/run/exonet_CV_0.8_101_all_Big.py &\n",
      "python3 /home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8/run/exonet_CV_1.8_101_all_Big.py &\n",
      "python3 /home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8/run/exonet_CV_2.8_101_all_Big.py &\n",
      "python3 /home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8/run/exonet_CV_3.8_101_all_Big.py &\n",
      "python3 /home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8/run/exonet_CV_4.8_101_all_Big.py &\n",
      "python3 /home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8/run/exonet_CV_5.8_101_all_Big.py &\n",
      "python3 /home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8/run/exonet_CV_6.8_101_all_Big.py &\n",
      "python3 /home/hosborn/TESS/final_runs/exonet_multiclass4_CV_globcents3b_k8/run/exonet_CV_7.8_101_all_Big.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Don't continue\n",
    "cont=False\n",
    "\n",
    "import pickle\n",
    "savedicname=os.getcwd().split('/')[-1]+'_dic'\n",
    "foldname=os.getcwd()\n",
    "names_of_files=[]\n",
    "\n",
    "#for batch in batch_sizes:\n",
    "batch_size = 64\n",
    "models={'Big':'ExtranetModel'}\n",
    "augment={'all':'all'}\n",
    "filepaths={'101':'/home/hosborn/TESS/processed_dv_101_centfixed2/'}\n",
    "\n",
    "n_epochs=700\n",
    "\n",
    "#N GPUS:\n",
    "nkfolds=8\n",
    "\n",
    "for fpath in filepaths:\n",
    "    ### divide train and validation sets\n",
    "    kf = KFold(n_splits=nkfolds, shuffle=True)\n",
    "    files_all = np.sort(glob.glob(path.join(filepaths[fpath],'all','*info2.npy')))\n",
    "    '''\n",
    "    #kepler_train_data = KeplerDataLoader(filepath=path.join(filepaths[fpath],'train'))\n",
    "    #kepler_val_data = KeplerDataLoader(filepath=path.join(filepaths[fpath],'val'))\n",
    "    '''\n",
    "    \n",
    "    mod='Big'\n",
    "    aug='all'\n",
    "    ### loop over folds\n",
    "    kcount = 0\n",
    "    precision_all, recall_all, ap_all = [], [], []\n",
    "    for train_index, val_index in kf.split(files_all):\n",
    "        ### initialize model\n",
    "\n",
    "        savename='exonet_CV_'+str(kcount)+'.'+str(nkfolds)+'_'+fpath+'_'+aug+'_'+mod\n",
    "\n",
    "        ### grab training and validation data\n",
    "        files_train, files_val = files_all[train_index], files_all[val_index]\n",
    "        pickle.dump(files_train,open(path.join(foldname,savename+'_trainfiles.pickle'),'wb'))\n",
    "        pickle.dump(files_val,open(path.join(foldname,savename+'_valfiles.pickle'),'wb'))\n",
    "\n",
    "        kepler_val_data = KeplerDataLoaderCrossVal(infofiles=files_val)\n",
    "        kepler_train_data = KeplerDataLoaderCrossVal(infofiles=files_train)\n",
    "\n",
    "        #Loading balancer:\n",
    "        if not path.exists(path.join(foldname,savename+'_BBS.pickle')):\n",
    "            kepler_batch_sampler = BalancedBatchSampler(batch_size, kepler_train_data, 4) #batch_size, dataset, n_classes\n",
    "            pickle.dump(kepler_batch_sampler, open(path.join(foldname,savename+'_BBS.pickle'),'wb'))\n",
    "\n",
    "        names_of_files+=['python3 '+WritePyFile(foldname,savename,savedicname,mod,aug,fpath,kcount,cont=cont,n_epochs=n_epochs,gpu=np.arange(8)[kcount])]\n",
    "        kcount+=1\n",
    "\n",
    "with open(path.join(foldname,'all_exonet_multiclass_run.sh'),'w') as f_runall:\n",
    "    print(\"\\n#Running all files in \"+foldname+' \\n'+' &\\n'.join(names_of_files))\n",
    "    f_runall.write(\"#Running all files in \"+foldname+' \\n'+' &\\n'.join(names_of_files))\n",
    "\n",
    "os.system('chmod +x *_exonet_multiclass_run.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[10297, 3039, 5806]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_sizes={'32':32,'64':64}\n",
    "batch_size = 64\n",
    "#lrs={'slow':5e-5,'fast':5e-4}\n",
    "filepaths={'101':'/home/hosborn/TESS/processed_dv_101_centfixed2/'}\n",
    "models={'Big':'ExtranetModel'}\n",
    "#augment={'none':'','all':'all','noise':'noise','xshift':'xshift','yshift':'yshift','mirror':'mirror'}\n",
    "augment={'all':'all'}\n",
    "#augment={'all':'all','noise':'noise'}#,'xshift':'xshift','mirror':'mirror'}\n",
    "#augment={'all':'all'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nkfolds=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "savedicname='all_exonet_multiclass_CV_wfixedcent_dic'\n",
    "foldname='exonet_multiclass_CV_wfixedcent'\n",
    "\n",
    "if path.exists(path.join(foldname,savedicname+'.pickle')):\n",
    "    outputvals=pickle.load(open(path.join(foldname,savedicname+'.pickle'),'rb'))\n",
    "else:\n",
    "    outputvals={}\n",
    "\n",
    "if not path.isdir(foldname):\n",
    "    os.system('mkdir '+foldname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "\n",
    "#for batch in batch_sizes:\n",
    "#    for lr in lrs:           \n",
    "models={'Big':'ExtranetModel'}\n",
    "augment={'all':'all'}\n",
    "\n",
    "for fpath in filepaths:\n",
    "    ### divide train and validation sets\n",
    "    kf = KFold(n_splits=nkfolds, shuffle=True)\n",
    "    files_all = np.sort(glob.glob(path.join(filepaths[fpath],'all','*info.npy')))\n",
    "    '''\n",
    "    #kepler_train_data = KeplerDataLoader(filepath=path.join(filepaths[fpath],'train'))\n",
    "    #kepler_val_data = KeplerDataLoader(filepath=path.join(filepaths[fpath],'val'))\n",
    "    '''\n",
    "    \n",
    "    for mod in models:\n",
    "        for aug in augment:\n",
    "            ### loop over folds\n",
    "            kcount = 0\n",
    "            precision_all, recall_all, ap_all = [], [], []\n",
    "            for train_index, val_index in kf.split(files_all):\n",
    "                ### initialize model\n",
    "                if mod=='Big':\n",
    "                    #Have to have separate models for Big as no global pooling means 201-long LC produces more params\n",
    "                    model=eval(models[mod]+'_'+fpath+'().cuda()')\n",
    "                    lr = 1.5e-6\n",
    "                else:\n",
    "                    model=models[mod]().cuda()\n",
    "                    lr = 2.5e-4\n",
    "                \n",
    "                savename='exonet_CV'+str(kcount)+'_'+fpath+'_'+aug+'_'+mod\n",
    "                \n",
    "                \n",
    "                \n",
    "                if savename not in list(outputvals.keys()):\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                    criterion = nn.BCELoss()\n",
    "                    batch_size = 64\n",
    "                    n_epochs = 300\n",
    "                    \n",
    "                    ### grab training and validation data\n",
    "                    files_train, files_val = files_all[train_index], files_all[val_index]\n",
    "                    kepler_val_data = KeplerDataLoaderCrossVal(infofiles=files_val)\n",
    "                    kepler_train_data = KeplerDataLoaderCrossVal(infofiles=files_train)\n",
    "                        \n",
    "                    #Loadingin balancer:\n",
    "                    fname=path.join(foldname,savename+'_BBS.pickle')\n",
    "                    if path.exists(fname):\n",
    "                        kepler_batch_sampler = pickle.load(open(fname,'rb'))\n",
    "                    else:\n",
    "                        kepler_batch_sampler = BalancedBatchSampler(batch_size, kepler_train_data, 2) #batch_size, dataset, n_classes\n",
    "                        pickle.dump(kepler_batch_sampler, open(fname,'wb'))\n",
    "                    \n",
    "                    WritePyFile(balancerfname=fname)\n",
    "                    \n",
    "                    kepler_data_loader = DataLoader(kepler_train_data, batch_sampler = kepler_batch_sampler, num_workers=4)\n",
    "                    kepler_val_loader = DataLoader(kepler_val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "                \n",
    "                    \n",
    "                    \n",
    "                    if not path.exists(path.join(foldname,savename+'.pth')):\n",
    "\n",
    "                        ### train model\n",
    "                        print(\"training \"+savename)\n",
    "                        loss_train_epoch, loss_val_epoch, acc_val_epoch, ap_val_epoch, pred_val_final, gt_val_final  = train_model(n_epochs, kepler_data_loader, kepler_val_loader, model, criterion, optimizer,augment=augment[aug])\n",
    "                        print(\"saving \"+savename)\n",
    "                        torch.save(model.state_dict(),path.join(foldname,savename+'.pth'))\n",
    "                    else:\n",
    "                        model.load_state_dict(torch.load(path.join(foldname,savename+'.pth')))\n",
    "                        loss_train_epoch, loss_val_epoch, acc_val_epoch, ap_val_epoch, pred_val_final, gt_val_final  = train_model(3, kepler_data_loader, kepler_val_loader, model, criterion, optimizer,augment=augment[aug])\n",
    "                    \n",
    "                    outputvals[savename]={'loss_train_epoch':loss_train_epoch, 'loss_val_epoch':loss_val_epoch,\n",
    "                                          'acc_val_epoch':acc_val_epoch,'ap_val_epoch':ap_val_epoch,\n",
    "                                          'pred_val_final':pred_val_final, 'gt_val_final':gt_val_final,\n",
    "                                          'k':kcount,'fpath':fpath,'aug':aug,'mod':mod,'unqid':fpath+'_'+aug+'_'+mod}\n",
    "                \n",
    "                else:\n",
    "                    print(\"dictionary of trained values already exists\")\n",
    "                    '''\n",
    "                    loss_train_epoch=outputvals[savename]['loss_train_epoch']\n",
    "                    loss_val_epoch=outputvals[savename]['loss_val_epoch']\n",
    "                    acc_val_epoch=outputvals[savename]['acc_val_epoch']\n",
    "                    pred_val_final= outputvals[savename]['pred_val_final']\n",
    "                    gt_val_final= outputvals[savename]['gt_val_final']\n",
    "                    #tn, fp, fn, tp= outputvals[savename]['matrix_0.5']\n",
    "                    #average_precision= outputvals[savename]['average_precision_pl']\n",
    "                    \n",
    "                    #epoch_val_recall_pl,epoch_val_recall_ebs,epoch_val_recall_unk = recall_val\n",
    "                    #epoch_val_acc,epoch_val_acc_pl,epoch_val_acc_ebs,epoch_val_acc_unk = acc_val\n",
    "                    outputvals[savename]={'loss_train_epoch':loss_train_epoch, 'loss_val_epoch':loss_val_epoch,\n",
    "                              'acc_val_epoch':acc_val_epoch,'ap_val_epoch':ap_val_epoch,\n",
    "                              'pred_val_final':pred_val_final, 'gt_val_final':gt_val_final,\n",
    "                              'k':kcount,'fpath':fpath,'aug':aug,'mod':mod,'unqid':fpath+'_'+aug+'_'+mod}\n",
    "                    '''\n",
    "                kcount+=1\n",
    "                pickle.dump(outputvals,open(path.join(foldname,savedicname+'.pickle'),'wb'))\n",
    "                \n",
    "            print('#Assembling Cross-Val results after '+str(kcount)+' CVs')\n",
    "            dickeys=[key for key in outputvals if (outputvals[key]['unqid']==fpath+'_'+aug+'_'+mod)*~np.isnan(outputvals[key]['k'])]\n",
    "            loss_train_epoch=[];loss_val_epoch=[];gt_val_final=[];pred_val_final=[]\n",
    "            ap_val_epoch=[];acc_val_epoch=[]\n",
    "            for key in dickeys:\n",
    "                gt_val_final.append(outputvals[key]['gt_val_final'])\n",
    "                pred_val_final.append(outputvals[key]['pred_val_final'])\n",
    "                loss_train_epoch.append(outputvals[key]['loss_train_epoch'])\n",
    "                loss_val_epoch.append(outputvals[key]['loss_val_epoch'])\n",
    "                ap_val_epoch.append(outputvals[key]['ap_val_epoch'])\n",
    "                acc_val_epoch.append(outputvals[key]['acc_val_epoch'])\n",
    "            gt_val_final=np.hstack(gt_val_final)\n",
    "            pred_val_final=np.hstack(pred_val_final)\n",
    "            loss_train_epoch=[np.hstack(l) for l in loss_train_epoch]\n",
    "            loss_val_epoch=[np.hstack(l) for l in loss_val_epoch]\n",
    "            ap_val_epoch=[np.hstack(l) for l in ap_val_epoch]\n",
    "            acc_val_epoch=[np.hstack(l) for l in acc_val_epoch]\n",
    "            ### transform from loss per sample to loss per batch (multiple by batch size to compare to Chris')\n",
    "            #loss_train_batch = [x.item()* batch_size for x in loss_train_epoch]\n",
    "            #loss_val_batch = [x.item()* batch_size for x in loss_val_epoch]\n",
    "\n",
    "            ### calculate average precision + precision-recall curves\n",
    "            P, R, _ = precision_recall_curve(gt_val_final, pred_val_final)\n",
    "            AP = average_precision_score(gt_val_final, pred_val_final, average='weighted')\n",
    "            print(\"weighted average precision = {0:0.4f}\".format(AP))\n",
    "\n",
    "            ### convert prediction to bytes based on threshold\n",
    "            thresh = [0.9, 0.7, 0.5]\n",
    "            prec_thresh, recall_thresh = np.zeros(len(thresh)), np.zeros(len(thresh))\n",
    "            for n, nval in enumerate(thresh):\n",
    "                pred_byte = np.zeros(len(pred_val_final))\n",
    "                for i, val in enumerate(pred_val_final):\n",
    "                    if val > nval:\n",
    "                        pred_byte[i] = 1.0\n",
    "                    else:\n",
    "                        pred_byte[i] = 0.0\n",
    "                prec_thresh[n] = precision_score(gt_val_final, pred_byte)\n",
    "                recall_thresh[n] = recall_score(gt_val_final, pred_byte)\n",
    "                print(savename,\": thresh = {0:0.2f}, precision = {1:0.2f}, recall = {2:0.2f}\".format(thresh[n], prec_thresh[n], recall_thresh[n]))\n",
    "                tn, fp, fn, tp = confusion_matrix(gt_val_final, pred_byte).ravel()\n",
    "                print(savename,\":    TN = {0:0}, FP = {1:0}, FN = {2:0}, TP = {3:0}\".format(tn, fp, fn, tp))\n",
    "            for key in dickeys:\n",
    "                outputvals[key].update({'CV_P':P,'CV_R':R,'CV_AP':AP,'CV_matrix_0.5':[tn, fp, fn, tp]})\n",
    "            \n",
    "            outputvals['exonet_CVall_'+fpath+'_'+aug+'_'+mod]={'loss_train_epoch':loss_train_epoch, 'loss_val_epoch':loss_val_epoch,\n",
    "                                          'acc_val_epoch':acc_val_epoch,'ap_val_epoch':ap_val_epoch,\n",
    "                                          'pred_val_final':pred_val_final, 'gt_val_final':gt_val_final,\n",
    "                                          'k':np.nan,'fpath':fpath,'aug':aug,'mod':mod,'unqid':fpath+'_'+aug+'_'+mod,\n",
    "                                          'P':P,'R':R,'AP':AP,'matrix_0.5':[tn, fp, fn, tp]}\n",
    "\n",
    "            plt.clf()\n",
    "            plt.subplot(2,2,1)\n",
    "            ### plot values\n",
    "            plt.step(R, P)\n",
    "            plt.title(fpath+'_'+aug+'_'+mod+': Precision vs. Recall, AP={0:0.3f}'.format(AP))\n",
    "\n",
    "            plt.subplot(2,2,2)\n",
    "            plt.title(fpath+'_'+aug+'_'+mod+': Loss per Epoch')\n",
    "            #plt.plot(np.arange(len(loss_train_batch)), loss_train_batch,color=sns.color_palette()[0])\n",
    "            #plt.plot(np.arange(len(loss_val_batch)), loss_val_batch,color=sns.color_palette()[1])\n",
    "            for n,key in enumerate(dickeys):\n",
    "                plt.plot(np.array(outputvals[key]['loss_train_epoch']),':',alpha=0.75,color=sns.color_palette()[n])\n",
    "                plt.plot(np.array(outputvals[key]['loss_val_epoch']),'--',alpha=0.75,color=sns.color_palette()[n])\n",
    "                plt.plot([0.0,0.0],[0.0,0.0],'-',alpha=0.75,label=\"k=\"+str(outputvals[key]['k']))\n",
    "            plt.plot([0.0,0.0],[0.0,0.0],'--',label='validation')\n",
    "            plt.plot([0.0,0.0],[0.0,0.0],':',label='training')\n",
    "            plt.legend(loc=2)\n",
    "            \n",
    "            plt.subplot(2,2,3)\n",
    "            plt.title(fpath+'_'+aug+'_'+mod+': Average Precision per Epoch')\n",
    "            #plt.plot(np.arange(len(ap_val_epoch)), ap_val_epoch)\n",
    "            for key in dickeys:\n",
    "                plt.plot(outputvals[key]['ap_val_epoch'],'-',alpha=0.75)\n",
    "                plt.plot([0.0,0.0],[0.0,0.0],'-',alpha=0.75,label=\"k=\"+str(outputvals[key]['k']))\n",
    "            plt.legend(loc=4)\n",
    "            \n",
    "            plt.subplot(2,2,4)\n",
    "            plt.title(fpath+'_'+aug+'_'+mod+': Accuracy per Epoch')\n",
    "            #plt.plot(np.arange(len(acc_val_epoch)), acc_val_epoch)\n",
    "            for key in dickeys:\n",
    "                plt.plot(outputvals[key]['acc_val_epoch'],':',alpha=0.75)\n",
    "                plt.plot([0.0,0.0],[0.0,0.0],'-',alpha=0.75,label=\"k=\"+str(outputvals[key]['k']))\n",
    "            plt.legend(loc=4)\n",
    "            \n",
    "            plt.savefig(path.join(foldname,'exonet_CVall_'+fpath+'_'+aug+'_'+mod+\".png\"))\n",
    "            pickle.dump(outputvals,open(path.join(foldname,savedicname+'.pickle'),'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dic=pickle.load(open('exonet_CV_0.8_101_all_Big_tempdic.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc_val_epoch_bebs': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8333333333333334,\n",
       "  0.5675675675675675,\n",
       "  0.6846846846846847,\n",
       "  0.6666666666666666,\n",
       "  0.6467065868263473,\n",
       "  0.6194690265486725,\n",
       "  0.6707317073170732,\n",
       "  0.5182795698924731,\n",
       "  0.5915032679738562,\n",
       "  0.6372093023255814,\n",
       "  0.6555023923444976,\n",
       "  0.676056338028169,\n",
       "  0.6682242990654206,\n",
       "  0.6714975845410628,\n",
       "  0.551219512195122,\n",
       "  0.6037037037037037,\n",
       "  0.6330935251798561,\n",
       "  0.7013888888888888,\n",
       "  0.6724137931034483,\n",
       "  0.5882352941176471,\n",
       "  0.6470588235294118,\n",
       "  0.6439393939393939,\n",
       "  0.6105919003115264,\n",
       "  0.696969696969697,\n",
       "  0.6455223880597015,\n",
       "  0.6600790513833992,\n",
       "  0.6012658227848101,\n",
       "  0.6589861751152074,\n",
       "  0.656,\n",
       "  0.6045845272206304,\n",
       "  0.6510638297872341,\n",
       "  0.5905882352941176,\n",
       "  0.6090425531914894,\n",
       "  0.5659090909090909,\n",
       "  0.6404494382022472,\n",
       "  0.5789473684210527,\n",
       "  0.6586206896551724,\n",
       "  0.658008658008658,\n",
       "  0.6592920353982301,\n",
       "  0.5967741935483871,\n",
       "  0.6257668711656442,\n",
       "  0.6,\n",
       "  0.6354515050167224,\n",
       "  0.6433566433566433,\n",
       "  0.6312292358803987,\n",
       "  0.7352941176470589,\n",
       "  0.5008025682182986,\n",
       "  0.6094182825484764,\n",
       "  0.6577946768060836,\n",
       "  0.6681614349775785,\n",
       "  0.689119170984456,\n",
       "  0.6610169491525424,\n",
       "  0.6919431279620853,\n",
       "  0.579175704989154,\n",
       "  0.6303630363036303,\n",
       "  0.6619217081850534,\n",
       "  0.5117540687160941,\n",
       "  0.6293929712460063,\n",
       "  0.556640625,\n",
       "  0.6541353383458647,\n",
       "  0.575107296137339,\n",
       "  0.6606334841628959,\n",
       "  0.6303030303030303,\n",
       "  0.5838779956427015,\n",
       "  0.6041666666666666,\n",
       "  0.6384364820846905,\n",
       "  0.7070063694267515,\n",
       "  0.6459854014598541,\n",
       "  0.5846501128668171,\n",
       "  0.6214511041009464,\n",
       "  0.6566666666666666,\n",
       "  0.6626016260162602,\n",
       "  0.6137566137566137,\n",
       "  0.6263440860215054,\n",
       "  0.5995423340961098,\n",
       "  0.6253521126760564,\n",
       "  0.49326599326599324,\n",
       "  0.667953667953668,\n",
       "  0.5907172995780591,\n",
       "  0.6586206896551724,\n",
       "  0.6202898550724638,\n",
       "  0.5811965811965812,\n",
       "  0.648,\n",
       "  0.6070460704607046,\n",
       "  0.6328767123287671,\n",
       "  0.6329113924050633,\n",
       "  0.6153846153846154,\n",
       "  0.610126582278481,\n",
       "  0.6108247422680413,\n",
       "  0.6744186046511628,\n",
       "  0.6320474777448071,\n",
       "  0.49846153846153846,\n",
       "  0.6573208722741433,\n",
       "  0.6923076923076923,\n",
       "  0.6214099216710183,\n",
       "  0.58675799086758,\n",
       "  0.6411764705882353,\n",
       "  0.607981220657277,\n",
       "  0.5777351247600768,\n",
       "  0.6219512195121951,\n",
       "  0.546112115732369,\n",
       "  0.623229461756374,\n",
       "  0.6727272727272727,\n",
       "  0.6004366812227074,\n",
       "  0.6331360946745562,\n",
       "  0.6630434782608695,\n",
       "  0.6430769230769231,\n",
       "  0.6133651551312649,\n",
       "  0.6204986149584487,\n",
       "  0.6144578313253012,\n",
       "  0.6653992395437263,\n",
       "  0.6303724928366762,\n",
       "  0.5777777777777777,\n",
       "  0.6369230769230769,\n",
       "  0.52099533437014,\n",
       "  0.6129032258064516,\n",
       "  0.6026200873362445,\n",
       "  0.6551724137931034,\n",
       "  0.6145833333333334,\n",
       "  0.6253229974160207,\n",
       "  0.6197183098591549,\n",
       "  0.5974304068522484,\n",
       "  0.6717557251908397,\n",
       "  0.6349614395886889,\n",
       "  0.6299694189602446,\n",
       "  0.6299019607843137,\n",
       "  0.6246913580246913,\n",
       "  0.6634920634920635,\n",
       "  0.6467065868263473,\n",
       "  0.7130434782608696,\n",
       "  0.6578947368421053,\n",
       "  0.6764705882352942,\n",
       "  0.6559139784946236,\n",
       "  0.6172248803827751,\n",
       "  0.6677215189873418,\n",
       "  0.6494845360824743,\n",
       "  0.6879699248120301,\n",
       "  0.7063492063492064,\n",
       "  0.6564245810055865],\n",
       " 'acc_val_epoch_ebs': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.48184818481848185,\n",
       "  0.44793713163064836,\n",
       "  0.427263479145473,\n",
       "  0.467280163599182,\n",
       "  0.47780126849894294,\n",
       "  0.4651866801210898,\n",
       "  0.4807480748074808,\n",
       "  0.4732510288065844,\n",
       "  0.4476744186046512,\n",
       "  0.5011961722488039,\n",
       "  0.48066875653082547,\n",
       "  0.4902173913043478,\n",
       "  0.47412882787750793,\n",
       "  0.5069284064665127,\n",
       "  0.4866023579849946,\n",
       "  0.5108433734939759,\n",
       "  0.5098039215686274,\n",
       "  0.4622918707149853,\n",
       "  0.4822546972860125,\n",
       "  0.49192680301399355,\n",
       "  0.5125570776255708,\n",
       "  0.5355805243445693,\n",
       "  0.5415584415584416,\n",
       "  0.5377833753148614,\n",
       "  0.48917748917748916,\n",
       "  0.5331724969843185,\n",
       "  0.6115879828326181,\n",
       "  0.5659574468085107,\n",
       "  0.5558408215661104,\n",
       "  0.5525982256020279,\n",
       "  0.5813630041724618,\n",
       "  0.5789473684210527,\n",
       "  0.5633986928104575,\n",
       "  0.6079545454545454,\n",
       "  0.5463786531130876,\n",
       "  0.5996784565916399,\n",
       "  0.5219347581552306,\n",
       "  0.608,\n",
       "  0.5965732087227414,\n",
       "  0.5801630434782609,\n",
       "  0.5958083832335329,\n",
       "  0.6009174311926605,\n",
       "  0.504885993485342,\n",
       "  0.5803698435277382,\n",
       "  0.5852272727272727,\n",
       "  0.5780346820809249,\n",
       "  0.5388409371146733,\n",
       "  0.5765645805592543,\n",
       "  0.5990566037735849,\n",
       "  0.5731543624161074,\n",
       "  0.6307129798903108,\n",
       "  0.594383775351014,\n",
       "  0.630859375,\n",
       "  0.5721854304635762,\n",
       "  0.6295620437956204,\n",
       "  0.5941605839416059,\n",
       "  0.5760869565217391,\n",
       "  0.5579617834394904,\n",
       "  0.6582278481012658,\n",
       "  0.6173044925124792,\n",
       "  0.613747954173486,\n",
       "  0.6071987480438185,\n",
       "  0.5824175824175825,\n",
       "  0.5937940761636107,\n",
       "  0.5478971962616822,\n",
       "  0.6614173228346457,\n",
       "  0.6105919003115264,\n",
       "  0.5883152173913043,\n",
       "  0.5538089480048367,\n",
       "  0.562268803945746,\n",
       "  0.576379974326059,\n",
       "  0.5871313672922251,\n",
       "  0.6614173228346457,\n",
       "  0.5962910128388017,\n",
       "  0.6128093158660844,\n",
       "  0.6666666666666666,\n",
       "  0.597953216374269,\n",
       "  0.6593406593406593,\n",
       "  0.586013986013986,\n",
       "  0.6747787610619469,\n",
       "  0.5714285714285714,\n",
       "  0.6201923076923077,\n",
       "  0.6471698113207547,\n",
       "  0.6412859560067682,\n",
       "  0.590778097982709,\n",
       "  0.5373303167420814,\n",
       "  0.5883977900552486,\n",
       "  0.6425992779783394,\n",
       "  0.5856741573033708,\n",
       "  0.5966850828729282,\n",
       "  0.5734720416124838,\n",
       "  0.6211382113821138,\n",
       "  0.6565464895635673,\n",
       "  0.6465364120781527,\n",
       "  0.6333907056798623,\n",
       "  0.6498740554156172,\n",
       "  0.5617283950617284,\n",
       "  0.6680244399185336,\n",
       "  0.5937940761636107,\n",
       "  0.6026986506746627,\n",
       "  0.6641651031894934,\n",
       "  0.5730622617534943,\n",
       "  0.6031007751937985,\n",
       "  0.6258064516129033,\n",
       "  0.6124620060790273,\n",
       "  0.6185243328100472,\n",
       "  0.6313131313131313,\n",
       "  0.6274193548387097,\n",
       "  0.5714285714285714,\n",
       "  0.6163141993957704,\n",
       "  0.6752873563218391,\n",
       "  0.6298600311041991,\n",
       "  0.5518867924528302,\n",
       "  0.6175548589341693,\n",
       "  0.6381461675579323,\n",
       "  0.620845921450151,\n",
       "  0.6456140350877193,\n",
       "  0.6756198347107438,\n",
       "  0.594017094017094,\n",
       "  0.678082191780822,\n",
       "  0.6149162861491628,\n",
       "  0.5874499332443258,\n",
       "  0.6533575317604355,\n",
       "  0.600864553314121,\n",
       "  0.596931659693166,\n",
       "  0.616793893129771,\n",
       "  0.6415094339622641,\n",
       "  0.6106194690265486,\n",
       "  0.6312399355877617,\n",
       "  0.583989501312336,\n",
       "  0.6273291925465838,\n",
       "  0.6659919028340081,\n",
       "  0.6082621082621082,\n",
       "  0.7142857142857143,\n",
       "  0.6216640502354788,\n",
       "  0.651705565529623,\n",
       "  0.5991792065663475,\n",
       "  0.6243980738362761,\n",
       "  0.6341059602649006,\n",
       "  0.6130500758725341,\n",
       "  0.6573556797020484,\n",
       "  0.600547195622435,\n",
       "  0.6561922365988909,\n",
       "  0.6025641025641025,\n",
       "  0.6666666666666666,\n",
       "  0.6522491349480969,\n",
       "  0.616191904047976,\n",
       "  0.641156462585034,\n",
       "  0.5843137254901961,\n",
       "  0.6055944055944056,\n",
       "  0.5820512820512821,\n",
       "  0.6032831737346102,\n",
       "  0.6446280991735537,\n",
       "  0.6334841628959276,\n",
       "  0.5986486486486486,\n",
       "  0.6026845637583893,\n",
       "  0.6159217877094972,\n",
       "  0.6648936170212766],\n",
       " 'acc_val_epoch_pl': [0.1644234267006184,\n",
       "  0.18999151823579305,\n",
       "  0.19467125053717232,\n",
       "  0.18574338085539716,\n",
       "  0.21449704142011836,\n",
       "  0.19620811287477954,\n",
       "  0.22015113350125945,\n",
       "  0.20696055684454756,\n",
       "  0.22383419689119172,\n",
       "  0.20741435945565462,\n",
       "  0.2418710781517399,\n",
       "  0.22760041194644695,\n",
       "  0.2207070707070707,\n",
       "  0.22588832487309646,\n",
       "  0.2009111617312073,\n",
       "  0.23060796645702306,\n",
       "  0.20743460302891234,\n",
       "  0.22322775263951736,\n",
       "  0.20955882352941177,\n",
       "  0.24931053502482073,\n",
       "  0.22701592193117617,\n",
       "  0.3652694610778443,\n",
       "  0.3925657298277425,\n",
       "  0.37178423236514524,\n",
       "  0.42125237191650855,\n",
       "  0.6330578512396694,\n",
       "  0.5544041450777202,\n",
       "  0.4744136460554371,\n",
       "  0.5314009661835749,\n",
       "  0.43737769080234834,\n",
       "  0.42095416276894293,\n",
       "  0.4392614188532556,\n",
       "  0.4468718967229394,\n",
       "  0.6530612244897959,\n",
       "  0.2785800240673887,\n",
       "  0.4633642930856553,\n",
       "  0.37551355792933444,\n",
       "  0.3685483870967742,\n",
       "  0.38396624472573837,\n",
       "  0.43307839388145314,\n",
       "  0.3568627450980392,\n",
       "  0.4422890397672163,\n",
       "  0.3627684964200477,\n",
       "  0.47624076029567053,\n",
       "  0.5354609929078015,\n",
       "  0.5581683168316832,\n",
       "  0.46845915201654603,\n",
       "  0.6712538226299695,\n",
       "  0.4754440961337513,\n",
       "  0.6211699164345403,\n",
       "  0.625,\n",
       "  0.6992,\n",
       "  0.4617737003058104,\n",
       "  0.49398907103825135,\n",
       "  0.5707070707070707,\n",
       "  0.4208566108007449,\n",
       "  0.49291166848418755,\n",
       "  0.420129270544783,\n",
       "  0.3563664596273292,\n",
       "  0.6637037037037037,\n",
       "  0.56125,\n",
       "  0.5375446960667462,\n",
       "  0.3663446054750403,\n",
       "  0.570528967254408,\n",
       "  0.5817245817245817,\n",
       "  0.6367521367521367,\n",
       "  0.468944099378882,\n",
       "  0.6426512968299711,\n",
       "  0.5960264900662252,\n",
       "  0.39325842696629215,\n",
       "  0.6010709504685409,\n",
       "  0.6331444759206799,\n",
       "  0.6564327485380117,\n",
       "  0.58777633289987,\n",
       "  0.47151898734177217,\n",
       "  0.3594361785434612,\n",
       "  0.5446756425948592,\n",
       "  0.6103542234332425,\n",
       "  0.4509415262636274,\n",
       "  0.3603174603174603,\n",
       "  0.35371517027863775,\n",
       "  0.4435564435564436,\n",
       "  0.42168674698795183,\n",
       "  0.3598425196850394,\n",
       "  0.3614649681528662,\n",
       "  0.5310262529832935,\n",
       "  0.5141562853907135,\n",
       "  0.44509232264334303,\n",
       "  0.45656565656565656,\n",
       "  0.5805194805194805,\n",
       "  0.45871559633027525,\n",
       "  0.5133876600698487,\n",
       "  0.6612903225806451,\n",
       "  0.6099195710455764,\n",
       "  0.5531400966183575,\n",
       "  0.6978193146417445,\n",
       "  0.6892307692307692,\n",
       "  0.731023102310231,\n",
       "  0.7517123287671232,\n",
       "  0.6431593794076164,\n",
       "  0.6864535768645358,\n",
       "  0.7316666666666667,\n",
       "  0.6772247360482655,\n",
       "  0.6854103343465046,\n",
       "  0.6589595375722543,\n",
       "  0.691717791411043,\n",
       "  0.7389162561576355,\n",
       "  0.654178674351585,\n",
       "  0.75,\n",
       "  0.6875,\n",
       "  0.7474916387959866,\n",
       "  0.6817496229260935,\n",
       "  0.6930091185410334,\n",
       "  0.6900763358778625,\n",
       "  0.6681481481481482,\n",
       "  0.6637298091042585,\n",
       "  0.7612456747404844,\n",
       "  0.7364085667215815,\n",
       "  0.7078125,\n",
       "  0.7382113821138211,\n",
       "  0.7563884156729132,\n",
       "  0.7069767441860465,\n",
       "  0.700920245398773,\n",
       "  0.7437185929648241,\n",
       "  0.7233704292527822,\n",
       "  0.6497175141242938,\n",
       "  0.7388429752066116,\n",
       "  0.6920731707317073,\n",
       "  0.7474916387959866,\n",
       "  0.6740412979351033,\n",
       "  0.7006172839506173,\n",
       "  0.7015384615384616,\n",
       "  0.661849710982659,\n",
       "  0.7446457990115322,\n",
       "  0.7694974003466204,\n",
       "  0.7220447284345048,\n",
       "  0.7305194805194806,\n",
       "  0.705607476635514,\n",
       "  0.6996904024767802,\n",
       "  0.6983154670750383,\n",
       "  0.7438423645320197,\n",
       "  0.734860883797054,\n",
       "  0.7563025210084033,\n",
       "  0.7413509060955519,\n",
       "  0.7698961937716263,\n",
       "  0.743421052631579,\n",
       "  0.6914893617021277,\n",
       "  0.7956989247311828,\n",
       "  0.6623188405797101,\n",
       "  0.7389885807504079,\n",
       "  0.6953846153846154,\n",
       "  0.6764705882352942,\n",
       "  0.7136150234741784,\n",
       "  0.7122641509433962,\n",
       "  0.7176656151419558,\n",
       "  0.7512437810945274,\n",
       "  0.7333333333333333,\n",
       "  0.7479131886477463,\n",
       "  0.7229299363057324,\n",
       "  0.7262479871175523,\n",
       "  0.6656976744186046,\n",
       "  0.7140600315955766,\n",
       "  0.7222222222222222,\n",
       "  0.7015384615384616,\n",
       "  0.7120500782472613,\n",
       "  0.7350565428109854,\n",
       "  0.7217806041335453,\n",
       "  0.7365853658536585,\n",
       "  0.7701543739279588,\n",
       "  0.7176656151419558,\n",
       "  0.7550675675675675,\n",
       "  0.7129337539432177,\n",
       "  0.729903536977492,\n",
       "  0.7558922558922558,\n",
       "  0.7194928684627575,\n",
       "  0.7746967071057193,\n",
       "  0.652050919377652,\n",
       "  0.7693631669535284,\n",
       "  0.7446457990115322,\n",
       "  0.7572156196943973,\n",
       "  0.760539629005059,\n",
       "  0.7350565428109854,\n",
       "  0.7397708674304418,\n",
       "  0.7571669477234402,\n",
       "  0.6925418569254186,\n",
       "  0.7529215358931552,\n",
       "  0.7181102362204724,\n",
       "  0.6835066864784547,\n",
       "  0.7334410339256866,\n",
       "  0.7262479871175523,\n",
       "  0.7724137931034483,\n",
       "  0.7406199021207178,\n",
       "  0.6543909348441926,\n",
       "  0.737012987012987,\n",
       "  0.7245222929936306,\n",
       "  0.6907993966817496,\n",
       "  0.6661849710982659,\n",
       "  0.7354838709677419,\n",
       "  0.8029197080291971,\n",
       "  0.6794682422451994,\n",
       "  0.6971080669710806,\n",
       "  0.7358184764991896,\n",
       "  0.7545909849749582,\n",
       "  0.7100313479623824,\n",
       "  0.7401960784313726,\n",
       "  0.729903536977492,\n",
       "  0.7169811320754716,\n",
       "  0.7545909849749582,\n",
       "  0.7696245733788396,\n",
       "  0.7872340425531915,\n",
       "  0.7105263157894737,\n",
       "  0.7292993630573248,\n",
       "  0.7622259696458684,\n",
       "  0.7724137931034483,\n",
       "  0.7575757575757576,\n",
       "  0.754180602006689,\n",
       "  0.7323717948717948,\n",
       "  0.7300319488817891,\n",
       "  0.7442622950819672,\n",
       "  0.7571669477234402,\n",
       "  0.7401315789473685,\n",
       "  0.7323717948717948,\n",
       "  0.7192429022082019,\n",
       "  0.7342995169082126,\n",
       "  0.7781629116117851,\n",
       "  0.693939393939394,\n",
       "  0.7382113821138211,\n",
       "  0.7701543739279588,\n",
       "  0.7592592592592593,\n",
       "  0.7529215358931552,\n",
       "  0.7422003284072249,\n",
       "  0.7312,\n",
       "  0.7508250825082509,\n",
       "  0.7414634146341463,\n",
       "  0.7413509060955519,\n",
       "  0.7149606299212599,\n",
       "  0.7165354330708661,\n",
       "  0.7187993680884676,\n",
       "  0.7525083612040134,\n",
       "  0.661849710982659,\n",
       "  0.7326892109500805,\n",
       "  0.7619047619047619,\n",
       "  0.7622259696458684,\n",
       "  0.7264,\n",
       "  0.7529411764705882,\n",
       "  0.7418300653594772,\n",
       "  0.7201907790143084,\n",
       "  0.7178683385579937,\n",
       "  0.7545909849749582,\n",
       "  0.710236220472441],\n",
       " 'acc_val_epoch_unk': [0.799163179916318,\n",
       "  0.5698412698412698,\n",
       "  0.6021180030257186,\n",
       "  0.8348968105065666,\n",
       "  0.6625,\n",
       "  0.7805555555555556,\n",
       "  0.638085742771685,\n",
       "  0.7286914765906363,\n",
       "  0.6436672967863895,\n",
       "  0.7456242707117853,\n",
       "  0.6048582995951417,\n",
       "  0.638623326959847,\n",
       "  0.6825396825396826,\n",
       "  0.6218074656188605,\n",
       "  0.8184110970996217,\n",
       "  0.6407407407407407,\n",
       "  0.7861557478368356,\n",
       "  0.6906906906906907,\n",
       "  0.7931034482758621,\n",
       "  0.6306382978723404,\n",
       "  0.7838616714697406,\n",
       "  0.6069268829026938,\n",
       "  0.6307692307692307,\n",
       "  0.6472237801458216,\n",
       "  0.6215098241985523,\n",
       "  0.5824590851867394,\n",
       "  0.5997292418772563,\n",
       "  0.6146341463414634,\n",
       "  0.6046296296296296,\n",
       "  0.6408952187182095,\n",
       "  0.6612819176654507,\n",
       "  0.6365492598264421,\n",
       "  0.6330136294800606,\n",
       "  0.5852828583581454,\n",
       "  0.8220211161387632,\n",
       "  0.6433878157503715,\n",
       "  0.6764539808018069,\n",
       "  0.7053775743707094,\n",
       "  0.7027176927343317,\n",
       "  0.6627188465499485,\n",
       "  0.7419731465265615,\n",
       "  0.6371997956055186,\n",
       "  0.707683419988446,\n",
       "  0.6403723664870161,\n",
       "  0.6101774042950514,\n",
       "  0.6110091743119266,\n",
       "  0.6467095497278575,\n",
       "  0.5848329048843187,\n",
       "  0.6420482520925652,\n",
       "  0.5881057268722467,\n",
       "  0.5911867364746946,\n",
       "  0.5856961489631825,\n",
       "  0.6392625809666168,\n",
       "  0.6338639652677279,\n",
       "  0.6074681238615665,\n",
       "  0.6859979101358412,\n",
       "  0.6422018348623854,\n",
       "  0.65249343832021,\n",
       "  0.7182352941176471,\n",
       "  0.5884133160397752,\n",
       "  0.6124314442413162,\n",
       "  0.6272684969753374,\n",
       "  0.7262313860252004,\n",
       "  0.6071103008204193,\n",
       "  0.6110357304387155,\n",
       "  0.5984251968503937,\n",
       "  0.6552917903066271,\n",
       "  0.5967741935483871,\n",
       "  0.6023287057769816,\n",
       "  0.7116329874385582,\n",
       "  0.6028558679161089,\n",
       "  0.598159509202454,\n",
       "  0.5963541666666666,\n",
       "  0.6038756196484903,\n",
       "  0.6612745098039216,\n",
       "  0.7305669199298656,\n",
       "  0.6361123906034085,\n",
       "  0.6064773735581189,\n",
       "  0.663466397170288,\n",
       "  0.7447916666666666,\n",
       "  0.7588443396226415,\n",
       "  0.6869652742828385,\n",
       "  0.6940806705081194,\n",
       "  0.7433061699650757,\n",
       "  0.75,\n",
       "  0.6381395348837209,\n",
       "  0.6346793349168646,\n",
       "  0.664624808575804,\n",
       "  0.6701701701701702,\n",
       "  0.6266907123534716,\n",
       "  0.680119581464873,\n",
       "  0.6514795678722405,\n",
       "  0.8976377952755905,\n",
       "  0.9354575163398693,\n",
       "  0.929481733220051,\n",
       "  0.9152046783625731,\n",
       "  0.9073275862068966,\n",
       "  0.9108554996405464,\n",
       "  0.8836120401337793,\n",
       "  0.9219586840091814,\n",
       "  0.930715935334873,\n",
       "  0.865979381443299,\n",
       "  0.9137426900584795,\n",
       "  0.9106382978723404,\n",
       "  0.9236471460340994,\n",
       "  0.8884353741496599,\n",
       "  0.9052558782849239,\n",
       "  0.8961748633879781,\n",
       "  0.859493670886076,\n",
       "  0.9290617848970252,\n",
       "  0.9155027932960894,\n",
       "  0.9237410071942446,\n",
       "  0.9418070444104135,\n",
       "  0.912033779028853,\n",
       "  0.9179743223965763,\n",
       "  0.9301634472511144,\n",
       "  0.9227967953386744,\n",
       "  0.9265129682997119,\n",
       "  0.92166549047283,\n",
       "  0.9405286343612335,\n",
       "  0.9282160625444208,\n",
       "  0.9405204460966543,\n",
       "  0.9202279202279202,\n",
       "  0.9043419267299865,\n",
       "  0.935832732516222,\n",
       "  0.9426229508196722,\n",
       "  0.9434389140271493,\n",
       "  0.9155027932960894,\n",
       "  0.9380987472365512,\n",
       "  0.9098417068134893,\n",
       "  0.9367272727272727,\n",
       "  0.9518796992481203,\n",
       "  0.9376832844574781,\n",
       "  0.9366998577524893,\n",
       "  0.9403534609720177,\n",
       "  0.9388928828181164,\n",
       "  0.9328621908127208,\n",
       "  0.9491778774289985,\n",
       "  0.9474885844748858,\n",
       "  0.9512743628185907,\n",
       "  0.9390243902439024,\n",
       "  0.9377236936292055,\n",
       "  0.9331456720619282,\n",
       "  0.9464809384164223,\n",
       "  0.921124828532236,\n",
       "  0.9499263622974963,\n",
       "  0.9486803519061584,\n",
       "  0.9278350515463918,\n",
       "  0.9511645379413974,\n",
       "  0.9472140762463344,\n",
       "  0.8987935656836461,\n",
       "  0.940622737146995,\n",
       "  0.9497413155949741,\n",
       "  0.9292786421499293,\n",
       "  0.9544776119402985,\n",
       "  0.9483636363636364,\n",
       "  0.942070963070239,\n",
       "  0.9465703971119134,\n",
       "  0.9535740604274134,\n",
       "  0.9480994152046783,\n",
       "  0.9696,\n",
       "  0.9504071058475203,\n",
       "  0.9583023082650782,\n",
       "  0.940622737146995,\n",
       "  0.95,\n",
       "  0.9531135531135531,\n",
       "  0.9446441409058232,\n",
       "  0.9357990230286113,\n",
       "  0.9417613636363636,\n",
       "  0.9408795962509012,\n",
       "  0.9356890459363958,\n",
       "  0.9289693593314763,\n",
       "  0.9515062454077884,\n",
       "  0.93125,\n",
       "  0.9495614035087719,\n",
       "  0.9510086455331412,\n",
       "  0.96484375,\n",
       "  0.9524158125915081,\n",
       "  0.9472161966738973,\n",
       "  0.9407988587731811,\n",
       "  0.9538799414348462,\n",
       "  0.958364312267658,\n",
       "  0.9537444933920705,\n",
       "  0.948644793152639,\n",
       "  0.928072625698324,\n",
       "  0.9524838012958964,\n",
       "  0.9350741002117149,\n",
       "  0.9569486404833837,\n",
       "  0.963076923076923,\n",
       "  0.9443651925820257,\n",
       "  0.9396735273243435,\n",
       "  0.9574468085106383,\n",
       "  0.9656518345042935,\n",
       "  0.9625468164794008,\n",
       "  0.9569093610698366,\n",
       "  0.9492537313432836,\n",
       "  0.9606656580937972,\n",
       "  0.9568397951719093,\n",
       "  0.9352170916609235,\n",
       "  0.965464313123561,\n",
       "  0.9670588235294117,\n",
       "  0.9548104956268222,\n",
       "  0.9532710280373832,\n",
       "  0.9531024531024531,\n",
       "  0.9580573951434879,\n",
       "  0.9605947955390335,\n",
       "  0.9571322985957132,\n",
       "  0.9574621485219899,\n",
       "  0.9466571834992887,\n",
       "  0.9436222692036645,\n",
       "  0.9634146341463414,\n",
       "  0.9510591672753835,\n",
       "  0.9494584837545126,\n",
       "  0.9530346820809249,\n",
       "  0.9530685920577617,\n",
       "  0.9602356406480118,\n",
       "  0.9533187454412837,\n",
       "  0.9529667149059334,\n",
       "  0.9556686046511628,\n",
       "  0.9601769911504425,\n",
       "  0.9620535714285714,\n",
       "  0.9634055265123226,\n",
       "  0.9588537839823659,\n",
       "  0.9622641509433962,\n",
       "  0.9566473988439307,\n",
       "  0.9520648967551623,\n",
       "  0.9596774193548387,\n",
       "  0.9539568345323741,\n",
       "  0.9570284049526584,\n",
       "  0.959479015918958,\n",
       "  0.9531700288184438,\n",
       "  0.9636767976278725,\n",
       "  0.9571843251088534,\n",
       "  0.9565217391304348,\n",
       "  0.9297036526533425,\n",
       "  0.9652567975830816,\n",
       "  0.9539568345323741,\n",
       "  0.9577259475218659,\n",
       "  0.9474431818181818,\n",
       "  0.9541484716157205,\n",
       "  0.9540816326530612,\n",
       "  0.9536567704561911,\n",
       "  0.9657483246463142,\n",
       "  0.9645232815964523,\n",
       "  0.9576642335766423,\n",
       "  0.9506084466714388,\n",
       "  0.9623493975903614,\n",
       "  0.9656460044809559,\n",
       "  0.9465165376495426,\n",
       "  0.9447938504542278],\n",
       " 'ap_val_epoch': [0.28521507428466675,\n",
       "  0.3439956815729829,\n",
       "  0.3736314328606837,\n",
       "  0.3663864287293422,\n",
       "  0.4066620310508925,\n",
       "  0.3822634750890649,\n",
       "  0.4038669088738163,\n",
       "  0.3807260176706464,\n",
       "  0.39848142164066147,\n",
       "  0.39233784546730677,\n",
       "  0.43792502877766837,\n",
       "  0.40383967253687775,\n",
       "  0.42256917606760014,\n",
       "  0.3907470022995844,\n",
       "  0.41410875876123066,\n",
       "  0.40979445943100345,\n",
       "  0.4008901651181103,\n",
       "  0.4299284869030995,\n",
       "  0.4065386093912357,\n",
       "  0.4393940188776594,\n",
       "  0.4721457371238118,\n",
       "  0.5582633651016368,\n",
       "  0.5643016637766443,\n",
       "  0.5553531462353206,\n",
       "  0.5717927288712021,\n",
       "  0.6179038373885525,\n",
       "  0.6220984762092829,\n",
       "  0.5960265945338787,\n",
       "  0.6151208598116521,\n",
       "  0.6043230478012735,\n",
       "  0.6035243821620421,\n",
       "  0.6075307885072343,\n",
       "  0.5977135781682653,\n",
       "  0.6502178576103822,\n",
       "  0.5364189241215103,\n",
       "  0.6258526030957765,\n",
       "  0.5787747442278476,\n",
       "  0.6113993454513911,\n",
       "  0.6244847551910819,\n",
       "  0.6271156358531453,\n",
       "  0.6266400842131922,\n",
       "  0.6051488165381487,\n",
       "  0.604031271607451,\n",
       "  0.633239523620483,\n",
       "  0.6284645847813288,\n",
       "  0.6486411014161126,\n",
       "  0.6449112048034003,\n",
       "  0.6673752669805444,\n",
       "  0.6455428178462624,\n",
       "  0.6537627594019622,\n",
       "  0.6512443513637785,\n",
       "  0.672647621189903,\n",
       "  0.632218804270887,\n",
       "  0.6461627119576561,\n",
       "  0.6576826418300293,\n",
       "  0.6558791965723147,\n",
       "  0.6649194059665369,\n",
       "  0.6182144947315982,\n",
       "  0.6055366825956044,\n",
       "  0.6694838704199646,\n",
       "  0.6583465885580149,\n",
       "  0.6723592383849422,\n",
       "  0.6498178548725435,\n",
       "  0.6633469171535318,\n",
       "  0.6742031232212524,\n",
       "  0.6803102428172294,\n",
       "  0.6610824372238086,\n",
       "  0.6742731742113104,\n",
       "  0.6730148328195774,\n",
       "  0.6416646510824033,\n",
       "  0.6710130180254507,\n",
       "  0.6802698060163672,\n",
       "  0.6835001728069092,\n",
       "  0.663697059865413,\n",
       "  0.6727161867612907,\n",
       "  0.6304492785649293,\n",
       "  0.6931685190182716,\n",
       "  0.6930388634047655,\n",
       "  0.6590958538894065,\n",
       "  0.6635166261201911,\n",
       "  0.6546176147604488,\n",
       "  0.6974653412694581,\n",
       "  0.6669083225835417,\n",
       "  0.6519415483707485,\n",
       "  0.667819988012439,\n",
       "  0.6932132180498609,\n",
       "  0.6886865593025402,\n",
       "  0.6526285850958036,\n",
       "  0.6728335779828583,\n",
       "  0.7045592072032087,\n",
       "  0.7046519748606199,\n",
       "  0.7131797764050992,\n",
       "  0.7448684286822782,\n",
       "  0.7022023946257879,\n",
       "  0.6728139832790205,\n",
       "  0.7586989938926412,\n",
       "  0.7598668749427855,\n",
       "  0.7629169661248205,\n",
       "  0.7759635791658162,\n",
       "  0.7362565024652553,\n",
       "  0.7331488746382496,\n",
       "  0.7889922334563273,\n",
       "  0.7509716903043984,\n",
       "  0.7702015249187784,\n",
       "  0.7346422597534362,\n",
       "  0.7612383545339707,\n",
       "  0.7789026558326648,\n",
       "  0.7727629154282806,\n",
       "  0.7976549620773052,\n",
       "  0.752728563726272,\n",
       "  0.807330694231591,\n",
       "  0.814890623000073,\n",
       "  0.8174821261975438,\n",
       "  0.8262783803152728,\n",
       "  0.8192261166666635,\n",
       "  0.822545533679358,\n",
       "  0.84288265483169,\n",
       "  0.8467051524189216,\n",
       "  0.8423194736803941,\n",
       "  0.8514885170020083,\n",
       "  0.8570850456928241,\n",
       "  0.8400411973997592,\n",
       "  0.8461745676493704,\n",
       "  0.8623186137554854,\n",
       "  0.8507170506459747,\n",
       "  0.823778521809276,\n",
       "  0.8466207680083507,\n",
       "  0.8388351121521219,\n",
       "  0.8502197902398496,\n",
       "  0.8410827955536356,\n",
       "  0.8479461695538489,\n",
       "  0.8390842222490783,\n",
       "  0.8327054659700249,\n",
       "  0.863781112178904,\n",
       "  0.8518327452850909,\n",
       "  0.8555876692903623,\n",
       "  0.8537214137453704,\n",
       "  0.8447767559981306,\n",
       "  0.838737923513813,\n",
       "  0.851768900425082,\n",
       "  0.8597208055392284,\n",
       "  0.8546903991414022,\n",
       "  0.8668014949084848,\n",
       "  0.8654359150478713,\n",
       "  0.8771242249270995,\n",
       "  0.8619799462005662,\n",
       "  0.8536106398665404,\n",
       "  0.8820639613383167,\n",
       "  0.8275224928942854,\n",
       "  0.8511944381201265,\n",
       "  0.8508747682068625,\n",
       "  0.8442786498419845,\n",
       "  0.8544349484477934,\n",
       "  0.8677806673665299,\n",
       "  0.8438673434569877,\n",
       "  0.8669850554421699,\n",
       "  0.8521780026168821,\n",
       "  0.8639076167288416,\n",
       "  0.8588085097310316,\n",
       "  0.8605874358799681,\n",
       "  0.8177690773235766,\n",
       "  0.8549236047810439,\n",
       "  0.8580340503815995,\n",
       "  0.8488695212676491,\n",
       "  0.8614757067456873,\n",
       "  0.8641730731807412,\n",
       "  0.8685703871056396,\n",
       "  0.865818231635438,\n",
       "  0.8724073956342686,\n",
       "  0.8571813237407868,\n",
       "  0.8720811833485724,\n",
       "  0.861697385575973,\n",
       "  0.8487863527618013,\n",
       "  0.8736270653519449,\n",
       "  0.8670998500617666,\n",
       "  0.8731421294887104,\n",
       "  0.8185551696800409,\n",
       "  0.8543187489252887,\n",
       "  0.8660945313965077,\n",
       "  0.8787808792711922,\n",
       "  0.8707054103335111,\n",
       "  0.8615604424718979,\n",
       "  0.8630584378710928,\n",
       "  0.8726240991669637,\n",
       "  0.8606599356553404,\n",
       "  0.8748054965864536,\n",
       "  0.8680307123498726,\n",
       "  0.8426226188697776,\n",
       "  0.8421914987676165,\n",
       "  0.8746248214393532,\n",
       "  0.8737899811638821,\n",
       "  0.8701533734688005,\n",
       "  0.828766390735135,\n",
       "  0.8549037243774009,\n",
       "  0.8603668899414609,\n",
       "  0.856254701769467,\n",
       "  0.8384494006892225,\n",
       "  0.8658843645455353,\n",
       "  0.8856673250820776,\n",
       "  0.8395774508461832,\n",
       "  0.836560992445275,\n",
       "  0.8652132572358476,\n",
       "  0.8705934550020966,\n",
       "  0.8597879489194363,\n",
       "  0.8555885040268688,\n",
       "  0.8595323103573422,\n",
       "  0.8632421515849152,\n",
       "  0.8746852530633971,\n",
       "  0.880908563579247,\n",
       "  0.8834413032155167,\n",
       "  0.849999252024892,\n",
       "  0.8590860420965497,\n",
       "  0.8680341782755026,\n",
       "  0.869930418860873,\n",
       "  0.8712654741413045,\n",
       "  0.8664450626010989,\n",
       "  0.8618163812161898,\n",
       "  0.8679414606650757,\n",
       "  0.8758406595716044,\n",
       "  0.876476417599629,\n",
       "  0.8687652474933082,\n",
       "  0.8577780321834847,\n",
       "  0.8631544037735075,\n",
       "  0.8683978342548323,\n",
       "  0.8797206026036902,\n",
       "  0.8542273244170251,\n",
       "  0.8717751611245138,\n",
       "  0.8791693980167611,\n",
       "  0.874854600991087,\n",
       "  0.8767867955763433,\n",
       "  0.8774599101742412,\n",
       "  0.8627788767740416,\n",
       "  0.8717553547229152,\n",
       "  0.8716701101163702,\n",
       "  0.885139898413569,\n",
       "  0.8601715607270438,\n",
       "  0.868934364886105,\n",
       "  0.8711370611484741,\n",
       "  0.8809199522923895,\n",
       "  0.8475259554212842,\n",
       "  0.861172660847243,\n",
       "  0.8731726841169002,\n",
       "  0.864185278027682,\n",
       "  0.8651428664607208,\n",
       "  0.8781643486077233,\n",
       "  0.8761363922227533,\n",
       "  0.8685542647332004,\n",
       "  0.859152608231074,\n",
       "  0.8757927445917186,\n",
       "  0.876041065820842],\n",
       " 'ap_val_epoch_pl': [0.18394697854982714,\n",
       "  0.2515998290355007,\n",
       "  0.30488050734432187,\n",
       "  0.32148826468060887,\n",
       "  0.36646311550096283,\n",
       "  0.3454924618321844,\n",
       "  0.3593180782947918,\n",
       "  0.34877141364998626,\n",
       "  0.34537307854124144,\n",
       "  0.348941662819074,\n",
       "  0.36821240176705594,\n",
       "  0.35705510960745707,\n",
       "  0.3662048486194314,\n",
       "  0.3490170373608416,\n",
       "  0.3598034439249608,\n",
       "  0.3622875566333343,\n",
       "  0.3727499774466211,\n",
       "  0.39639395164941177,\n",
       "  0.3970611516129027,\n",
       "  0.4284833755186845,\n",
       "  0.5157934820386555,\n",
       "  0.6253707184395468,\n",
       "  0.7105954811591486,\n",
       "  0.7348714948718343,\n",
       "  0.7503087247791155,\n",
       "  0.7173192008539886,\n",
       "  0.7465195513980728,\n",
       "  0.7590181686040068,\n",
       "  0.7685340378547026,\n",
       "  0.7859704242953451,\n",
       "  0.7690358449938929,\n",
       "  0.7926490268597386,\n",
       "  0.7933102531202704,\n",
       "  0.7827640160646421,\n",
       "  0.7673530032690239,\n",
       "  0.8183918149037019,\n",
       "  0.7800628175733826,\n",
       "  0.8234626996237415,\n",
       "  0.8243304416781737,\n",
       "  0.8224507762113958,\n",
       "  0.8151493851691877,\n",
       "  0.7895582665036076,\n",
       "  0.806036986075878,\n",
       "  0.818492287592505,\n",
       "  0.8187191552274753,\n",
       "  0.828868047886593,\n",
       "  0.8298909774306221,\n",
       "  0.8295597864443568,\n",
       "  0.8168779732533994,\n",
       "  0.8042558503893403,\n",
       "  0.8267135156998235,\n",
       "  0.8436104823229064,\n",
       "  0.827837479553157,\n",
       "  0.8466937796292421,\n",
       "  0.8375102589762017,\n",
       "  0.8451945810989281,\n",
       "  0.8485050571814401,\n",
       "  0.8039920849637099,\n",
       "  0.8225725683001002,\n",
       "  0.8443883488903217,\n",
       "  0.8167158388591885,\n",
       "  0.8575678256182415,\n",
       "  0.851638126596262,\n",
       "  0.8404541523023644,\n",
       "  0.8685078306312483,\n",
       "  0.8359608277126899,\n",
       "  0.8278813294382076,\n",
       "  0.8437566006607663,\n",
       "  0.8434023315825322,\n",
       "  0.8553198401512035,\n",
       "  0.8256119509578523,\n",
       "  0.8578558803948749,\n",
       "  0.8578590078071897,\n",
       "  0.8266193890183164,\n",
       "  0.858787268603442,\n",
       "  0.8209009745974375,\n",
       "  0.8766897958966159,\n",
       "  0.8978933866439046,\n",
       "  0.8581355504071139,\n",
       "  0.876709558698503,\n",
       "  0.8664948326626145,\n",
       "  0.8897960347488605,\n",
       "  0.8545365724018926,\n",
       "  0.8486540507880131,\n",
       "  0.872465386045902,\n",
       "  0.8764903444424863,\n",
       "  0.8768170840085556,\n",
       "  0.8325966095581974,\n",
       "  0.819082943235366,\n",
       "  0.8791277759148877,\n",
       "  0.8962252029577673,\n",
       "  0.8724214289208087,\n",
       "  0.8573090010018501,\n",
       "  0.8032676011584016,\n",
       "  0.7911892414638596,\n",
       "  0.8791403070957022,\n",
       "  0.900540578805928,\n",
       "  0.8788904807977733,\n",
       "  0.9000173825668702,\n",
       "  0.8663719304260019,\n",
       "  0.8646138752926025,\n",
       "  0.9098191550480046,\n",
       "  0.8654239628898126,\n",
       "  0.8965709217917157,\n",
       "  0.8457074541446731,\n",
       "  0.8674719966289096,\n",
       "  0.9135110514856181,\n",
       "  0.8913118030817241,\n",
       "  0.9023566504174533,\n",
       "  0.902098374464414,\n",
       "  0.9030938556090131,\n",
       "  0.8912558339370737,\n",
       "  0.8970184832427854,\n",
       "  0.8820883524343561,\n",
       "  0.8700882776838452,\n",
       "  0.8876161649784655,\n",
       "  0.9089618938550348,\n",
       "  0.9091749564153287,\n",
       "  0.910440523933476,\n",
       "  0.9179577508439931,\n",
       "  0.9135779219140525,\n",
       "  0.8931914080120614,\n",
       "  0.9200509443025301,\n",
       "  0.912265352427753,\n",
       "  0.9000757617602901,\n",
       "  0.8684076051932982,\n",
       "  0.9020972827222046,\n",
       "  0.8625641675735449,\n",
       "  0.9097599382432626,\n",
       "  0.8595600762267114,\n",
       "  0.8766843200372807,\n",
       "  0.8699591807659305,\n",
       "  0.8676023195919228,\n",
       "  0.9050108742030197,\n",
       "  0.9097296941329299,\n",
       "  0.8949147874355001,\n",
       "  0.8849995039336036,\n",
       "  0.8906350051220984,\n",
       "  0.8972278755445416,\n",
       "  0.9026608588240773,\n",
       "  0.8892619363034879,\n",
       "  0.8902095770942259,\n",
       "  0.893535138491517,\n",
       "  0.9060806087313757,\n",
       "  0.9092334510533613,\n",
       "  0.9034225564115965,\n",
       "  0.8807617055003315,\n",
       "  0.9161293141790736,\n",
       "  0.8447903944030358,\n",
       "  0.8857509914157443,\n",
       "  0.8599008241186522,\n",
       "  0.8629994432381755,\n",
       "  0.8814571932714431,\n",
       "  0.9101167902856508,\n",
       "  0.8623954311439278,\n",
       "  0.8906455234050031,\n",
       "  0.894015615855202,\n",
       "  0.894133969979574,\n",
       "  0.8765760675864921,\n",
       "  0.8779135897826807,\n",
       "  0.8626702410584337,\n",
       "  0.9057679843077961,\n",
       "  0.8872812712108111,\n",
       "  0.8682095122549478,\n",
       "  0.8793281805234081,\n",
       "  0.8887875973272458,\n",
       "  0.8998039173698403,\n",
       "  0.8704338636460466,\n",
       "  0.8907624868771007,\n",
       "  0.871712261063765,\n",
       "  0.8961652520259584,\n",
       "  0.8771508693142775,\n",
       "  0.872897404190373,\n",
       "  0.8891778883159706,\n",
       "  0.9013851339779699,\n",
       "  0.8860969704422093,\n",
       "  0.8270165610533102,\n",
       "  0.914412182632362,\n",
       "  0.8965196484818647,\n",
       "  0.9102108240882685,\n",
       "  0.8960655079005675,\n",
       "  0.8823811381451291,\n",
       "  0.9037251103584009,\n",
       "  0.8734484619438636,\n",
       "  0.8810478630575718,\n",
       "  0.8827177669474846,\n",
       "  0.8959787476750581,\n",
       "  0.8513896382684969,\n",
       "  0.8712735710645633,\n",
       "  0.9136210828524665,\n",
       "  0.9093196260401896,\n",
       "  0.8932875634441999,\n",
       "  0.8571719301624875,\n",
       "  0.8758419005289231,\n",
       "  0.8956929880535133,\n",
       "  0.8981367773727951,\n",
       "  0.8590907799022958,\n",
       "  0.8779162221201204,\n",
       "  0.9036910385199267,\n",
       "  0.8584959153688921,\n",
       "  0.8741345464425502,\n",
       "  0.8796724531621181,\n",
       "  0.8821669705930242,\n",
       "  0.8652543839597373,\n",
       "  0.886814641234146,\n",
       "  0.880351836350032,\n",
       "  0.8715618779797497,\n",
       "  0.885653177308266,\n",
       "  0.9019078750517735,\n",
       "  0.9041342806943207,\n",
       "  0.8652876858972528,\n",
       "  0.8636193277291949,\n",
       "  0.8865595380596248,\n",
       "  0.897947047210865,\n",
       "  0.880297626160909,\n",
       "  0.886203228477704,\n",
       "  0.8754617731645234,\n",
       "  0.8730836221468268,\n",
       "  0.892079347030899,\n",
       "  0.8995219472376116,\n",
       "  0.8875155778198782,\n",
       "  0.8689321803334461,\n",
       "  0.8611177819712531,\n",
       "  0.8674827274944703,\n",
       "  0.890226700339465,\n",
       "  0.8723853627448516,\n",
       "  0.8724882926482219,\n",
       "  0.8854372195350987,\n",
       "  0.8844665215770663,\n",
       "  0.8806780973686986,\n",
       "  0.8889658916571761,\n",
       "  0.8658395942940992,\n",
       "  0.8697272218548034,\n",
       "  0.878677218957178,\n",
       "  0.9086126167160313,\n",
       "  0.8809368765532523,\n",
       "  0.8760891237680972,\n",
       "  0.877021527854426,\n",
       "  0.8858314550082804,\n",
       "  0.8280056156782186,\n",
       "  0.8617260015433305,\n",
       "  0.8863734139319377,\n",
       "  0.8812100579034918,\n",
       "  0.875552370698831,\n",
       "  0.8930272804089355,\n",
       "  0.8846092212692204,\n",
       "  0.8861448618419498,\n",
       "  0.865724473403974,\n",
       "  0.8857812197718133,\n",
       "  0.8729424191907406],\n",
       " 'gt_val_final': array([2, 0, 0, ..., 0, 0, 0]),\n",
       " 'loss_train_epoch': [array([0.06469619], dtype=float32),\n",
       "  array([0.06373382], dtype=float32),\n",
       "  array([0.06264745], dtype=float32),\n",
       "  array([0.06198553], dtype=float32),\n",
       "  array([0.06146203], dtype=float32),\n",
       "  array([0.06135869], dtype=float32),\n",
       "  array([0.06126989], dtype=float32),\n",
       "  array([0.061134], dtype=float32),\n",
       "  array([0.06094045], dtype=float32),\n",
       "  array([0.06093868], dtype=float32),\n",
       "  array([0.0609144], dtype=float32),\n",
       "  array([0.0609325], dtype=float32),\n",
       "  array([0.06070621], dtype=float32),\n",
       "  array([0.06076922], dtype=float32),\n",
       "  array([0.0607646], dtype=float32),\n",
       "  array([0.06074787], dtype=float32),\n",
       "  array([0.06063214], dtype=float32),\n",
       "  array([0.06060062], dtype=float32),\n",
       "  array([0.06041015], dtype=float32),\n",
       "  array([0.06031683], dtype=float32),\n",
       "  array([0.06005596], dtype=float32),\n",
       "  array([0.05969358], dtype=float32),\n",
       "  array([0.05886923], dtype=float32),\n",
       "  array([0.05839719], dtype=float32),\n",
       "  array([0.05809754], dtype=float32),\n",
       "  array([0.05804874], dtype=float32),\n",
       "  array([0.05761594], dtype=float32),\n",
       "  array([0.05752151], dtype=float32),\n",
       "  array([0.05734141], dtype=float32),\n",
       "  array([0.05723904], dtype=float32),\n",
       "  array([0.05735165], dtype=float32),\n",
       "  array([0.0568834], dtype=float32),\n",
       "  array([0.05688505], dtype=float32),\n",
       "  array([0.05673376], dtype=float32),\n",
       "  array([0.05685159], dtype=float32),\n",
       "  array([0.05660829], dtype=float32),\n",
       "  array([0.05654791], dtype=float32),\n",
       "  array([0.0565092], dtype=float32),\n",
       "  array([0.05645696], dtype=float32),\n",
       "  array([0.05632725], dtype=float32),\n",
       "  array([0.05636491], dtype=float32),\n",
       "  array([0.05622271], dtype=float32),\n",
       "  array([0.05623309], dtype=float32),\n",
       "  array([0.05606969], dtype=float32),\n",
       "  array([0.05616293], dtype=float32),\n",
       "  array([0.05602441], dtype=float32),\n",
       "  array([0.05603937], dtype=float32),\n",
       "  array([0.05606027], dtype=float32),\n",
       "  array([0.05609339], dtype=float32),\n",
       "  array([0.0559586], dtype=float32),\n",
       "  array([0.05593015], dtype=float32),\n",
       "  array([0.05597208], dtype=float32),\n",
       "  array([0.05584459], dtype=float32),\n",
       "  array([0.05576869], dtype=float32),\n",
       "  array([0.05576964], dtype=float32),\n",
       "  array([0.05582568], dtype=float32),\n",
       "  array([0.05576043], dtype=float32),\n",
       "  array([0.05569581], dtype=float32),\n",
       "  array([0.05580051], dtype=float32),\n",
       "  array([0.05574854], dtype=float32),\n",
       "  array([0.05569075], dtype=float32),\n",
       "  array([0.05554982], dtype=float32),\n",
       "  array([0.05571873], dtype=float32),\n",
       "  array([0.05561073], dtype=float32),\n",
       "  array([0.05567883], dtype=float32),\n",
       "  array([0.05552559], dtype=float32),\n",
       "  array([0.055555], dtype=float32),\n",
       "  array([0.05557511], dtype=float32),\n",
       "  array([0.05547032], dtype=float32),\n",
       "  array([0.05537043], dtype=float32),\n",
       "  array([0.05542139], dtype=float32),\n",
       "  array([0.05536586], dtype=float32),\n",
       "  array([0.05525862], dtype=float32),\n",
       "  array([0.0553046], dtype=float32),\n",
       "  array([0.05513309], dtype=float32),\n",
       "  array([0.05523809], dtype=float32),\n",
       "  array([0.05522016], dtype=float32),\n",
       "  array([0.05534894], dtype=float32),\n",
       "  array([0.05523056], dtype=float32),\n",
       "  array([0.05518175], dtype=float32),\n",
       "  array([0.05522589], dtype=float32),\n",
       "  array([0.05511799], dtype=float32),\n",
       "  array([0.05511352], dtype=float32),\n",
       "  array([0.05502049], dtype=float32),\n",
       "  array([0.05500438], dtype=float32),\n",
       "  array([0.05512268], dtype=float32),\n",
       "  array([0.05501422], dtype=float32),\n",
       "  array([0.0551294], dtype=float32),\n",
       "  array([0.05484927], dtype=float32),\n",
       "  array([0.05496396], dtype=float32),\n",
       "  array([0.05491874], dtype=float32),\n",
       "  array([0.05483478], dtype=float32),\n",
       "  array([0.05352361], dtype=float32),\n",
       "  array([0.05275879], dtype=float32),\n",
       "  array([0.05280634], dtype=float32),\n",
       "  array([0.05256498], dtype=float32),\n",
       "  array([0.05257751], dtype=float32),\n",
       "  array([0.05258638], dtype=float32),\n",
       "  array([0.05246727], dtype=float32),\n",
       "  array([0.05225778], dtype=float32),\n",
       "  array([0.0523753], dtype=float32),\n",
       "  array([0.05228407], dtype=float32),\n",
       "  array([0.05211691], dtype=float32),\n",
       "  array([0.05208666], dtype=float32),\n",
       "  array([0.05199637], dtype=float32),\n",
       "  array([0.05207995], dtype=float32),\n",
       "  array([0.05215377], dtype=float32),\n",
       "  array([0.05189672], dtype=float32),\n",
       "  array([0.05189968], dtype=float32),\n",
       "  array([0.05193179], dtype=float32),\n",
       "  array([0.0517804], dtype=float32),\n",
       "  array([0.05165985], dtype=float32),\n",
       "  array([0.05157641], dtype=float32),\n",
       "  array([0.05130109], dtype=float32),\n",
       "  array([0.05120221], dtype=float32),\n",
       "  array([0.05112302], dtype=float32),\n",
       "  array([0.05115446], dtype=float32),\n",
       "  array([0.05107667], dtype=float32),\n",
       "  array([0.05090266], dtype=float32),\n",
       "  array([0.05082864], dtype=float32),\n",
       "  array([0.0507774], dtype=float32),\n",
       "  array([0.05080677], dtype=float32),\n",
       "  array([0.05070893], dtype=float32),\n",
       "  array([0.05059911], dtype=float32),\n",
       "  array([0.05057236], dtype=float32),\n",
       "  array([0.05056655], dtype=float32),\n",
       "  array([0.05048355], dtype=float32),\n",
       "  array([0.0507028], dtype=float32),\n",
       "  array([0.05057467], dtype=float32),\n",
       "  array([0.05051344], dtype=float32),\n",
       "  array([0.05045064], dtype=float32),\n",
       "  array([0.05042113], dtype=float32),\n",
       "  array([0.0503249], dtype=float32),\n",
       "  array([0.05037213], dtype=float32),\n",
       "  array([0.05042056], dtype=float32),\n",
       "  array([0.05029697], dtype=float32),\n",
       "  array([0.05020544], dtype=float32),\n",
       "  array([0.05028122], dtype=float32),\n",
       "  array([0.05019729], dtype=float32),\n",
       "  array([0.05021359], dtype=float32),\n",
       "  array([0.05019803], dtype=float32),\n",
       "  array([0.05018596], dtype=float32),\n",
       "  array([0.05028035], dtype=float32),\n",
       "  array([0.05021939], dtype=float32),\n",
       "  array([0.05007245], dtype=float32),\n",
       "  array([0.05008709], dtype=float32),\n",
       "  array([0.05007677], dtype=float32),\n",
       "  array([0.05016929], dtype=float32),\n",
       "  array([0.05012258], dtype=float32),\n",
       "  array([0.05000248], dtype=float32),\n",
       "  array([0.05000895], dtype=float32),\n",
       "  array([0.05008129], dtype=float32),\n",
       "  array([0.05007014], dtype=float32),\n",
       "  array([0.04992162], dtype=float32),\n",
       "  array([0.04997325], dtype=float32),\n",
       "  array([0.04996628], dtype=float32),\n",
       "  array([0.05002583], dtype=float32),\n",
       "  array([0.05008638], dtype=float32),\n",
       "  array([0.04993836], dtype=float32),\n",
       "  array([0.04987685], dtype=float32),\n",
       "  array([0.04979837], dtype=float32),\n",
       "  array([0.04989589], dtype=float32),\n",
       "  array([0.04991617], dtype=float32),\n",
       "  array([0.04991595], dtype=float32),\n",
       "  array([0.0497647], dtype=float32),\n",
       "  array([0.04985273], dtype=float32),\n",
       "  array([0.04964525], dtype=float32),\n",
       "  array([0.04983859], dtype=float32),\n",
       "  array([0.04971278], dtype=float32),\n",
       "  array([0.04970149], dtype=float32),\n",
       "  array([0.04980286], dtype=float32),\n",
       "  array([0.04978329], dtype=float32),\n",
       "  array([0.0496268], dtype=float32),\n",
       "  array([0.04975552], dtype=float32),\n",
       "  array([0.04982587], dtype=float32),\n",
       "  array([0.0496765], dtype=float32),\n",
       "  array([0.04974946], dtype=float32),\n",
       "  array([0.04961825], dtype=float32),\n",
       "  array([0.04966646], dtype=float32),\n",
       "  array([0.04969615], dtype=float32),\n",
       "  array([0.04963434], dtype=float32),\n",
       "  array([0.04966158], dtype=float32),\n",
       "  array([0.04961877], dtype=float32),\n",
       "  array([0.04955617], dtype=float32),\n",
       "  array([0.049613], dtype=float32),\n",
       "  array([0.04962581], dtype=float32),\n",
       "  array([0.04949614], dtype=float32),\n",
       "  array([0.04969583], dtype=float32),\n",
       "  array([0.0495162], dtype=float32),\n",
       "  array([0.04954353], dtype=float32),\n",
       "  array([0.04950235], dtype=float32),\n",
       "  array([0.04950515], dtype=float32),\n",
       "  array([0.04955963], dtype=float32),\n",
       "  array([0.04943294], dtype=float32),\n",
       "  array([0.04954619], dtype=float32),\n",
       "  array([0.04945171], dtype=float32),\n",
       "  array([0.04948572], dtype=float32),\n",
       "  array([0.04954962], dtype=float32),\n",
       "  array([0.04954151], dtype=float32),\n",
       "  array([0.04943826], dtype=float32),\n",
       "  array([0.04945125], dtype=float32),\n",
       "  array([0.04940579], dtype=float32),\n",
       "  array([0.04953372], dtype=float32),\n",
       "  array([0.04951712], dtype=float32),\n",
       "  array([0.04942042], dtype=float32),\n",
       "  array([0.0494097], dtype=float32),\n",
       "  array([0.04946461], dtype=float32),\n",
       "  array([0.04945899], dtype=float32),\n",
       "  array([0.04943232], dtype=float32),\n",
       "  array([0.04947644], dtype=float32),\n",
       "  array([0.04927104], dtype=float32),\n",
       "  array([0.0494625], dtype=float32),\n",
       "  array([0.04934383], dtype=float32),\n",
       "  array([0.04931736], dtype=float32),\n",
       "  array([0.04938582], dtype=float32),\n",
       "  array([0.04927011], dtype=float32),\n",
       "  array([0.04930621], dtype=float32),\n",
       "  array([0.04939218], dtype=float32),\n",
       "  array([0.04924483], dtype=float32),\n",
       "  array([0.04928484], dtype=float32),\n",
       "  array([0.04931105], dtype=float32),\n",
       "  array([0.04933063], dtype=float32),\n",
       "  array([0.04924479], dtype=float32),\n",
       "  array([0.04928579], dtype=float32),\n",
       "  array([0.04930224], dtype=float32),\n",
       "  array([0.04928445], dtype=float32),\n",
       "  array([0.04915537], dtype=float32),\n",
       "  array([0.04916452], dtype=float32),\n",
       "  array([0.04923727], dtype=float32),\n",
       "  array([0.04930605], dtype=float32),\n",
       "  array([0.04912169], dtype=float32),\n",
       "  array([0.04919492], dtype=float32),\n",
       "  array([0.04924877], dtype=float32),\n",
       "  array([0.04907866], dtype=float32),\n",
       "  array([0.04910673], dtype=float32),\n",
       "  array([0.04923299], dtype=float32),\n",
       "  array([0.04919527], dtype=float32),\n",
       "  array([0.04904678], dtype=float32),\n",
       "  array([0.04914853], dtype=float32),\n",
       "  array([0.04892351], dtype=float32),\n",
       "  array([0.04908927], dtype=float32),\n",
       "  array([0.04918609], dtype=float32),\n",
       "  array([0.04911393], dtype=float32),\n",
       "  array([0.04908838], dtype=float32),\n",
       "  array([0.04904321], dtype=float32),\n",
       "  array([0.04907207], dtype=float32),\n",
       "  array([0.04891874], dtype=float32),\n",
       "  array([0.04903378], dtype=float32),\n",
       "  array([0.04907274], dtype=float32),\n",
       "  array([0.04903217], dtype=float32)],\n",
       " 'loss_val_epoch': [0.022893877552854647,\n",
       "  0.0225198074197833,\n",
       "  0.022256935456670433,\n",
       "  0.02232337349070762,\n",
       "  0.02138518265771419,\n",
       "  0.0217850671076232,\n",
       "  0.021408015944392806,\n",
       "  0.02162391785158211,\n",
       "  0.021213337439928988,\n",
       "  0.021435616325981007,\n",
       "  0.020784235064443973,\n",
       "  0.02128848748673078,\n",
       "  0.021108107713650827,\n",
       "  0.02148416817906391,\n",
       "  0.02124629984578614,\n",
       "  0.021130109886567754,\n",
       "  0.021340344644773756,\n",
       "  0.02107570449032458,\n",
       "  0.02131967404121695,\n",
       "  0.020851013970023976,\n",
       "  0.020396587519920177,\n",
       "  0.01908155106795999,\n",
       "  0.0187881921668608,\n",
       "  0.01886238726267374,\n",
       "  0.01866255100153217,\n",
       "  0.01806692330234022,\n",
       "  0.01811456744131474,\n",
       "  0.018361059696958407,\n",
       "  0.01819161462336978,\n",
       "  0.01835083865736383,\n",
       "  0.01828404698505938,\n",
       "  0.018347271634552534,\n",
       "  0.01835708030894738,\n",
       "  0.017842520033339738,\n",
       "  0.019082140890629574,\n",
       "  0.01808769986970996,\n",
       "  0.018583530083877177,\n",
       "  0.01834030227967534,\n",
       "  0.018170137162827744,\n",
       "  0.01810067596844082,\n",
       "  0.01815898287567588,\n",
       "  0.018311552892886652,\n",
       "  0.018417032846964027,\n",
       "  0.018001253825114912,\n",
       "  0.018035854201718986,\n",
       "  0.01789244047928367,\n",
       "  0.017951607863746652,\n",
       "  0.017799837043486446,\n",
       "  0.01797497607617972,\n",
       "  0.01792737789741322,\n",
       "  0.017854995676472165,\n",
       "  0.01771767551161678,\n",
       "  0.018083019588526632,\n",
       "  0.017925377351691925,\n",
       "  0.01785274745629654,\n",
       "  0.017903723710352478,\n",
       "  0.01783560429870524,\n",
       "  0.018258716527078365,\n",
       "  0.018388367720556707,\n",
       "  0.017777920407623334,\n",
       "  0.01784964003556544,\n",
       "  0.017737788209315125,\n",
       "  0.018090563446004067,\n",
       "  0.01784339838398189,\n",
       "  0.01777785657400108,\n",
       "  0.017719596903646488,\n",
       "  0.01788895388683641,\n",
       "  0.017730577563345833,\n",
       "  0.01777794594107223,\n",
       "  0.017974711804983605,\n",
       "  0.017786269845413556,\n",
       "  0.01770673953545301,\n",
       "  0.017659417117934628,\n",
       "  0.017819968891271465,\n",
       "  0.01777160598571042,\n",
       "  0.01818573299341572,\n",
       "  0.017613437759828376,\n",
       "  0.01762663216954733,\n",
       "  0.01787695571919841,\n",
       "  0.017970634113194314,\n",
       "  0.018001385322376745,\n",
       "  0.01762203614874537,\n",
       "  0.017837270356245948,\n",
       "  0.018025959990270325,\n",
       "  0.017916109986413752,\n",
       "  0.017610403109426638,\n",
       "  0.017730986098528228,\n",
       "  0.017952647075116873,\n",
       "  0.017752155881011023,\n",
       "  0.017530752792256264,\n",
       "  0.017591619427743527,\n",
       "  0.01753729829188172,\n",
       "  0.016122944383736116,\n",
       "  0.01656116858384057,\n",
       "  0.01696337147090968,\n",
       "  0.016034705891028306,\n",
       "  0.016002478848499466,\n",
       "  0.01596866617879395,\n",
       "  0.015837188067046832,\n",
       "  0.016219379113540752,\n",
       "  0.016205081658829328,\n",
       "  0.015757108787934942,\n",
       "  0.016015738368513114,\n",
       "  0.015880600036866213,\n",
       "  0.016083124970176933,\n",
       "  0.015854876363772145,\n",
       "  0.01576416367986595,\n",
       "  0.015871891854118948,\n",
       "  0.01572143345313219,\n",
       "  0.01611427194781731,\n",
       "  0.015694274800209955,\n",
       "  0.015763561090471914,\n",
       "  0.01566347124745728,\n",
       "  0.015472684040605783,\n",
       "  0.015487098949182146,\n",
       "  0.015533498332522799,\n",
       "  0.015521544848420374,\n",
       "  0.015309129533678334,\n",
       "  0.015309566155654519,\n",
       "  0.015204014707920223,\n",
       "  0.015107131866087397,\n",
       "  0.015246437256594738,\n",
       "  0.015235849812008451,\n",
       "  0.015058640016609406,\n",
       "  0.015144966053994624,\n",
       "  0.015410322421685439,\n",
       "  0.0152647115459723,\n",
       "  0.015196518087323252,\n",
       "  0.01530292490559569,\n",
       "  0.015159883971514312,\n",
       "  0.015129866848987749,\n",
       "  0.015138613331908362,\n",
       "  0.015226752244165464,\n",
       "  0.0149481108230121,\n",
       "  0.015292028506277715,\n",
       "  0.015038993304353482,\n",
       "  0.015012384897255036,\n",
       "  0.015170097351074219,\n",
       "  0.015353953503221872,\n",
       "  0.015162167938518396,\n",
       "  0.014957194347458192,\n",
       "  0.015053077554766592,\n",
       "  0.01494480168803469,\n",
       "  0.01496770263835289,\n",
       "  0.01487365145600305,\n",
       "  0.015006158565740828,\n",
       "  0.01507952254779208,\n",
       "  0.014772369200924793,\n",
       "  0.01522783869241615,\n",
       "  0.015073953702587042,\n",
       "  0.015097136797515584,\n",
       "  0.015082366973999513,\n",
       "  0.01499997819443464,\n",
       "  0.014957259457752886,\n",
       "  0.015005829184250022,\n",
       "  0.014907493489174799,\n",
       "  0.015139335928512226,\n",
       "  0.015029230590166656,\n",
       "  0.01492786918139681,\n",
       "  0.014918119233934435,\n",
       "  0.015417713078469477,\n",
       "  0.01509313825941788,\n",
       "  0.01499994755429596,\n",
       "  0.015039528230107931,\n",
       "  0.014921390068738495,\n",
       "  0.014882071110777747,\n",
       "  0.014862576322542776,\n",
       "  0.014958242495535528,\n",
       "  0.014786643675532207,\n",
       "  0.0149934926984141,\n",
       "  0.014864501544589818,\n",
       "  0.014952577899897115,\n",
       "  0.015033230404936804,\n",
       "  0.014767443798632028,\n",
       "  0.014930862978280309,\n",
       "  0.014728015046841009,\n",
       "  0.01525934441501357,\n",
       "  0.015087731551613354,\n",
       "  0.014894353976370978,\n",
       "  0.014828011692767162,\n",
       "  0.014842214673717657,\n",
       "  0.014867585985216909,\n",
       "  0.014966304682025628,\n",
       "  0.014756571656090508,\n",
       "  0.014954559295531737,\n",
       "  0.014734007747297785,\n",
       "  0.014873032269867229,\n",
       "  0.015227351003542163,\n",
       "  0.015090819822257781,\n",
       "  0.014853056176120498,\n",
       "  0.01481080470155361,\n",
       "  0.014833696715164695,\n",
       "  0.015184352675594958,\n",
       "  0.015005758967265546,\n",
       "  0.014902379139360175,\n",
       "  0.014976446567928615,\n",
       "  0.015047299335280576,\n",
       "  0.014787164557889762,\n",
       "  0.014644340658124033,\n",
       "  0.015031425190099592,\n",
       "  0.015174158446121726,\n",
       "  0.014793392166076415,\n",
       "  0.014947293752647308,\n",
       "  0.01481454918183476,\n",
       "  0.015010947364081978,\n",
       "  0.014839568131739197,\n",
       "  0.01485348641473446,\n",
       "  0.014709645007030073,\n",
       "  0.014701111728407771,\n",
       "  0.014714191237606677,\n",
       "  0.014975046058256463,\n",
       "  0.014954302684370294,\n",
       "  0.01477365098005956,\n",
       "  0.014807994745502191,\n",
       "  0.014737466252951258,\n",
       "  0.0147940075221949,\n",
       "  0.014861300926770233,\n",
       "  0.014795950617656171,\n",
       "  0.014716005389151006,\n",
       "  0.014746797451851678,\n",
       "  0.014816039058578062,\n",
       "  0.014886738625236623,\n",
       "  0.014803365531216663,\n",
       "  0.01477855978561054,\n",
       "  0.014676686431190257,\n",
       "  0.015128471446005376,\n",
       "  0.014731470999149593,\n",
       "  0.014658990474430273,\n",
       "  0.014739482118741895,\n",
       "  0.01470077085686496,\n",
       "  0.014714315074833841,\n",
       "  0.014784646959828245,\n",
       "  0.014721147825759418,\n",
       "  0.014795705496546734,\n",
       "  0.014726179191865117,\n",
       "  0.014909907676768272,\n",
       "  0.014751189205062437,\n",
       "  0.014768071921474963,\n",
       "  0.014678153327829548,\n",
       "  0.014927613846907814,\n",
       "  0.014857797737581184,\n",
       "  0.014735247396241869,\n",
       "  0.014850465807729616,\n",
       "  0.014800279813917128,\n",
       "  0.014693404456857377,\n",
       "  0.014695491816304933,\n",
       "  0.014898323151002446,\n",
       "  0.01485711344115067,\n",
       "  0.014723236461879419,\n",
       "  0.014682165909324146],\n",
       " 'optimizer': Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 1.5e-05\n",
       "     weight_decay: 0\n",
       " ),\n",
       " 'pred_val_final': array([[1.2428334e-05, 6.1317536e-08, 9.8674959e-03, 9.9012005e-01],\n",
       "        [9.9999976e-01, 1.8784151e-07, 1.5059199e-16, 1.9183210e-16],\n",
       "        [1.0000000e+00, 1.1315442e-33, 0.0000000e+00, 0.0000000e+00],\n",
       "        ...,\n",
       "        [1.0000000e+00, 2.5425415e-12, 1.2555313e-21, 5.5716728e-26],\n",
       "        [3.8598979e-01, 2.5045863e-01, 3.5776529e-01, 5.7862974e-03],\n",
       "        [9.9723923e-01, 2.7603672e-03, 3.0165276e-07, 2.9045284e-09]],\n",
       "       dtype=float32),\n",
       " 'recall_val_epoch_bebs': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.010683760683760684,\n",
       "  0.1794871794871795,\n",
       "  0.1623931623931624,\n",
       "  0.20085470085470086,\n",
       "  0.23076923076923078,\n",
       "  0.14957264957264957,\n",
       "  0.23504273504273504,\n",
       "  0.5149572649572649,\n",
       "  0.38675213675213677,\n",
       "  0.29273504273504275,\n",
       "  0.29273504273504275,\n",
       "  0.3076923076923077,\n",
       "  0.3055555555555556,\n",
       "  0.297008547008547,\n",
       "  0.4829059829059829,\n",
       "  0.3482905982905983,\n",
       "  0.37606837606837606,\n",
       "  0.21581196581196582,\n",
       "  0.3333333333333333,\n",
       "  0.405982905982906,\n",
       "  0.37606837606837606,\n",
       "  0.36324786324786323,\n",
       "  0.4188034188034188,\n",
       "  0.19658119658119658,\n",
       "  0.3696581196581197,\n",
       "  0.35683760683760685,\n",
       "  0.405982905982906,\n",
       "  0.3055555555555556,\n",
       "  0.3504273504273504,\n",
       "  0.45085470085470086,\n",
       "  0.3269230769230769,\n",
       "  0.5363247863247863,\n",
       "  0.4893162393162393,\n",
       "  0.532051282051282,\n",
       "  0.36538461538461536,\n",
       "  0.5170940170940171,\n",
       "  0.4081196581196581,\n",
       "  0.3247863247863248,\n",
       "  0.31837606837606836,\n",
       "  0.47435897435897434,\n",
       "  0.4358974358974359,\n",
       "  0.4935897435897436,\n",
       "  0.405982905982906,\n",
       "  0.39316239316239315,\n",
       "  0.405982905982906,\n",
       "  0.21367521367521367,\n",
       "  0.6666666666666666,\n",
       "  0.4700854700854701,\n",
       "  0.3696581196581197,\n",
       "  0.31837606837606836,\n",
       "  0.2841880341880342,\n",
       "  0.3333333333333333,\n",
       "  0.31196581196581197,\n",
       "  0.5705128205128205,\n",
       "  0.4081196581196581,\n",
       "  0.3974358974358974,\n",
       "  0.6047008547008547,\n",
       "  0.42094017094017094,\n",
       "  0.6089743589743589,\n",
       "  0.3717948717948718,\n",
       "  0.5726495726495726,\n",
       "  0.31196581196581197,\n",
       "  0.4444444444444444,\n",
       "  0.5726495726495726,\n",
       "  0.5576923076923077,\n",
       "  0.4188034188034188,\n",
       "  0.23717948717948717,\n",
       "  0.3782051282051282,\n",
       "  0.5534188034188035,\n",
       "  0.42094017094017094,\n",
       "  0.42094017094017094,\n",
       "  0.3482905982905983,\n",
       "  0.49572649572649574,\n",
       "  0.49786324786324787,\n",
       "  0.5598290598290598,\n",
       "  0.47435897435897434,\n",
       "  0.6260683760683761,\n",
       "  0.3696581196581197,\n",
       "  0.5982905982905983,\n",
       "  0.4081196581196581,\n",
       "  0.45726495726495725,\n",
       "  0.5811965811965812,\n",
       "  0.34615384615384615,\n",
       "  0.47863247863247865,\n",
       "  0.4935897435897436,\n",
       "  0.42735042735042733,\n",
       "  0.47863247863247865,\n",
       "  0.5149572649572649,\n",
       "  0.5064102564102564,\n",
       "  0.3717948717948718,\n",
       "  0.4551282051282051,\n",
       "  0.6923076923076923,\n",
       "  0.45085470085470086,\n",
       "  0.25,\n",
       "  0.5085470085470085,\n",
       "  0.5491452991452992,\n",
       "  0.4658119658119658,\n",
       "  0.5534188034188035,\n",
       "  0.6431623931623932,\n",
       "  0.4358974358974359,\n",
       "  0.6452991452991453,\n",
       "  0.4700854700854701,\n",
       "  0.3952991452991453,\n",
       "  0.5876068376068376,\n",
       "  0.45726495726495725,\n",
       "  0.391025641025641,\n",
       "  0.4465811965811966,\n",
       "  0.5491452991452992,\n",
       "  0.47863247863247865,\n",
       "  0.5448717948717948,\n",
       "  0.37393162393162394,\n",
       "  0.4700854700854701,\n",
       "  0.6111111111111112,\n",
       "  0.4423076923076923,\n",
       "  0.7158119658119658,\n",
       "  0.48717948717948717,\n",
       "  0.5897435897435898,\n",
       "  0.405982905982906,\n",
       "  0.5042735042735043,\n",
       "  0.5170940170940171,\n",
       "  0.4700854700854701,\n",
       "  0.5961538461538461,\n",
       "  0.37606837606837606,\n",
       "  0.5277777777777778,\n",
       "  0.44017094017094016,\n",
       "  0.5491452991452992,\n",
       "  0.5405982905982906,\n",
       "  0.4465811965811966,\n",
       "  0.46153846153846156,\n",
       "  0.3504273504273504,\n",
       "  0.42735042735042733,\n",
       "  0.39316239316239315,\n",
       "  0.391025641025641,\n",
       "  0.5512820512820513,\n",
       "  0.45085470085470086,\n",
       "  0.40384615384615385,\n",
       "  0.391025641025641,\n",
       "  0.3803418803418803,\n",
       "  0.5021367521367521],\n",
       " 'recall_val_epoch_ebs': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8326996197718631,\n",
       "  0.8669201520912547,\n",
       "  0.7984790874524715,\n",
       "  0.8688212927756654,\n",
       "  0.8593155893536122,\n",
       "  0.876425855513308,\n",
       "  0.8307984790874525,\n",
       "  0.8745247148288974,\n",
       "  0.8783269961977186,\n",
       "  0.7965779467680608,\n",
       "  0.8745247148288974,\n",
       "  0.8574144486692015,\n",
       "  0.8536121673003803,\n",
       "  0.8346007604562737,\n",
       "  0.8631178707224335,\n",
       "  0.8060836501901141,\n",
       "  0.7908745247148289,\n",
       "  0.8973384030418251,\n",
       "  0.8783269961977186,\n",
       "  0.8688212927756654,\n",
       "  0.8536121673003803,\n",
       "  0.8155893536121673,\n",
       "  0.7927756653992395,\n",
       "  0.811787072243346,\n",
       "  0.8593155893536122,\n",
       "  0.8403041825095057,\n",
       "  0.5418250950570342,\n",
       "  0.7585551330798479,\n",
       "  0.8231939163498099,\n",
       "  0.8288973384030418,\n",
       "  0.7946768060836502,\n",
       "  0.7737642585551331,\n",
       "  0.8193916349809885,\n",
       "  0.6102661596958175,\n",
       "  0.8174904942965779,\n",
       "  0.7091254752851711,\n",
       "  0.8821292775665399,\n",
       "  0.7224334600760456,\n",
       "  0.7281368821292775,\n",
       "  0.811787072243346,\n",
       "  0.7566539923954373,\n",
       "  0.747148288973384,\n",
       "  0.8840304182509505,\n",
       "  0.7756653992395437,\n",
       "  0.7832699619771863,\n",
       "  0.7604562737642585,\n",
       "  0.8307984790874525,\n",
       "  0.8231939163498099,\n",
       "  0.7243346007604563,\n",
       "  0.811787072243346,\n",
       "  0.655893536121673,\n",
       "  0.7243346007604563,\n",
       "  0.6140684410646388,\n",
       "  0.8212927756653993,\n",
       "  0.655893536121673,\n",
       "  0.7737642585551331,\n",
       "  0.8060836501901141,\n",
       "  0.8326996197718631,\n",
       "  0.5931558935361216,\n",
       "  0.7053231939163498,\n",
       "  0.7129277566539924,\n",
       "  0.7376425855513308,\n",
       "  0.8060836501901141,\n",
       "  0.8003802281368821,\n",
       "  0.8916349809885932,\n",
       "  0.4790874524714829,\n",
       "  0.7452471482889734,\n",
       "  0.8231939163498099,\n",
       "  0.870722433460076,\n",
       "  0.8669201520912547,\n",
       "  0.8536121673003803,\n",
       "  0.8326996197718631,\n",
       "  0.6387832699619772,\n",
       "  0.7946768060836502,\n",
       "  0.8003802281368821,\n",
       "  0.49049429657794674,\n",
       "  0.7775665399239544,\n",
       "  0.5703422053231939,\n",
       "  0.7965779467680608,\n",
       "  0.5798479087452472,\n",
       "  0.8517110266159695,\n",
       "  0.7357414448669202,\n",
       "  0.6520912547528517,\n",
       "  0.720532319391635,\n",
       "  0.779467680608365,\n",
       "  0.903041825095057,\n",
       "  0.8098859315589354,\n",
       "  0.6768060836501901,\n",
       "  0.7927756653992395,\n",
       "  0.8212927756653993,\n",
       "  0.8384030418250951,\n",
       "  0.7262357414448669,\n",
       "  0.6577946768060836,\n",
       "  0.6920152091254753,\n",
       "  0.6996197718631179,\n",
       "  0.49049429657794674,\n",
       "  0.8650190114068441,\n",
       "  0.623574144486692,\n",
       "  0.8003802281368821,\n",
       "  0.7642585551330798,\n",
       "  0.6730038022813688,\n",
       "  0.8574144486692015,\n",
       "  0.7395437262357415,\n",
       "  0.7376425855513308,\n",
       "  0.7661596958174905,\n",
       "  0.7490494296577946,\n",
       "  0.7129277566539924,\n",
       "  0.7395437262357415,\n",
       "  0.8669201520912547,\n",
       "  0.7756653992395437,\n",
       "  0.4467680608365019,\n",
       "  0.7699619771863118,\n",
       "  0.8897338403041825,\n",
       "  0.7490494296577946,\n",
       "  0.6806083650190115,\n",
       "  0.7813688212927756,\n",
       "  0.6996197718631179,\n",
       "  0.6216730038022814,\n",
       "  0.7927756653992395,\n",
       "  0.564638783269962,\n",
       "  0.7680608365019012,\n",
       "  0.8365019011406845,\n",
       "  0.6844106463878327,\n",
       "  0.7927756653992395,\n",
       "  0.8136882129277566,\n",
       "  0.7680608365019012,\n",
       "  0.7110266159695817,\n",
       "  0.7870722433460076,\n",
       "  0.7452471482889734,\n",
       "  0.8460076045627376,\n",
       "  0.7680608365019012,\n",
       "  0.6254752851711026,\n",
       "  0.811787072243346,\n",
       "  0.4467680608365019,\n",
       "  0.752851711026616,\n",
       "  0.6901140684410646,\n",
       "  0.8326996197718631,\n",
       "  0.7395437262357415,\n",
       "  0.7281368821292775,\n",
       "  0.7680608365019012,\n",
       "  0.6711026615969582,\n",
       "  0.8346007604562737,\n",
       "  0.6749049429657795,\n",
       "  0.8041825095057035,\n",
       "  0.7034220532319392,\n",
       "  0.7167300380228137,\n",
       "  0.7813688212927756,\n",
       "  0.7167300380228137,\n",
       "  0.8498098859315589,\n",
       "  0.8231939163498099,\n",
       "  0.8631178707224335,\n",
       "  0.8384030418250951,\n",
       "  0.7414448669201521,\n",
       "  0.7984790874524715,\n",
       "  0.8422053231939164,\n",
       "  0.8536121673003803,\n",
       "  0.8384030418250951,\n",
       "  0.7129277566539924],\n",
       " 'recall_val_epoch_pl': [0.9617021276595744,\n",
       "  0.9531914893617022,\n",
       "  0.9638297872340426,\n",
       "  0.9702127659574468,\n",
       "  0.925531914893617,\n",
       "  0.9468085106382979,\n",
       "  0.9297872340425531,\n",
       "  0.948936170212766,\n",
       "  0.9191489361702128,\n",
       "  0.9404255319148936,\n",
       "  0.902127659574468,\n",
       "  0.9404255319148936,\n",
       "  0.9297872340425531,\n",
       "  0.9468085106382979,\n",
       "  0.9382978723404255,\n",
       "  0.9361702127659575,\n",
       "  0.9617021276595744,\n",
       "  0.9446808510638298,\n",
       "  0.9702127659574468,\n",
       "  0.9617021276595744,\n",
       "  0.9404255319148936,\n",
       "  0.9085106382978724,\n",
       "  0.9212765957446809,\n",
       "  0.9531914893617022,\n",
       "  0.9446808510638298,\n",
       "  0.8148936170212766,\n",
       "  0.9106382978723404,\n",
       "  0.9468085106382979,\n",
       "  0.9361702127659575,\n",
       "  0.951063829787234,\n",
       "  0.9574468085106383,\n",
       "  0.9617021276595744,\n",
       "  0.9574468085106383,\n",
       "  0.8851063829787233,\n",
       "  0.9851063829787234,\n",
       "  0.9553191489361702,\n",
       "  0.9723404255319149,\n",
       "  0.9723404255319149,\n",
       "  0.9680851063829787,\n",
       "  0.9638297872340426,\n",
       "  0.9680851063829787,\n",
       "  0.9702127659574468,\n",
       "  0.9702127659574468,\n",
       "  0.9595744680851064,\n",
       "  0.9638297872340426,\n",
       "  0.9595744680851064,\n",
       "  0.9638297872340426,\n",
       "  0.9340425531914893,\n",
       "  0.9680851063829787,\n",
       "  0.948936170212766,\n",
       "  0.925531914893617,\n",
       "  0.9297872340425531,\n",
       "  0.9638297872340426,\n",
       "  0.9617021276595744,\n",
       "  0.9617021276595744,\n",
       "  0.9617021276595744,\n",
       "  0.9617021276595744,\n",
       "  0.9680851063829787,\n",
       "  0.9765957446808511,\n",
       "  0.9531914893617022,\n",
       "  0.9553191489361702,\n",
       "  0.9595744680851064,\n",
       "  0.9680851063829787,\n",
       "  0.9638297872340426,\n",
       "  0.9617021276595744,\n",
       "  0.951063829787234,\n",
       "  0.9638297872340426,\n",
       "  0.948936170212766,\n",
       "  0.9574468085106383,\n",
       "  0.9680851063829787,\n",
       "  0.9553191489361702,\n",
       "  0.951063829787234,\n",
       "  0.9553191489361702,\n",
       "  0.9617021276595744,\n",
       "  0.951063829787234,\n",
       "  0.9765957446808511,\n",
       "  0.9468085106382979,\n",
       "  0.9531914893617022,\n",
       "  0.9680851063829787,\n",
       "  0.9659574468085106,\n",
       "  0.9723404255319149,\n",
       "  0.9446808510638298,\n",
       "  0.9680851063829787,\n",
       "  0.9723404255319149,\n",
       "  0.9659574468085106,\n",
       "  0.9468085106382979,\n",
       "  0.9659574468085106,\n",
       "  0.9744680851063829,\n",
       "  0.9617021276595744,\n",
       "  0.951063829787234,\n",
       "  0.9574468085106383,\n",
       "  0.9382978723404255,\n",
       "  0.9595744680851064,\n",
       "  0.9680851063829787,\n",
       "  0.9744680851063829,\n",
       "  0.9531914893617022,\n",
       "  0.9531914893617022,\n",
       "  0.9425531914893617,\n",
       "  0.9340425531914893,\n",
       "  0.9702127659574468,\n",
       "  0.9595744680851064,\n",
       "  0.9340425531914893,\n",
       "  0.9553191489361702,\n",
       "  0.9595744680851064,\n",
       "  0.9702127659574468,\n",
       "  0.9595744680851064,\n",
       "  0.9574468085106383,\n",
       "  0.9659574468085106,\n",
       "  0.9446808510638298,\n",
       "  0.9595744680851064,\n",
       "  0.951063829787234,\n",
       "  0.9617021276595744,\n",
       "  0.9702127659574468,\n",
       "  0.9617021276595744,\n",
       "  0.9595744680851064,\n",
       "  0.9617021276595744,\n",
       "  0.9361702127659575,\n",
       "  0.951063829787234,\n",
       "  0.9638297872340426,\n",
       "  0.9659574468085106,\n",
       "  0.9446808510638298,\n",
       "  0.9702127659574468,\n",
       "  0.9723404255319149,\n",
       "  0.9446808510638298,\n",
       "  0.9680851063829787,\n",
       "  0.9787234042553191,\n",
       "  0.951063829787234,\n",
       "  0.9659574468085106,\n",
       "  0.951063829787234,\n",
       "  0.9723404255319149,\n",
       "  0.9659574468085106,\n",
       "  0.9702127659574468,\n",
       "  0.9744680851063829,\n",
       "  0.9617021276595744,\n",
       "  0.9446808510638298,\n",
       "  0.9617021276595744,\n",
       "  0.9574468085106383,\n",
       "  0.9638297872340426,\n",
       "  0.9617021276595744,\n",
       "  0.9702127659574468,\n",
       "  0.9638297872340426,\n",
       "  0.9553191489361702,\n",
       "  0.9574468085106383,\n",
       "  0.9574468085106383,\n",
       "  0.9468085106382979,\n",
       "  0.9617021276595744,\n",
       "  0.9680851063829787,\n",
       "  0.9446808510638298,\n",
       "  0.9723404255319149,\n",
       "  0.9638297872340426,\n",
       "  0.9617021276595744,\n",
       "  0.9787234042553191,\n",
       "  0.9702127659574468,\n",
       "  0.9638297872340426,\n",
       "  0.9680851063829787,\n",
       "  0.9638297872340426,\n",
       "  0.9595744680851064,\n",
       "  0.9531914893617022,\n",
       "  0.9659574468085106,\n",
       "  0.9595744680851064,\n",
       "  0.9744680851063829,\n",
       "  0.9617021276595744,\n",
       "  0.9680851063829787,\n",
       "  0.9702127659574468,\n",
       "  0.9680851063829787,\n",
       "  0.9680851063829787,\n",
       "  0.9659574468085106,\n",
       "  0.9638297872340426,\n",
       "  0.9553191489361702,\n",
       "  0.9680851063829787,\n",
       "  0.951063829787234,\n",
       "  0.9617021276595744,\n",
       "  0.9659574468085106,\n",
       "  0.9553191489361702,\n",
       "  0.9659574468085106,\n",
       "  0.951063829787234,\n",
       "  0.9808510638297873,\n",
       "  0.951063829787234,\n",
       "  0.9617021276595744,\n",
       "  0.948936170212766,\n",
       "  0.9595744680851064,\n",
       "  0.9680851063829787,\n",
       "  0.9617021276595744,\n",
       "  0.9553191489361702,\n",
       "  0.9680851063829787,\n",
       "  0.9595744680851064,\n",
       "  0.9702127659574468,\n",
       "  0.9787234042553191,\n",
       "  0.9659574468085106,\n",
       "  0.9595744680851064,\n",
       "  0.9531914893617022,\n",
       "  0.9659574468085106,\n",
       "  0.9829787234042553,\n",
       "  0.9659574468085106,\n",
       "  0.9680851063829787,\n",
       "  0.9744680851063829,\n",
       "  0.9808510638297873,\n",
       "  0.9702127659574468,\n",
       "  0.9361702127659575,\n",
       "  0.9787234042553191,\n",
       "  0.9744680851063829,\n",
       "  0.9659574468085106,\n",
       "  0.9617021276595744,\n",
       "  0.9638297872340426,\n",
       "  0.9638297872340426,\n",
       "  0.9659574468085106,\n",
       "  0.9702127659574468,\n",
       "  0.9617021276595744,\n",
       "  0.9595744680851064,\n",
       "  0.9446808510638298,\n",
       "  0.9765957446808511,\n",
       "  0.9744680851063829,\n",
       "  0.9617021276595744,\n",
       "  0.9531914893617022,\n",
       "  0.9574468085106383,\n",
       "  0.9595744680851064,\n",
       "  0.9723404255319149,\n",
       "  0.9723404255319149,\n",
       "  0.9659574468085106,\n",
       "  0.9553191489361702,\n",
       "  0.9574468085106383,\n",
       "  0.9723404255319149,\n",
       "  0.9702127659574468,\n",
       "  0.9702127659574468,\n",
       "  0.9553191489361702,\n",
       "  0.9744680851063829,\n",
       "  0.9659574468085106,\n",
       "  0.9553191489361702,\n",
       "  0.9595744680851064,\n",
       "  0.9595744680851064,\n",
       "  0.9617021276595744,\n",
       "  0.9723404255319149,\n",
       "  0.9680851063829787,\n",
       "  0.9702127659574468,\n",
       "  0.9574468085106383,\n",
       "  0.9659574468085106,\n",
       "  0.9680851063829787,\n",
       "  0.9680851063829787,\n",
       "  0.9574468085106383,\n",
       "  0.9744680851063829,\n",
       "  0.9680851063829787,\n",
       "  0.9531914893617022,\n",
       "  0.9617021276595744,\n",
       "  0.9659574468085106,\n",
       "  0.9531914893617022,\n",
       "  0.9659574468085106,\n",
       "  0.9638297872340426,\n",
       "  0.9744680851063829,\n",
       "  0.9617021276595744,\n",
       "  0.9595744680851064],\n",
       " 'recall_val_epoch_unk': [0.12532808398950132,\n",
       "  0.23556430446194226,\n",
       "  0.26115485564304464,\n",
       "  0.291994750656168,\n",
       "  0.41732283464566927,\n",
       "  0.36876640419947504,\n",
       "  0.4199475065616798,\n",
       "  0.39829396325459315,\n",
       "  0.4468503937007874,\n",
       "  0.41929133858267714,\n",
       "  0.49015748031496065,\n",
       "  0.43832020997375326,\n",
       "  0.45144356955380577,\n",
       "  0.4153543307086614,\n",
       "  0.4258530183727034,\n",
       "  0.4540682414698163,\n",
       "  0.41732283464566927,\n",
       "  0.452755905511811,\n",
       "  0.4225721784776903,\n",
       "  0.4862204724409449,\n",
       "  0.5354330708661418,\n",
       "  0.7244094488188977,\n",
       "  0.7801837270341208,\n",
       "  0.7572178477690289,\n",
       "  0.7887139107611548,\n",
       "  0.910761154855643,\n",
       "  0.8720472440944882,\n",
       "  0.8267716535433071,\n",
       "  0.8569553805774278,\n",
       "  0.8267716535433071,\n",
       "  0.8326771653543307,\n",
       "  0.818241469816273,\n",
       "  0.8228346456692913,\n",
       "  0.9028871391076115,\n",
       "  0.7152230971128609,\n",
       "  0.8523622047244095,\n",
       "  0.7860892388451444,\n",
       "  0.8090551181102362,\n",
       "  0.8313648293963255,\n",
       "  0.844488188976378,\n",
       "  0.833989501312336,\n",
       "  0.818241469816273,\n",
       "  0.8038057742782152,\n",
       "  0.8576115485564304,\n",
       "  0.8576115485564304,\n",
       "  0.8740157480314961,\n",
       "  0.8576115485564304,\n",
       "  0.8956692913385826,\n",
       "  0.8556430446194225,\n",
       "  0.8759842519685039,\n",
       "  0.8891076115485564,\n",
       "  0.9081364829396326,\n",
       "  0.8418635170603674,\n",
       "  0.8622047244094488,\n",
       "  0.8753280839895013,\n",
       "  0.8615485564304461,\n",
       "  0.8727034120734908,\n",
       "  0.8156167979002624,\n",
       "  0.8011811023622047,\n",
       "  0.8930446194225722,\n",
       "  0.8792650918635171,\n",
       "  0.884514435695538,\n",
       "  0.8320209973753281,\n",
       "  0.8740157480314961,\n",
       "  0.886482939632546,\n",
       "  0.8976377952755905,\n",
       "  0.8694225721784777,\n",
       "  0.8982939632545932,\n",
       "  0.8825459317585301,\n",
       "  0.85498687664042,\n",
       "  0.886482939632546,\n",
       "  0.8956692913385826,\n",
       "  0.9015748031496063,\n",
       "  0.8792650918635171,\n",
       "  0.8851706036745407,\n",
       "  0.8202099737532809,\n",
       "  0.9061679790026247,\n",
       "  0.8969816272965879,\n",
       "  0.8615485564304461,\n",
       "  0.844488188976378,\n",
       "  0.844488188976378,\n",
       "  0.8956692913385826,\n",
       "  0.8694225721784777,\n",
       "  0.8379265091863517,\n",
       "  0.8523622047244095,\n",
       "  0.9002624671916011,\n",
       "  0.8766404199475065,\n",
       "  0.8543307086614174,\n",
       "  0.8786089238845144,\n",
       "  0.9120734908136483,\n",
       "  0.8956692913385826,\n",
       "  0.9101049868766404,\n",
       "  0.8228346456692913,\n",
       "  0.7513123359580053,\n",
       "  0.7178477690288714,\n",
       "  0.821522309711286,\n",
       "  0.8287401574803149,\n",
       "  0.8313648293963255,\n",
       "  0.8667979002624672,\n",
       "  0.7906824146981627,\n",
       "  0.7933070866141733,\n",
       "  0.8818897637795275,\n",
       "  0.8202099737532809,\n",
       "  0.84251968503937,\n",
       "  0.8175853018372703,\n",
       "  0.8569553805774278,\n",
       "  0.8589238845144357,\n",
       "  0.8608923884514436,\n",
       "  0.8910761154855643,\n",
       "  0.7992125984251969,\n",
       "  0.860236220472441,\n",
       "  0.84251968503937,\n",
       "  0.8070866141732284,\n",
       "  0.8503937007874016,\n",
       "  0.844488188976378,\n",
       "  0.821522309711286,\n",
       "  0.8313648293963255,\n",
       "  0.8438320209973753,\n",
       "  0.8569553805774278,\n",
       "  0.8405511811023622,\n",
       "  0.8569553805774278,\n",
       "  0.8300524934383202,\n",
       "  0.847769028871391,\n",
       "  0.8746719160104987,\n",
       "  0.8517060367454068,\n",
       "  0.8300524934383202,\n",
       "  0.8208661417322834,\n",
       "  0.860236220472441,\n",
       "  0.8353018372703412,\n",
       "  0.8674540682414699,\n",
       "  0.8451443569553806,\n",
       "  0.8307086614173228,\n",
       "  0.839238845144357,\n",
       "  0.8641732283464567,\n",
       "  0.8379265091863517,\n",
       "  0.8569553805774278,\n",
       "  0.8661417322834646,\n",
       "  0.8333333333333334,\n",
       "  0.8169291338582677,\n",
       "  0.8326771653543307,\n",
       "  0.8589238845144357,\n",
       "  0.8595800524934383,\n",
       "  0.8700787401574803,\n",
       "  0.8471128608923885,\n",
       "  0.881233595800525,\n",
       "  0.8464566929133859,\n",
       "  0.8490813648293963,\n",
       "  0.8858267716535433,\n",
       "  0.8307086614173228,\n",
       "  0.847769028871391,\n",
       "  0.8799212598425197,\n",
       "  0.8523622047244095,\n",
       "  0.8431758530183727,\n",
       "  0.8622047244094488,\n",
       "  0.839238845144357,\n",
       "  0.8556430446194225,\n",
       "  0.8536745406824147,\n",
       "  0.860236220472441,\n",
       "  0.8490813648293963,\n",
       "  0.8510498687664042,\n",
       "  0.7952755905511811,\n",
       "  0.84251968503937,\n",
       "  0.844488188976378,\n",
       "  0.8523622047244095,\n",
       "  0.860236220472441,\n",
       "  0.8536745406824147,\n",
       "  0.8622047244094488,\n",
       "  0.8799212598425197,\n",
       "  0.8700787401574803,\n",
       "  0.8562992125984252,\n",
       "  0.868766404199475,\n",
       "  0.8753280839895013,\n",
       "  0.8497375328083989,\n",
       "  0.8799212598425197,\n",
       "  0.8523622047244095,\n",
       "  0.8661417322834646,\n",
       "  0.8103674540682415,\n",
       "  0.8536745406824147,\n",
       "  0.8595800524934383,\n",
       "  0.865485564304462,\n",
       "  0.85498687664042,\n",
       "  0.8458005249343832,\n",
       "  0.8523622047244095,\n",
       "  0.8727034120734908,\n",
       "  0.8720472440944882,\n",
       "  0.8681102362204725,\n",
       "  0.8694225721784777,\n",
       "  0.8313648293963255,\n",
       "  0.821522309711286,\n",
       "  0.868766404199475,\n",
       "  0.868766404199475,\n",
       "  0.8562992125984252,\n",
       "  0.8116797900262467,\n",
       "  0.8431758530183727,\n",
       "  0.8451443569553806,\n",
       "  0.8346456692913385,\n",
       "  0.8333333333333334,\n",
       "  0.8582677165354331,\n",
       "  0.8904199475065617,\n",
       "  0.8254593175853019,\n",
       "  0.8090551181102362,\n",
       "  0.8595800524934383,\n",
       "  0.8700787401574803,\n",
       "  0.8667979002624672,\n",
       "  0.8543307086614174,\n",
       "  0.847769028871391,\n",
       "  0.8497375328083989,\n",
       "  0.8713910761154856,\n",
       "  0.8733595800524935,\n",
       "  0.8786089238845144,\n",
       "  0.8293963254593176,\n",
       "  0.8543307086614174,\n",
       "  0.8628608923884514,\n",
       "  0.865485564304462,\n",
       "  0.8661417322834646,\n",
       "  0.8556430446194225,\n",
       "  0.8576115485564304,\n",
       "  0.8641732283464567,\n",
       "  0.8628608923884514,\n",
       "  0.8543307086614174,\n",
       "  0.8484251968503937,\n",
       "  0.8464566929133859,\n",
       "  0.8562992125984252,\n",
       "  0.8700787401574803,\n",
       "  0.868766404199475,\n",
       "  0.8471128608923885,\n",
       "  0.8589238845144357,\n",
       "  0.8700787401574803,\n",
       "  0.8622047244094488,\n",
       "  0.8700787401574803,\n",
       "  0.8681102362204725,\n",
       "  0.8530183727034121,\n",
       "  0.865485564304462,\n",
       "  0.8661417322834646,\n",
       "  0.8851706036745407,\n",
       "  0.8385826771653543,\n",
       "  0.8700787401574803,\n",
       "  0.8622047244094488,\n",
       "  0.8753280839895013,\n",
       "  0.860236220472441,\n",
       "  0.8589238845144357,\n",
       "  0.8641732283464567,\n",
       "  0.8510498687664042,\n",
       "  0.8562992125984252,\n",
       "  0.8608923884514436,\n",
       "  0.8713910761154856,\n",
       "  0.8385826771653543,\n",
       "  0.8484251968503937,\n",
       "  0.8825459317585301,\n",
       "  0.8871391076115486]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hosborn/TESS/processed_dv_101_centfixed2/all/000002529724_00_01_info.npy'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one run per GPU:\n",
    "\n",
    "* 11 156.611 s/epoch , LOSS: train  =   [0.02326  ,   val  =  0.006663  ,   acc  =  0.84696\n",
    "* 11 156.633 s/epoch , LOSS: train  =   [0.02421  ,   val  =  0.006407  ,   acc  =  0.85749\n",
    "* 11 157.423 s/epoch , LOSS: train  =   [0.02325  ,   val  =  0.006607  ,   acc  =  0.84696\n",
    "* 11 157.655 s/epoch , LOSS: train  =   [0.02450  ,   val  =  0.006902  ,   acc  =  0.83898\n",
    "\n",
    "Projection: 26 hours for 8 folds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2, 1, 3, 2, 6, 1],\n",
       "       [5, 2, 1, 3, 2, 6, 1],\n",
       "       [5, 2, 1, 3, 2, 6, 1],\n",
       "       [5, 2, 1, 3, 2, 6, 1],\n",
       "       [5, 2, 1, 3, 2, 6, 1],\n",
       "       [5, 2, 1, 3, 2, 6, 1],\n",
       "       [5, 2, 1, 3, 2, 6, 1],\n",
       "       [5, 2, 1, 3, 2, 6, 1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([np.array([[5,2,1,3,2,6,1],[5,2,1,3,2,6,1],[5,2,1,3,2,6,1]]),np.array([[5,2,1,3,2,6,1],[5,2,1,3,2,6,1],[5,2,1,3,2,6,1],[5,2,1,3,2,6,1]]),np.array([[5,2,1,3,2,6,1]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Variable data has to be a tensor, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-a632e01076da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Variable data has to be a tensor, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "Variable(np.array([5,2,1,3,2,6,1])[:,np.newaxis]).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5],\n",
       "       [2],\n",
       "       [1],\n",
       "       [3],\n",
       "       [2],\n",
       "       [6],\n",
       "       [1]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join(foldname,'all_exonet_binary_run.sh'),'w') as f_runall:\n",
    "    f_runall.write(\"#Running all files in \"+foldname+' \\n\\n'+' &\\n'.join(names_of_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23095"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('exonet-run-all-cv-basefile.py','r') as f_x:\n",
    "        file2write=f_x.read()\n",
    "len(file2write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is \"a\" formatted string\n",
      "        on multiple lines\n"
     ]
    }
   ],
   "source": [
    "oldstring='formatted string'\n",
    "print('''this is \\\"a\\\" {string}\n",
    "        on multiple lines'''.format(string=oldstring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10:04 - LOSS: train  =   [0.00733485]  ,   val  =   0.002784695541649534  ,   acc  =   0.9392712550607287\n",
    "#11:02 - LOSS: train  =   [0.0071916]  ,   val  =   0.002710555315431382  ,   acc  =   0.9400424137266242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.type of ExtranetXSModel(\n",
       "  (fc_global): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (fc_local): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (final_layer): Sequential(\n",
       "    (0): Linear(in_features=54, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-running those models which did not converge with faster learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputvals_contd=outputvals.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary of trained values already exists\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'loss_train_epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-5e9df617aa79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m                                                 'gt_val_final':outputvals[savename]['gt_val_final']}\n\u001b[1;32m     74\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mpotkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'loss_train_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'loss_val_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'acc_val_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ap_val_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                         \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputvals[\\''\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msavename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\'][\\''\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpotkey\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\']=outputvals[\\''\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msavename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\'][\\''\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpotkey\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\']'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     '''\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loss_train_epoch'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1008x1008 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "\n",
    "#for batch in batch_sizes:\n",
    "#    for lr in lrs:            \n",
    "\n",
    "for fpath in filepaths:\n",
    "    ### divide train and validation sets\n",
    "    kf = KFold(n_splits=nkfolds, shuffle=True)\n",
    "    files_all = np.sort(glob.glob(path.join(filepaths[fpath],'all','*info.npy')))\n",
    "    '''\n",
    "    #kepler_train_data = KeplerDataLoader(filepath=path.join(filepaths[fpath],'train'))\n",
    "    #kepler_val_data = KeplerDataLoader(filepath=path.join(filepaths[fpath],'val'))\n",
    "    '''\n",
    "    \n",
    "    for mod in models:\n",
    "        for aug in augment:\n",
    "            ### loop over folds\n",
    "            kcount = 0\n",
    "            precision_all, recall_all, ap_all = [], [], []\n",
    "            for train_index, val_index in kf.split(files_all):\n",
    "                savename='exonet_CV'+str(kcount)+'_'+fpath+'_'+aug+'_'+mod\n",
    "                \n",
    "                #IF NOT CONVERGED, LETS RE-DO\n",
    "                if np.max(np.array(outputvals[savename]['ap_val_epoch']))<0.75:\n",
    "                    ### initialize model\n",
    "                    if mod=='Big':\n",
    "                        #Have to have separate models for Big as no global pooling means 201-long LC produces more params\n",
    "                        model=eval(models[mod]+'_'+fpath+'().cuda()')\n",
    "                        lr = 3.5e-4\n",
    "                    else:\n",
    "                        model=models[mod]().cuda()\n",
    "                        lr = 7.5e-4\n",
    "                    \n",
    "\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                    criterion = nn.BCELoss()\n",
    "                    batch_size = 64\n",
    "                    n_epochs = 100\n",
    "                    \n",
    "                    model.load_state_dict(torch.load(path.join(foldname,savename+'.pth')))\n",
    "\n",
    "                    ### grab training and validation data\n",
    "                    files_train, files_val = files_all[train_index], files_all[val_index]\n",
    "                    kepler_val_data = KeplerDataLoaderCrossVal(infofiles=files_val)\n",
    "                    kepler_train_data = KeplerDataLoaderCrossVal(infofiles=files_train)\n",
    "                        \n",
    "                    #Loading balancer:\n",
    "                    fname=path.join(foldname,savename+'_BBS.pickle')\n",
    "                    kepler_batch_sampler = pickle.load(open(fname,'rb'))\n",
    "\n",
    "                    kepler_data_loader = DataLoader(kepler_train_data, batch_sampler = kepler_batch_sampler, num_workers=4)\n",
    "                    kepler_val_loader = DataLoader(kepler_val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "                \n",
    "                    ### train model\n",
    "                    print(\"training \"+savename)\n",
    "                    loss_train_epoch, loss_val_epoch, acc_val_epoch, ap_val_epoch, pred_val_final, gt_val_final  = train_model(n_epochs, kepler_data_loader, kepler_val_loader, model, criterion, optimizer,augment=augment[aug])\n",
    "                    print(\"saving \"+savename)\n",
    "                    torch.save(model.state_dict(),path.join(foldname,savename+'.pth'))\n",
    "                    \n",
    "                    outputvals[savename]={'k':kcount,'fpath':fpath,'aug':aug,'mod':mod,'unqid':fpath+'_'+aug+'_'+mod,'pred_val_final':pred_val_final, 'gt_val_final':gt_val_final}\n",
    "                    for potkey in ['loss_train_epoch','loss_val_epoch','acc_val_epoch','ap_val_epoch']:\n",
    "                        exec('outputvals[\\''+savename+'\\'][\\''+potkey+'\\']='+potkey)\n",
    "                                          \n",
    "                else:\n",
    "                    print(\"dictionary of trained values already exists\")\n",
    "                    outputvals[savename]={'k':outputvals[savename]['k'],'fpath':outputvals[savename]['fpath'],\n",
    "                                                'aug':outputvals[savename]['aug'],'mod':outputvals[savename]['mod'],\n",
    "                                                'unqid':outputvals[savename]['unqid'],'pred_val_final':outputvals[savename]['pred_val_final'],\n",
    "                                                'gt_val_final':outputvals[savename]['gt_val_final']}\n",
    "                    for potkey in ['loss_train_epoch','loss_val_epoch','acc_val_epoch','ap_val_epoch']:\n",
    "                        exec('outputvals[\\''+savename+'\\'][\\''+potkey+'\\']=outputvals[\\''+savename+'\\'][\\''+potkey+'\\']')\n",
    "\n",
    "                    '''\n",
    "                    loss_train_epoch=outputvals[savename]['loss_train_epoch']\n",
    "                    loss_val_epoch=outputvals[savename]['loss_val_epoch']\n",
    "                    acc_val_epoch=outputvals[savename]['acc_val_epoch']\n",
    "                    pred_val_final= outputvals[savename]['pred_val_final']\n",
    "                    gt_val_final= outputvals[savename]['gt_val_final']\n",
    "                    #tn, fp, fn, tp= outputvals[savename]['matrix_0.5']\n",
    "                    #average_precision= outputvals[savename]['average_precision_pl']\n",
    "                    \n",
    "                    #epoch_val_recall_pl,epoch_val_recall_ebs,epoch_val_recall_unk = recall_val\n",
    "                    #epoch_val_acc,epoch_val_acc_pl,epoch_val_acc_ebs,epoch_val_acc_unk = acc_val\n",
    "                    outputvals[savename]={'loss_train_epoch':loss_train_epoch, 'loss_val_epoch':loss_val_epoch,\n",
    "                              'acc_val_epoch':acc_val_epoch,'ap_val_epoch':ap_val_epoch,\n",
    "                              'pred_val_final':pred_val_final, 'gt_val_final':gt_val_final,\n",
    "                              'k':kcount,'fpath':fpath,'aug':aug,'mod':mod,'unqid':fpath+'_'+aug+'_'+mod}\n",
    "                    '''\n",
    "                kcount+=1\n",
    "                pickle.dump(outputvals,open(path.join(foldname,savedicname+'.pickle'),'wb'))\n",
    "                \n",
    "            print('#Assembling Cross-Val results after '+str(kcount)+' CVs')\n",
    "            dickeys=[key for key in outputvals if (outputvals[key]['unqid']==fpath+'_'+aug+'_'+mod)*~np.isnan(outputvals[key]['k'])]\n",
    "            loss_train_epoch=[];loss_val_epoch=[];gt_val_final=[];pred_val_final=[]\n",
    "            ap_val_epoch=[];acc_val_epoch=[]\n",
    "            for key in dickeys:\n",
    "                gt_val_final.append(outputvals[key]['gt_val_final'])\n",
    "                pred_val_final.append(outputvals[key]['pred_val_final'])\n",
    "                loss_train_epoch.append(outputvals[key]['loss_train_epoch'])\n",
    "                loss_val_epoch.append(outputvals[key]['loss_val_epoch'])\n",
    "                ap_val_epoch.append(outputvals[key]['ap_val_epoch'])\n",
    "                acc_val_epoch.append(outputvals[key]['acc_val_epoch'])\n",
    "            gt_val_final=np.hstack(gt_val_final)\n",
    "            pred_val_final=np.hstack(pred_val_final)\n",
    "            loss_train_epoch=[np.hstack(l) for l in loss_train_epoch]\n",
    "            loss_val_epoch=[np.hstack(l) for l in loss_val_epoch]\n",
    "            ap_val_epoch=[np.hstack(l) for l in ap_val_epoch]\n",
    "            acc_val_epoch=[np.hstack(l) for l in acc_val_epoch]\n",
    "            ### transform from loss per sample to loss per batch (multiple by batch size to compare to Chris')\n",
    "            #loss_train_batch = [x.item()* batch_size for x in loss_train_epoch]\n",
    "            #loss_val_batch = [x.item()* batch_size for x in loss_val_epoch]\n",
    "\n",
    "            ### calculate average precision + precision-recall curves\n",
    "            P, R, _ = precision_recall_curve(gt_val_final, pred_val_final)\n",
    "            AP = average_precision_score(gt_val_final, pred_val_final, average=None)\n",
    "            print(\"average precision = {0:0.4f}\".format(AP))\n",
    "\n",
    "            ### convert prediction to bytes based on threshold\n",
    "            thresh = [0.9, 0.7, 0.5]\n",
    "            prec_thresh, recall_thresh = np.zeros(len(thresh)), np.zeros(len(thresh))\n",
    "            for n, nval in enumerate(thresh):\n",
    "                pred_byte = np.zeros(len(pred_val_final))\n",
    "                for i, val in enumerate(pred_val_final):\n",
    "                    if val > nval:\n",
    "                        pred_byte[i] = 1.0\n",
    "                    else:\n",
    "                        pred_byte[i] = 0.0\n",
    "                prec_thresh[n] = precision_score(gt_val_final, pred_byte)\n",
    "                recall_thresh[n] = recall_score(gt_val_final, pred_byte)\n",
    "                print(savename,\": thresh = {0:0.2f}, precision = {1:0.2f}, recall = {2:0.2f}\".format(thresh[n], prec_thresh[n], recall_thresh[n]))\n",
    "                tn, fp, fn, tp = confusion_matrix(gt_val_final, pred_byte).ravel()\n",
    "                print(savename,\":    TN = {0:0}, FP = {1:0}, FN = {2:0}, TP = {3:0}\".format(tn, fp, fn, tp))\n",
    "            for key in dickeys:\n",
    "                outputvals[savename].update({'CV_P':P,'CV_R':R,'CV_AP':AP,'CV_matrix_0.5':[tn, fp, fn, tp]})\n",
    "            \n",
    "            outputvals['exonet_CVall_'+fpath+'_'+aug+'_'+mod]={'loss_train_epoch':loss_train_epoch, 'loss_val_epoch':loss_val_epoch,\n",
    "                                          'acc_val_epoch':acc_val_epoch,'ap_val_epoch':ap_val_epoch,\n",
    "                                          'pred_val_final':pred_val_final, 'gt_val_final':gt_val_final,\n",
    "                                          'k':np.nan,'fpath':fpath,'aug':aug,'mod':mod,'unqid':fpath+'_'+aug+'_'+mod,\n",
    "                                          'P':P,'R':R,'AP':AP,'matrix_0.5':[tn, fp, fn, tp]}\n",
    "\n",
    "            plt.clf()\n",
    "            plt.subplot(2,2,1)\n",
    "            ### plot values\n",
    "            plt.step(R, P)\n",
    "            plt.title(fpath+'_'+aug+'_'+mod+': Precision vs. Recall, AP={0:0.3f}'.format(AP))\n",
    "\n",
    "            plt.subplot(2,2,2)\n",
    "            plt.title(fpath+'_'+aug+'_'+mod+': Loss per Epoch')\n",
    "            #plt.plot(np.arange(len(loss_train_batch)), loss_train_batch,color=sns.color_palette()[0])\n",
    "            #plt.plot(np.arange(len(loss_val_batch)), loss_val_batch,color=sns.color_palette()[1])\n",
    "            for n,key in enumerate(dickeys):\n",
    "                plt.plot(np.array(outputvals[key]['loss_train_epoch']),':',alpha=0.75,color=sns.color_palette()[n])\n",
    "                plt.plot(np.array(outputvals[key]['loss_val_epoch']),'--',alpha=0.75,color=sns.color_palette()[n])\n",
    "                plt.plot([0.0,0.0],[0.0,0.0],'-',alpha=0.75,label=\"k=\"+str(outputvals[key]['k']))\n",
    "            plt.plot([0.0,0.0],[0.0,0.0],'--',label='validation')\n",
    "            plt.plot([0.0,0.0],[0.0,0.0],':',label='training')\n",
    "            plt.legend(loc=2)\n",
    "            \n",
    "            plt.subplot(2,2,3)\n",
    "            plt.title(fpath+'_'+aug+'_'+mod+': Average Precision per Epoch')\n",
    "            #plt.plot(np.arange(len(ap_val_epoch)), ap_val_epoch)\n",
    "            for key in dickeys:\n",
    "                plt.plot(outputvals[key]['ap_val_epoch'],'-',alpha=0.75)\n",
    "                plt.plot([0.0,0.0],[0.0,0.0],'-',alpha=0.75,label=\"k=\"+str(outputvals[key]['k']))\n",
    "            plt.legend(loc=4)\n",
    "            \n",
    "            plt.subplot(2,2,4)\n",
    "            plt.title(fpath+'_'+aug+'_'+mod+': Accuracy per Epoch')\n",
    "            #plt.plot(np.arange(len(acc_val_epoch)), acc_val_epoch)\n",
    "            for key in dickeys:\n",
    "                plt.plot(outputvals[key]['acc_val_epoch'],':',alpha=0.75)\n",
    "                plt.plot([0.0,0.0],[0.0,0.0],'-',alpha=0.75,label=\"k=\"+str(outputvals[key]['k']))\n",
    "            plt.legend(loc=4)\n",
    "            \n",
    "            plt.savefig(path.join(foldname,'exonet_CVall_'+fpath+'_'+aug+'_'+mod+\".png\"))\n",
    "            pickle.dump(outputvals,open(path.join(foldname,savedicname+'.pickle'),'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Centroids To Best Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtranetModel_101_cent(nn.Module):\n",
    "\n",
    "    '''\n",
    "    PURPOSE: DEFINE EXTRANET MODEL ARCHITECTURE\n",
    "    INPUT: GLOBAL + LOCAL LIGHT CURVES AND CENTROID CURVES, STELLAR PARAMETERS\n",
    "    OUTPUT: BINARY CLASSIFIER\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        ### initialize model\n",
    "        super(ExtranetModel_101_cent, self).__init__()\n",
    "\n",
    "        ### define global convolutional lalyer\n",
    "        self.fc_global = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5, stride=2),\n",
    "            nn.Conv1d(16, 32, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5, stride=2),\n",
    "            nn.Conv1d(32, 64, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5, stride=2),\n",
    "            nn.Conv1d(64, 128, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5, stride=2),\n",
    "            nn.Conv1d(128, 256, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5, stride=2),\n",
    "        )\n",
    "\n",
    "        ### define local convolutional lalyer\n",
    "        self.fc_local = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(7, stride=2),\n",
    "            nn.Conv1d(16, 32, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(7, stride=2),\n",
    "        )\n",
    "        \n",
    "        ### define local convolutional lalyer\n",
    "        self.fc_local_cent = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(7, stride=2),\n",
    "            nn.Conv1d(16, 32, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(7, stride=2),\n",
    "        )\n",
    "        ### define fully connected layer that combines both views\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(8518, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            ### need output of 1 because using BCE for loss\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x_local, x_global, x_local_cent, x_star):\n",
    "\n",
    "        ### get outputs of global and local convolutional layers\n",
    "        out_global = self.fc_global(x_global)\n",
    "        out_local = self.fc_local(x_local)\n",
    "        out_local_cent = self.fc_local_cent(x_local_cent)\n",
    "        \n",
    "        ### flattening outputs (multi-dim tensor) from convolutional layers into vector\n",
    "        out_global = out_global.view(out_global.shape[0], -1)\n",
    "        out_local = out_local.view(out_local.shape[0], -1)\n",
    "        out_local_cent = out_local_cent.view(out_local_cent.shape[0], -1)\n",
    "\n",
    "        ### join two outputs together\n",
    "        out = torch.cat([out_global, out_local, out_local_cent, x_star.squeeze(1)], dim=1)\n",
    "        out = self.final_layer(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "            #m.bias.data.zero_()\n",
    "            #m.weight.data.zero_()\n",
    "            #m.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeplerDataLoaderCrossVal_cent(Dataset):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    PURPOSE: DATA LOADER FOR KERPLER LIGHT CURVES\n",
    "    INPUT: PATH TO DIRECTOR WITH LIGHT CURVES + INFO FILES\n",
    "    OUTPUT: LOCAL + GLOBAL VIEWS, LABELS\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, infofiles):\n",
    "        ### list of global, local, and info files (assumes certain names of files)\n",
    "        self.flist_global,self.flist_local=[],[]\n",
    "        for i,val in enumerate(infofiles):\n",
    "            self.flist_global.append(val.replace('_info.npy','_glob.npy'))\n",
    "            self.flist_local.append(val.replace('_info.npy','_loc.npy'))\n",
    "        self.flist_info = infofiles\n",
    "        \n",
    "        ### list of whitened centroid files\n",
    "        #self.flist_global_cen = np.sort(glob.glob(os.path.join(filepath, '*global_cen_w.npy')))\n",
    "        #self.flist_local_cen = np.sort(glob.glob(os.path.join(filepath, '*local_cen_w.npy')))\n",
    "        \n",
    "        ### ids = {TIC}_{TCE}\n",
    "        self.ids = np.sort(['_'.join(x.split('/')[-1].split('_')[1:3]) for x in self.flist_global])\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.ids.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        ### grab local and global views\n",
    "        data_global = np.nan_to_num(np.load(self.flist_global[idx],encoding='latin1'))\n",
    "        data_local = np.nan_to_num(np.load(self.flist_local[idx],encoding='latin1'))\n",
    "        '''\n",
    "        ### grab centroid views\n",
    "        data_global_cen = data_global[:,1]\n",
    "        '''\n",
    "        data_local_cen = data_local[:,1]\n",
    "        data_local=data_local[:,0]\n",
    "        data_global = data_global[:,0]\n",
    "        \n",
    "        ### info file contains: [0]kic, [1]tce, [2]period, [3]epoch, [4]duration, [5]label)\n",
    "        data_info = np.load(self.flist_info[idx],encoding='latin1')\n",
    "        label=1 if data_info[6]=='PL' else 0\n",
    "        \n",
    "        return (data_local.astype(float), data_global.astype(float), data_local_cen.astype(float), np.nan_to_num(data_info[7:13].astype(float))), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        torch.nn.init.constant_(m.bias.data, 0)\n",
    "        #m.bias.data.zero_()\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        torch.nn.init.constant_(m.bias.data, 0)\n",
    "        #torch.nn.init.xavier_uniform(m.bias.data)\n",
    "        #m.bias.data.fill_(0.01)\n",
    "        #m.weight.data.zero_()#m.weight.size()[1],m.weight.size()[0])\n",
    "        #m.bias.data=torch.ones(m.weight.size()[0])\n",
    "\n",
    "#model=model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cent(n_epochs, kepler_data_loader, kepler_val_loader, model, criterion, optimizer, augment=''):\n",
    "\n",
    "    '''\n",
    "    \n",
    "    PURPOSE: TRAIN MODEL \n",
    "    \n",
    "    INPUTS:  num_epoch = number of epochs for training\n",
    "             kepler_data_loader = data loader for Kepler dataset\n",
    "             model = model use for training\n",
    "             criterion = criterion for calculating loss\n",
    "             \n",
    "    OUTPUT:  epoch_{train/val}_loss = training set loss for each epoch\n",
    "             epoch_val_acc = validation set accuracy for each epoch\n",
    "             epoch_val_ap = validation set avg. precision for each epoch\n",
    "             final_val_pred = validation predictions from final model\n",
    "             final_val_gt = validation ground truths\n",
    "    \n",
    "    '''\n",
    "    minloss=1e9;bests=[]\n",
    "    \n",
    "    ### empty arrays to fill per-epoch outputs\n",
    "    epoch_train_loss = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_val_acc = []\n",
    "    epoch_val_ap = []\n",
    "\n",
    "    ### loop over number of epochs of training\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "        ####################\n",
    "        ### for training set\n",
    "        \n",
    "        ### loop over batches\n",
    "        train_loss = torch.zeros(1).cuda()\n",
    "        for x_train_data, y_train in kepler_data_loader:\n",
    "            \n",
    "            ### get local view, global view, and label for training\n",
    "            x_train_local, x_train_global, x_train_local_cent, x_train_star = x_train_data\n",
    "            \n",
    "            '''#Augmentation:'''\n",
    "            if 'all' in augment and 'noise' not in augment:\n",
    "                #Getting RMS (not std) of each lightcurve\n",
    "                local_roundn=int(np.floor(len(x_train_local[0])*0.35))\n",
    "                local_stds=np.hstack((x_train_local[:,:local_roundn],x_train_local[:,len(x_train_local[0])-local_roundn:]))\n",
    "                local_stds=np.std(local_stds,axis=0)\n",
    "\n",
    "                local_cent_stds=np.hstack((x_train_local[:,:local_roundn],x_train_local_cent[:,len(x_train_local_cent[0])-local_roundn:]))\n",
    "                \n",
    "                global_roundn=int(np.floor(len(x_train_global[0])*0.35))\n",
    "                global_stds=np.hstack((x_train_global[:,:global_roundn],x_train_global[:,len(x_train_global[0])-global_roundn:]))\n",
    "                global_stds=np.nanstd(global_stds,axis=0)\n",
    "                \n",
    "            '''\n",
    "            if 'all' in augment and 'yscale' not in augment:\n",
    "                intransit_roundn=int(np.floor(len(x_train_local[0])*0.415))\n",
    "                N_intrans=len(x_train_local[0])-2*intransit_roundn\n",
    "                intransit_stds=x_train_local[:,]\n",
    "                intransit_stds=np.nanstd(intransit_stds,axis=0)/np.sqrt(N_intrans)\n",
    "            '''\n",
    "            for batch_ind in range(x_train_global.shape[0]):\n",
    "                \n",
    "                if 'all' in augment and 'noise' not in augment:       \n",
    "                    sig_noise = abs(np.random.normal(0, 1.0))\n",
    "                    x_train_local[batch_ind] = x_train_local[batch_ind].float() + torch.Tensor(np.random.normal(0.0,sig_noise*local_stds[batch_ind],len(x_train_local[batch_ind])))\n",
    "                    \n",
    "                    if np.sum((local_cent_stds[batch_ind]!=0.0)&(~np.isnan(local_cent_stds[batch_ind])))>1:\n",
    "                        cent_noise=np.nanstd(local_cent_stds[batch_ind][(local_cent_stds[batch_ind]!=0.0)&(~np.isnan(local_cent_stds[batch_ind]))])\n",
    "                        x_train_local_cent[batch_ind] = x_train_local_cent[batch_ind].float() + torch.Tensor(np.random.normal(0.0,sig_noise*cent_noise,len(x_train_local_cent[batch_ind])))\n",
    "\n",
    "                    x_train_global[batch_ind] = x_train_global[batch_ind].float() + torch.Tensor(np.random.normal(0.0,sig_noise*global_stds[batch_ind],len(x_train_global[batch_ind])))\n",
    "\n",
    "                '''\n",
    "                if 'all' in augment or 'yscale' in augment:\n",
    "                    ### add random gaussian scaling\n",
    "                    scale_tensor = abs(np.random.normal(1.0, intransit_stds[batch_ind]))\n",
    "                    \n",
    "                    x_train_local[batch_ind] = x_train_local[batch_ind].float() * scale_tensor\n",
    "                    x_train_local_cent[batch_ind] = x_train_local_cent[batch_ind].float() * scale_tensor\n",
    "\n",
    "                    x_train_global[batch_ind] = x_train_global[batch_ind].float() * scale_tensor\n",
    "                '''\n",
    "                \n",
    "                if 'all' in augment and 'xshift' not in augment:\n",
    "                   ### shift by some random iteger\n",
    "                    shift_local = np.random.randint(-5, high=5)\n",
    "                    x_train_local[batch_ind] = roll_tensor(x_train_local[batch_ind], shift_local)\n",
    "                    x_train_local_cent[batch_ind] = roll_tensor(x_train_local_cent[batch_ind], shift_local)\n",
    "                    shift_global = np.random.randint(-30, high=30)\n",
    "                    x_train_global[batch_ind] = roll_tensor(x_train_global[batch_ind], shift_global)\n",
    "                \n",
    "                if 'all' in augment and 'mirror' not in augment:\n",
    "                    if random() < 0.5:\n",
    "                        x_train_local[batch_ind] = invert_tensor(x_train_local[batch_ind])\n",
    "                        x_train_local_cent[batch_ind] = invert_tensor(x_train_local_cent[batch_ind])\n",
    "                        x_train_global[batch_ind] =invert_tensor(x_train_global[batch_ind])\n",
    "            \n",
    "            x_train_local = Variable(x_train_local).type(torch.FloatTensor).cuda()\n",
    "            x_train_local_cent = Variable(x_train_local_cent).type(torch.FloatTensor).cuda()\n",
    "            x_train_global = Variable(x_train_global).type(torch.FloatTensor).cuda()    \n",
    "            x_train_star = Variable(x_train_star).type(torch.FloatTensor).cuda()        \n",
    "            y_train = Variable(y_train).type(torch.FloatTensor).cuda()\n",
    "\n",
    "            ### fix dimensions for next steps\n",
    "            x_train_local = x_train_local.unsqueeze(1) #NO UNSQUEEZING WHEN N_CHANNELS!=1\n",
    "            x_train_local_cent = x_train_local_cent.unsqueeze(1)\n",
    "            x_train_global = x_train_global.unsqueeze(1)\n",
    "            x_train_star = x_train_star.unsqueeze(1)\n",
    "            y_train = y_train.unsqueeze(1)\n",
    "\n",
    "            ### calculate loss using model\n",
    "            output_train = model(x_train_local, x_train_global, x_train_local_cent, x_train_star)\n",
    "            #print(output_train.max())\n",
    "            #print(output_train.min())\n",
    "            ##assert (output_train.cpu().numpy() >= 0. & output_train.cpu().numpy() <= 1.).all()\n",
    "            loss = criterion(output_train, y_train)\n",
    "            train_loss += loss.data\n",
    "\n",
    "            ### train model (zero gradients and back propogate results)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        ### record training loss for this epoch (divided by size of training dataset)\n",
    "        epoch_train_loss.append(train_loss.cpu().numpy() / len(kepler_data_loader.dataset))\n",
    "        \n",
    "        ######################\n",
    "        ### for validation set\n",
    "        \n",
    "        ### loop over batches\n",
    "        val_pred, val_gt, val_loss, num_corr = [], [], 0, 0\n",
    "        for x_val_data, y_val in kepler_val_loader:\n",
    "                        \n",
    "            ### get local view, global view, and label for validating\n",
    "            x_val_local, x_val_global, x_val_local_cent, x_val_star = x_val_data\n",
    "            x_val_local = Variable(x_val_local).type(torch.FloatTensor).cuda()\n",
    "            x_val_local_cent = Variable(x_val_local_cent).type(torch.FloatTensor).cuda()\n",
    "            x_val_global = Variable(x_val_global).type(torch.FloatTensor).cuda()\n",
    "            x_val_star = Variable(x_val_star).type(torch.FloatTensor).cuda()\n",
    " \n",
    "            ### fix dimensions for next steps\n",
    "            y_val = Variable(y_val).type(torch.FloatTensor).cuda()\n",
    "            x_val_local = x_val_local.unsqueeze(1)\n",
    "            x_val_local_cent = x_val_local_cent.unsqueeze(1)\n",
    "            x_val_global = x_val_global.unsqueeze(1)\n",
    "            x_val_star = x_val_star.unsqueeze(1)\n",
    "            y_val = y_val.unsqueeze(1)\n",
    "\n",
    "            ### calculate loss & add to sum over all batches\n",
    "            output_val = model(x_val_local, x_val_global, x_val_local_cent, x_val_star)\n",
    "            loss_val = criterion(output_val, y_val)\n",
    "            val_loss += loss_val.data\n",
    "\n",
    "            ### get number of correct predictions using threshold=0.5\n",
    "            ### & sum over all batches\n",
    "            output_pred = output_val >= 0.5\n",
    "            num_corr += output_pred.eq(y_val.byte()).sum().item()\n",
    "                        \n",
    "            ### record predictions and ground truth by model\n",
    "            ### (used for AP per epoch; reset at each epoch; final values output)\n",
    "            val_pred.append(output_val.data.cpu().numpy())\n",
    "            val_gt.append(y_val.data.cpu().numpy())\n",
    "            \n",
    "        ### record validation loss calculate for this epoch (divided by size of validation dataset)\n",
    "        epoch_val_loss.append(val_loss.cpu().numpy() / len(kepler_val_loader.dataset))\n",
    "        #print(\"epoch_val_loss\",epoch_val_loss,\" from \",val_loss.cpu().numpy())\n",
    "        \n",
    "        ### record validation accuracy (# correct predictions in val set) for this epoch\n",
    "        epoch_val_acc.append(num_corr / len(kepler_val_loader.dataset))\n",
    "        \n",
    "        ### calculate average precision for this epoch\n",
    "        epoch_val_ap.append(average_precision_score(np.concatenate(val_gt).ravel(), np.concatenate(val_pred).ravel(), average=None))            \n",
    "        \n",
    "        # Stopping the loop when the val_loss is below 1 (eg not ridiculously high), but has started to increase wrt previous bin of 25:\n",
    "        if epoch>75 and np.median(epoch_val_loss[-25:])<1.0 and (np.average(epoch_val_loss[-25:])-np.average(epoch_val_loss[-50:-25]))/np.std(epoch_val_loss[-25:])>1.25:\n",
    "            return epoch_train_loss, epoch_val_loss, epoch_val_acc, epoch_val_ap, bests[0], bests[1]\n",
    "        if epoch%5==0:\n",
    "            print(\"LOSS: train  =  \",epoch_train_loss[-1],' ,   val  =  ',epoch_val_loss[-1],' ,   acc  =  ',epoch_val_acc[-1])\n",
    "\n",
    "        if epoch_val_loss[-1]<minloss:\n",
    "            bests=[np.concatenate(val_pred).ravel(),np.concatenate(val_gt).ravel()]\n",
    "            minloss=epoch_val_loss[-1]\n",
    "\n",
    "    ### grab final predictions and ground truths for validation set\n",
    "    final_val_pred = bests[0]\n",
    "    final_val_gt = bests[1]\n",
    "    \n",
    "    return epoch_train_loss, epoch_val_loss, epoch_val_acc, epoch_val_ap, final_val_pred, final_val_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "savedicname='output_dict_binary_exonet_withCV_yscale_cent'\n",
    "foldname='binary_exonet_withCV_yscale_cent'\n",
    "\n",
    "if path.exists(path.join(foldname,savedicname+'.pickle')):\n",
    "    outputvals_cent=pickle.load(open(path.join(foldname,savedicname+'.pickle'),'rb'))\n",
    "else:\n",
    "    outputvals_cent={}\n",
    "\n",
    "if not path.isdir(foldname):\n",
    "    os.system('mkdir '+foldname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('error', RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balancer exists\n",
      "training exonet_CV0_101_all2_Big_centroids\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ab1246f88d48d89f44e4717f2d1f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: train  =   [0.07110046]  ,   val  =   0.02114476618887212  ,   acc  =   0.851166377482167\n",
      "LOSS: train  =   [0.02362137]  ,   val  =   0.013228608207342275  ,   acc  =   0.851166377482167\n",
      "LOSS: train  =   [0.02355318]  ,   val  =   0.013200379300489807  ,   acc  =   0.851166377482167\n",
      "LOSS: train  =   [0.02356813]  ,   val  =   0.013198758403483167  ,   acc  =   0.851166377482167\n",
      "LOSS: train  =   [0.02358048]  ,   val  =   0.01319911435364252  ,   acc  =   0.851166377482167\n",
      "LOSS: train  =   [0.02355073]  ,   val  =   0.013198642204877428  ,   acc  =   0.851166377482167\n",
      "LOSS: train  =   [0.02360758]  ,   val  =   0.0132004984408324  ,   acc  =   0.851166377482167\n",
      "LOSS: train  =   [0.02350894]  ,   val  =   0.01319752875747813  ,   acc  =   0.851166377482167\n",
      "LOSS: train  =   [0.02353844]  ,   val  =   0.013197811164216129  ,   acc  =   0.851166377482167\n",
      "saving exonet_CV0_101_all2_Big_centroids\n",
      "Accumulating dataset class distribution...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965008cbad7c4c89b280b725920bb57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20745), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing dataset with class distribution: [17623, 3122]\n",
      "training exonet_CV1_101_all2_Big_centroids\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86dddd9e5f14951a3dd05837bd5e0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: train  =   [0.04976527]  ,   val  =   0.01380927913031705  ,   acc  =   0.847473968376398\n",
      "LOSS: train  =   [0.03786083]  ,   val  =   0.018493203642374754  ,   acc  =   0.847473968376398\n",
      "LOSS: train  =   [0.03678989]  ,   val  =   0.021725151100261672  ,   acc  =   0.847473968376398\n",
      "LOSS: train  =   [0.03678788]  ,   val  =   0.021786121525821472  ,   acc  =   0.847473968376398\n",
      "LOSS: train  =   [0.03678788]  ,   val  =   0.021786122996973523  ,   acc  =   0.847473968376398\n",
      "LOSS: train  =   [0.03678788]  ,   val  =   0.021786124468125573  ,   acc  =   0.152526031623602\n",
      "LOSS: train  =   [0.03678788]  ,   val  =   0.021786124468125573  ,   acc  =   0.152526031623602\n",
      "LOSS: train  =   [0.03678788]  ,   val  =   0.021786124468125573  ,   acc  =   0.152526031623602\n",
      "LOSS: train  =   [0.03678788]  ,   val  =   0.021786122996973523  ,   acc  =   0.847473968376398\n",
      "LOSS: train  =   [0.03678788]  ,   val  =   0.021786122996973523  ,   acc  =   0.847473968376398\n",
      "saving exonet_CV1_101_all2_Big_centroids\n",
      "Accumulating dataset class distribution...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cc4fd35936408dbd2686553594515d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20745), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing dataset with class distribution: [17610, 3135]\n",
      "training exonet_CV2_101_all2_Big_centroids\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d6dbefbaab4dae8ba1bc2e56c56d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: train  =   [0.04972257]  ,   val  =   0.013708302195909811  ,   acc  =   0.8499807173158503\n",
      "LOSS: train  =   [0.03782963]  ,   val  =   0.018461207556439513  ,   acc  =   0.8499807173158503\n",
      "LOSS: train  =   [0.03675653]  ,   val  =   0.021723359237064782  ,   acc  =   0.8499807173158503\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786121525821472  ,   acc  =   0.8499807173158503\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786122996973523  ,   acc  =   0.8499807173158503\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786124468125573  ,   acc  =   0.15001928268414963\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786121525821472  ,   acc  =   0.8499807173158503\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786124468125573  ,   acc  =   0.15001928268414963\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786122996973523  ,   acc  =   0.8499807173158503\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786124468125573  ,   acc  =   0.15001928268414963\n",
      "saving exonet_CV2_101_all2_Big_centroids\n",
      "Accumulating dataset class distribution...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e63e3749454a5a84597f91b69e5993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20745), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing dataset with class distribution: [17633, 3112]\n",
      "training exonet_CV3_101_all2_Big_centroids\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b729e1a8b5a48aea98c5816d5064c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: train  =   [0.04980796]  ,   val  =   0.01388704716998364  ,   acc  =   0.8455456999614346\n",
      "LOSS: train  =   [0.03789204]  ,   val  =   0.018518909082144236  ,   acc  =   0.8455456999614346\n",
      "LOSS: train  =   [0.03682325]  ,   val  =   0.02172681791553431  ,   acc  =   0.8455456999614346\n",
      "LOSS: train  =   [0.03682129]  ,   val  =   0.021786121525821472  ,   acc  =   0.8455456999614346\n",
      "LOSS: train  =   [0.03682129]  ,   val  =   0.021786121525821472  ,   acc  =   0.8455456999614346\n",
      "LOSS: train  =   [0.03682129]  ,   val  =   0.021786124468125573  ,   acc  =   0.15445430003856536\n",
      "LOSS: train  =   [0.03682129]  ,   val  =   0.021786124468125573  ,   acc  =   0.15445430003856536\n",
      "LOSS: train  =   [0.03682129]  ,   val  =   0.021786124468125573  ,   acc  =   0.15445430003856536\n",
      "LOSS: train  =   [0.03682129]  ,   val  =   0.021786153891166574  ,   acc  =   0.15445430003856536\n",
      "LOSS: train  =   [0.03682129]  ,   val  =   0.021786124468125573  ,   acc  =   0.15445430003856536\n",
      "saving exonet_CV3_101_all2_Big_centroids\n",
      "Accumulating dataset class distribution...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6af01c0123a4b2a9e8026e32fd0398e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20745), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing dataset with class distribution: [17603, 3142]\n",
      "training exonet_CV4_101_all2_Big_centroids\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628be14a429e4505b66c897ec02dd34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: train  =   [0.04972257]  ,   val  =   0.013654241771528816  ,   acc  =   0.8513305052063247\n",
      "LOSS: train  =   [0.03782963]  ,   val  =   0.018446728477963508  ,   acc  =   0.8513305052063247\n",
      "LOSS: train  =   [0.03675653]  ,   val  =   0.021723112083520384  ,   acc  =   0.8513305052063247\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786121525821472  ,   acc  =   0.8513305052063247\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786121525821472  ,   acc  =   0.8513305052063247\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786124468125573  ,   acc  =   0.14866949479367528\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786121525821472  ,   acc  =   0.8513305052063247\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786124468125573  ,   acc  =   0.14866949479367528\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786121525821472  ,   acc  =   0.8513305052063247\n",
      "LOSS: train  =   [0.03675447]  ,   val  =   0.021786124468125573  ,   acc  =   0.14866949479367528\n",
      "saving exonet_CV4_101_all2_Big_centroids\n",
      "#Assembling Cross-Val results after 5 CVs\n",
      "average precision = 0.2177\n",
      "exonet_CV4_101_all2_Big_centroids : thresh = 0.90, precision = 0.90, recall = 0.04\n",
      "exonet_CV4_101_all2_Big_centroids :    TN = 61626, FP = 49, FN = 10481, TP = 453\n",
      "exonet_CV4_101_all2_Big_centroids : thresh = 0.70, precision = 0.84, recall = 0.05\n",
      "exonet_CV4_101_all2_Big_centroids :    TN = 61566, FP = 109, FN = 10340, TP = 594\n",
      "exonet_CV4_101_all2_Big_centroids : thresh = 0.50, precision = 0.24, recall = 0.13\n",
      "exonet_CV4_101_all2_Big_centroids :    TN = 57100, FP = 4575, FN = 9504, TP = 1430\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-4cecace134fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;31m#plt.plot(np.arange(len(loss_val_batch)), loss_val_batch,color=sns.color_palette()[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdickeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputvals_cent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_train_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_palette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputvals_cent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_val_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_palette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"k=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputvals_cent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1008 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "\n",
    "filepaths={'101':'/home/hugh/processed_dv_101/','201':'/home/hugh/processed_dv_201/'}\n",
    "\n",
    "augment={'all':'all','noise':'noise','xshift':'xshift','mirror':'mirror'}\n",
    "\n",
    "#for batch in batch_sizes:\n",
    "#    for lr in lrs:            \n",
    "nkfolds=5\n",
    "fpath='101'\n",
    "### divide train and validation sets\n",
    "kf = KFold(n_splits=nkfolds, shuffle=True)\n",
    "files_all = np.sort(glob.glob(path.join(filepaths[fpath],'all','*info.npy')))\n",
    "'''\n",
    "#kepler_train_data = KeplerDataLoader(filepath=path.join(filepaths[fpath],'train'))\n",
    "#kepler_val_data = KeplerDataLoader(filepath=path.join(filepaths[fpath],'val'))\n",
    "'''\n",
    "\n",
    "mod='Big'\n",
    "aug='all'\n",
    "### loop over folds\n",
    "kcount = 0\n",
    "precision_all, recall_all, ap_all = [], [], []\n",
    "for train_index, val_index in kf.split(files_all):\n",
    "    savename='exonet_CV'+str(kcount)+'_'+fpath+'_'+aug+'2_'+mod+'_centroids'\n",
    "    if savename not in outputvals_cent.keys():\n",
    "        model=ExtranetModel_101_cent().cuda()\n",
    "        model=model.apply(weights_init)\n",
    "        lr = 0.4e-5\n",
    "        \n",
    "        if kcount==0:\n",
    "            #Loading the pre-training (eg random) state_dict\n",
    "            init_statedict=model.state_dict().copy()\n",
    "        else:\n",
    "            #Loading the state_dict from pre-training.\n",
    "            model.load_state_dict(init_statedict)\n",
    "\n",
    "        optimizer=None\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=lr*0.025)\n",
    "        criterion=None\n",
    "        criterion = nn.BCELoss()\n",
    "        batch_size = 32\n",
    "        n_epochs = 50\n",
    "\n",
    "        ### grab training and validation data\n",
    "        files_train, files_val = files_all[train_index], files_all[val_index]\n",
    "        kepler_val_data = KeplerDataLoaderCrossVal_cent(infofiles=files_val)\n",
    "        kepler_train_data = KeplerDataLoaderCrossVal_cent(infofiles=files_train)\n",
    "\n",
    "        #Loading balancer:\n",
    "        fname=path.join(foldname,savename+'_BBS.pickle')\n",
    "        if path.exists(fname):\n",
    "            print(\"balancer exists\")\n",
    "            kepler_batch_sampler = pickle.load(open(fname,'rb'))\n",
    "        else:\n",
    "            kepler_batch_sampler = BalancedBatchSampler(batch_size, kepler_train_data, 2) #batch_size, dataset, n_classes\n",
    "            pickle.dump(kepler_batch_sampler, open(fname,'wb'))\n",
    "\n",
    "        kepler_data_loader = DataLoader(kepler_train_data, batch_sampler = kepler_batch_sampler, num_workers=4)\n",
    "        kepler_val_loader = DataLoader(kepler_val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "        ### train model\n",
    "        print(\"training \"+savename)\n",
    "        loss_train_epoch, loss_val_epoch, acc_val_epoch, ap_val_epoch, pred_val_final, gt_val_final  = train_model_cent(n_epochs, kepler_data_loader, kepler_val_loader, model, criterion, optimizer,augment=augment[aug])\n",
    "        print(\"saving \"+savename)\n",
    "        torch.save(model.state_dict(),path.join(foldname,savename+'.pth'))\n",
    "\n",
    "        outputvals_cent[savename]={'k':kcount,'fpath':fpath,'aug':aug,'mod':mod,'unqid':fpath+'_'+aug+'_'+mod,'pred_val_final':pred_val_final, 'gt_val_final':gt_val_final}\n",
    "        for potkey in ['loss_train_epoch','loss_val_epoch','acc_val_epoch','ap_val_epoch']:\n",
    "            exec('outputvals_cent[\\''+savename+'\\'][\\''+potkey+'\\']='+potkey)\n",
    "\n",
    "    else:\n",
    "        print(\"dictionary of trained values already exists\")\n",
    "        #outputvals_cent[savename]={'k':outputvals_cent[savename]['k'],'fpath':outputvals_cent[savename]['fpath'],\n",
    "        #                            'aug':outputvals_cent[savename]['aug'],'mod':outputvals_cent[savename]['mod'],\n",
    "        #                            'unqid':outputvals_cent[savename]['unqid'],'pred_val_final':outputvals_cent[savename]['pred_val_final'],\n",
    "        #                            'gt_val_final':outputvals_cent[savename]['gt_val_final']}\n",
    "        #for potkey in ['loss_train_epoch','loss_val_epoch','acc_val_epoch','ap_val_epoch']:\n",
    "        #    exec('outputvals_cent[\\''+savename+'\\'][\\''+potkey+'\\']=outputvals_cent[\\''+savename+'\\'][\\''+potkey+'\\']')\n",
    "\n",
    "        '''\n",
    "        loss_train_epoch=outputvals_cent[savename]['loss_train_epoch']\n",
    "        loss_val_epoch=outputvals_cent[savename]['loss_val_epoch']\n",
    "        acc_val_epoch=outputvals_cent[savename]['acc_val_epoch']\n",
    "        pred_val_final= outputvals_cent[savename]['pred_val_final']\n",
    "        gt_val_final= outputvals_cent[savename]['gt_val_final']\n",
    "        #tn, fp, fn, tp= outputvals_cent[savename]['matrix_0.5']\n",
    "        #average_precision= outputvals_cent[savename]['average_precision_pl']\n",
    "\n",
    "        #epoch_val_recall_pl,epoch_val_recall_ebs,epoch_val_recall_unk = recall_val\n",
    "        #epoch_val_acc,epoch_val_acc_pl,epoch_val_acc_ebs,epoch_val_acc_unk = acc_val\n",
    "        outputvals_cent[savename]={'loss_train_epoch':loss_train_epoch, 'loss_val_epoch':loss_val_epoch,\n",
    "                  'acc_val_epoch':acc_val_epoch,'ap_val_epoch':ap_val_epoch,\n",
    "                  'pred_val_final':pred_val_final, 'gt_val_final':gt_val_final,\n",
    "                  'k':kcount,'fpath':fpath,'aug':aug,'mod':mod,'unqid':fpath+'_'+aug+'_'+mod}\n",
    "        '''\n",
    "    kcount+=1\n",
    "    pickle.dump(outputvals_cent,open(path.join(foldname,savedicname+'.pickle'),'wb'))\n",
    "\n",
    "print('#Assembling Cross-Val results after '+str(kcount)+' CVs')\n",
    "dickeys=[key for key in outputvals_cent if (outputvals_cent[key]['unqid']==fpath+'_'+aug+'_'+mod)*~np.isnan(outputvals_cent[key]['k'])]\n",
    "loss_train_epoch=[];loss_val_epoch=[];gt_val_final=[];pred_val_final=[]\n",
    "ap_val_epoch=[];acc_val_epoch=[]\n",
    "for key in dickeys:\n",
    "    gt_val_final.append(outputvals_cent[key]['gt_val_final'])\n",
    "    pred_val_final.append(outputvals_cent[key]['pred_val_final'])\n",
    "    loss_train_epoch.append(outputvals_cent[key]['loss_train_epoch'])\n",
    "    loss_val_epoch.append(outputvals_cent[key]['loss_val_epoch'])\n",
    "    ap_val_epoch.append(outputvals_cent[key]['ap_val_epoch'])\n",
    "    acc_val_epoch.append(outputvals_cent[key]['acc_val_epoch'])\n",
    "gt_val_final=np.hstack(gt_val_final)\n",
    "pred_val_final=np.hstack(pred_val_final)\n",
    "loss_train_epoch=[np.hstack(l) for l in loss_train_epoch]\n",
    "loss_val_epoch=[np.hstack(l) for l in loss_val_epoch]\n",
    "ap_val_epoch=[np.hstack(l) for l in ap_val_epoch]\n",
    "acc_val_epoch=[np.hstack(l) for l in acc_val_epoch]\n",
    "### transform from loss per sample to loss per batch (multiple by batch size to compare to Chris')\n",
    "#loss_train_batch = [x.item()* batch_size for x in loss_train_epoch]\n",
    "#loss_val_batch = [x.item()* batch_size for x in loss_val_epoch]\n",
    "\n",
    "### calculate average precision + precision-recall curves\n",
    "P, R, _ = precision_recall_curve(gt_val_final, pred_val_final)\n",
    "AP = average_precision_score(gt_val_final, pred_val_final, average=None)\n",
    "print(\"average precision = {0:0.4f}\".format(AP))\n",
    "\n",
    "### convert prediction to bytes based on threshold\n",
    "thresh = [0.9, 0.7, 0.5]\n",
    "prec_thresh, recall_thresh = np.zeros(len(thresh)), np.zeros(len(thresh))\n",
    "for n, nval in enumerate(thresh):\n",
    "    pred_byte = np.zeros(len(pred_val_final))\n",
    "    for i, val in enumerate(pred_val_final):\n",
    "        if val > nval:\n",
    "            pred_byte[i] = 1.0\n",
    "        else:\n",
    "            pred_byte[i] = 0.0\n",
    "    prec_thresh[n] = precision_score(gt_val_final, pred_byte)\n",
    "    recall_thresh[n] = recall_score(gt_val_final, pred_byte)\n",
    "    print(savename,\": thresh = {0:0.2f}, precision = {1:0.2f}, recall = {2:0.2f}\".format(thresh[n], prec_thresh[n], recall_thresh[n]))\n",
    "    tn, fp, fn, tp = confusion_matrix(gt_val_final, pred_byte).ravel()\n",
    "    print(savename,\":    TN = {0:0}, FP = {1:0}, FN = {2:0}, TP = {3:0}\".format(tn, fp, fn, tp))\n",
    "for key in dickeys:\n",
    "    outputvals_cent[savename].update({'CV_P':P,'CV_R':R,'CV_AP':AP,'CV_matrix_0.5':[tn, fp, fn, tp]})\n",
    "\n",
    "outputvals_cent['exonet_CVall_'+fpath+'_'+aug+'_'+mod]={'loss_train_epoch':loss_train_epoch, 'loss_val_epoch':loss_val_epoch,\n",
    "                              'acc_val_epoch':acc_val_epoch,'ap_val_epoch':ap_val_epoch,\n",
    "                              'pred_val_final':pred_val_final, 'gt_val_final':gt_val_final,\n",
    "                              'k':np.nan,'fpath':fpath,'aug':aug,'mod':mod,'unqid':fpath+'_'+aug+'_'+mod,\n",
    "                              'P':P,'R':R,'AP':AP,'matrix_0.5':[tn, fp, fn, tp]}\n",
    "\n",
    "plt.clf()\n",
    "plt.subplot(2,2,1)\n",
    "### plot values\n",
    "plt.step(R, P)\n",
    "plt.title(fpath+'_'+aug+'_'+mod+': Precision vs. Recall, AP={0:0.3f}'.format(AP))\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(fpath+'_'+aug+'_'+mod+': Loss per Epoch')\n",
    "#plt.plot(np.arange(len(loss_train_batch)), loss_train_batch,color=sns.color_palette()[0])\n",
    "#plt.plot(np.arange(len(loss_val_batch)), loss_val_batch,color=sns.color_palette()[1])\n",
    "for n,key in enumerate(dickeys):\n",
    "    plt.plot(np.array(outputvals_cent[key]['loss_train_epoch']),':',alpha=0.75,color=sns.color_palette()[n])\n",
    "    plt.plot(np.array(outputvals_cent[key]['loss_val_epoch']),'--',alpha=0.75,color=sns.color_palette()[n])\n",
    "    plt.plot([0.0,0.0],[0.0,0.0],'-',alpha=0.75,label=\"k=\"+str(outputvals_cent[key]['k']))\n",
    "plt.plot([0.0,0.0],[0.0,0.0],'--',label='validation')\n",
    "plt.plot([0.0,0.0],[0.0,0.0],':',label='training')\n",
    "plt.legend(loc=2)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(fpath+'_'+aug+'_'+mod+': Average Precision per Epoch')\n",
    "#plt.plot(np.arange(len(ap_val_epoch)), ap_val_epoch)\n",
    "for key in dickeys:\n",
    "    plt.plot(outputvals_cent[key]['ap_val_epoch'],'-',alpha=0.75)\n",
    "    plt.plot([0.0,0.0],[0.0,0.0],'-',alpha=0.75,label=\"k=\"+str(outputvals_cent[key]['k']))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(fpath+'_'+aug+'_'+mod+': Accuracy per Epoch')\n",
    "#plt.plot(np.arange(len(acc_val_epoch)), acc_val_epoch)\n",
    "for key in dickeys:\n",
    "    plt.plot(outputvals_cent[key]['acc_val_epoch'],':',alpha=0.75)\n",
    "    plt.plot([0.0,0.0],[0.0,0.0],'-',alpha=0.75,label=\"k=\"+str(outputvals_cent[key]['k']))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.savefig(path.join(foldname,'exonet_CVall_'+fpath+'_'+aug+'_'+mod+\".png\"))\n",
    "pickle.dump(outputvals_cent,open(path.join(foldname,savedicname+'.pickle'),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With LR = 1e-5\n",
    "\n",
    "LOSS: train  =   [0.02709127]  ,   val  =   0.014523319548119095  ,   acc  =   0.8532870638133796\n",
    "\n",
    "LOSS: train  =   [0.0227688]  ,   val  =   0.013065480073568838  ,   acc  =   0.8532870638133796\n",
    "\n",
    "... Acc gets stuck :/\n",
    "\n",
    "With LR = 0.5e-5:\n",
    "\n",
    "LOSS: train  =   [0.03308793]  ,   val  =   0.01901011984541808  ,   acc  =   0.8424908424908425\n",
    "\n",
    "LOSS: train  =   [0.02296007]  ,   val  =   0.013639740937473641  ,   acc  =   0.8424908424908425\n",
    "\n",
    "LOSS: train  =   [0.0230208]  ,   val  =   0.01363973652486836  ,   acc  =   0.8424908424908425\n",
    "\n",
    "... Acc gets stuck :/\n",
    "\n",
    "With LR = 0.2e-5:\n",
    "\n",
    "LOSS: train  =   [0.0351423]  ,   val  =   0.020615969868048277  ,   acc  =   0.8538654328128013\n",
    "\n",
    "LOSS: train  =   [0.02721931]  ,   val  =   0.015485367518480697  ,   acc  =   0.8538654328128013\n",
    "\n",
    "LOSS: train  =   [0.02333767]  ,   val  =   0.013090011217195633  ,   acc  =   0.8538654328128013\n",
    "\n",
    "... Acc gets stuck :/\n",
    "\n",
    "REMOVED NAN/ZEROS FROM CENTROID STD - REDO CENTROID PROCESSING!\n",
    "\n",
    "With LR = 0.5e-5:\n",
    "\n",
    "LOSS: train  =   [0.03727083]  ,   val  =   0.021485666422478972  ,   acc  =   0.8488529014844804\n",
    "\n",
    "LOSS: train  =   [0.02348866]  ,   val  =   0.013325201607816868  ,   acc  =   0.8488529014844804\n",
    "\n",
    "CONSTANTLY STUCK\n",
    "\n",
    "Ok, trying with centroids as sepearte layer (and with parameter reset and bias reset to zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ExtranetModel_101_cent().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc_global.0.weight',\n",
       "              tensor([[[ 0.1012, -0.1619,  0.3686, -0.1567,  0.2587]],\n",
       "              \n",
       "                      [[ 0.3961, -0.1553, -0.0419,  0.4275,  0.3696]],\n",
       "              \n",
       "                      [[ 0.3448,  0.2764,  0.3840,  0.4334,  0.2068]],\n",
       "              \n",
       "                      [[-0.1962,  0.3689,  0.1665, -0.0816, -0.1924]],\n",
       "              \n",
       "                      [[-0.3459,  0.1251, -0.1040,  0.3395, -0.3818]],\n",
       "              \n",
       "                      [[-0.3002, -0.4339, -0.0003, -0.4121,  0.1499]],\n",
       "              \n",
       "                      [[ 0.0687,  0.3025,  0.3894, -0.2284, -0.2569]],\n",
       "              \n",
       "                      [[-0.0896,  0.0574, -0.3589, -0.3632, -0.2667]],\n",
       "              \n",
       "                      [[-0.1507, -0.0200,  0.1980, -0.2160,  0.2357]],\n",
       "              \n",
       "                      [[-0.3168,  0.1713, -0.0664,  0.1121,  0.2851]],\n",
       "              \n",
       "                      [[-0.3029,  0.0234,  0.1531, -0.2024, -0.0706]],\n",
       "              \n",
       "                      [[ 0.3694, -0.1187, -0.0481, -0.4044,  0.4077]],\n",
       "              \n",
       "                      [[-0.2110,  0.3111, -0.3653, -0.3038,  0.1640]],\n",
       "              \n",
       "                      [[ 0.1330, -0.1321, -0.3275,  0.0334,  0.4273]],\n",
       "              \n",
       "                      [[ 0.3932, -0.2013, -0.0571, -0.4394, -0.0546]],\n",
       "              \n",
       "                      [[ 0.2163,  0.0519, -0.3441,  0.3513,  0.3082]]], device='cuda:0')),\n",
       "             ('fc_global.0.bias',\n",
       "              tensor([-0.0699,  0.3622,  0.0534, -0.1368, -0.0109, -0.2105, -0.3959,  0.0466,\n",
       "                       0.0831,  0.0890,  0.0771, -0.4181,  0.2100, -0.0172,  0.3936, -0.3732],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_global.2.weight',\n",
       "              tensor([[[-0.0549,  0.0673,  0.0224, -0.1009, -0.0751],\n",
       "                       [ 0.0368, -0.0552, -0.0918,  0.0148, -0.0909],\n",
       "                       [ 0.0587,  0.1039, -0.0341, -0.0559,  0.0121],\n",
       "                       ...,\n",
       "                       [ 0.0071,  0.0792,  0.0113, -0.0147, -0.1050],\n",
       "                       [-0.0007, -0.0790, -0.0706, -0.0319,  0.1078],\n",
       "                       [ 0.0622, -0.0793, -0.0180, -0.0702,  0.1117]],\n",
       "              \n",
       "                      [[ 0.0954, -0.0427,  0.0867, -0.1090,  0.0431],\n",
       "                       [ 0.0175, -0.0219, -0.0411, -0.1015, -0.0660],\n",
       "                       [-0.0277,  0.0934, -0.0248, -0.0411, -0.0684],\n",
       "                       ...,\n",
       "                       [-0.0494,  0.0949,  0.0379, -0.0157, -0.0878],\n",
       "                       [ 0.0087, -0.1044,  0.0162, -0.1093,  0.0983],\n",
       "                       [ 0.0279,  0.0969,  0.0069, -0.0049,  0.0842]],\n",
       "              \n",
       "                      [[ 0.1053, -0.1116, -0.1096,  0.0060,  0.1061],\n",
       "                       [ 0.0669, -0.0821,  0.0102, -0.0079, -0.0257],\n",
       "                       [-0.0684,  0.0006, -0.0643, -0.0127,  0.0348],\n",
       "                       ...,\n",
       "                       [ 0.0412,  0.0278,  0.0332,  0.0132,  0.0927],\n",
       "                       [ 0.0316, -0.0239,  0.0278,  0.1031, -0.0810],\n",
       "                       [-0.0978, -0.0237,  0.0969,  0.0532,  0.0484]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0093,  0.0357, -0.0482,  0.0826,  0.0067],\n",
       "                       [-0.0358, -0.0152,  0.0008, -0.1070,  0.0690],\n",
       "                       [-0.0279, -0.0854, -0.0894, -0.1013,  0.0172],\n",
       "                       ...,\n",
       "                       [-0.0087, -0.0193,  0.0786,  0.0894, -0.0528],\n",
       "                       [ 0.0399,  0.0434, -0.0670, -0.0006, -0.0852],\n",
       "                       [ 0.0631,  0.0434, -0.0574,  0.1052, -0.0428]],\n",
       "              \n",
       "                      [[-0.0643, -0.0522,  0.0223, -0.0937, -0.0096],\n",
       "                       [ 0.0350, -0.0688, -0.0719,  0.0390, -0.0605],\n",
       "                       [-0.0960,  0.0502, -0.0185,  0.0105, -0.0231],\n",
       "                       ...,\n",
       "                       [ 0.0893, -0.0006,  0.0891, -0.1015, -0.0897],\n",
       "                       [-0.0262, -0.0430,  0.0879,  0.0617, -0.0130],\n",
       "                       [-0.0796,  0.0265, -0.0394, -0.0908,  0.0124]],\n",
       "              \n",
       "                      [[-0.0560,  0.0583,  0.0657,  0.0270,  0.0868],\n",
       "                       [-0.1100, -0.0917, -0.0329,  0.0154,  0.1057],\n",
       "                       [-0.0825, -0.0710,  0.1055,  0.0185,  0.0584],\n",
       "                       ...,\n",
       "                       [-0.0992, -0.1005,  0.1046, -0.0164, -0.0169],\n",
       "                       [-0.0845, -0.1086, -0.0626, -0.1041,  0.0758],\n",
       "                       [ 0.0042, -0.0141, -0.0100,  0.0767, -0.0442]]], device='cuda:0')),\n",
       "             ('fc_global.2.bias',\n",
       "              tensor([-0.0913,  0.0771,  0.0244, -0.0354, -0.0146, -0.0269,  0.0697,  0.0088,\n",
       "                       0.0473, -0.0431, -0.0056, -0.0504,  0.0532,  0.0913, -0.0528, -0.0720],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_global.5.weight',\n",
       "              tensor([[[-0.0960, -0.0566, -0.1017, -0.0148, -0.0487],\n",
       "                       [ 0.0694,  0.0139, -0.0189,  0.1089,  0.0808],\n",
       "                       [ 0.0080, -0.0660,  0.0396,  0.0450, -0.0796],\n",
       "                       ...,\n",
       "                       [-0.0919,  0.0910, -0.0611,  0.0389, -0.0838],\n",
       "                       [ 0.1115,  0.0155,  0.0469, -0.0101, -0.0601],\n",
       "                       [ 0.0068,  0.0966, -0.0438,  0.0701,  0.0102]],\n",
       "              \n",
       "                      [[ 0.0831, -0.0826, -0.0375, -0.0135, -0.0753],\n",
       "                       [ 0.0576, -0.0553, -0.0378,  0.0507,  0.1007],\n",
       "                       [-0.0733,  0.0843,  0.0352,  0.0534,  0.0867],\n",
       "                       ...,\n",
       "                       [ 0.0560, -0.0579, -0.1068, -0.1106, -0.1025],\n",
       "                       [ 0.0948,  0.0215, -0.0905,  0.0173, -0.0795],\n",
       "                       [ 0.0002, -0.0797, -0.0737, -0.0650, -0.1034]],\n",
       "              \n",
       "                      [[-0.0421,  0.0516, -0.0537, -0.0824, -0.0354],\n",
       "                       [-0.0886, -0.0201, -0.0388,  0.0623, -0.1015],\n",
       "                       [ 0.0378, -0.0925,  0.0426,  0.1038, -0.1114],\n",
       "                       ...,\n",
       "                       [-0.0064, -0.0282, -0.0099, -0.0763,  0.0235],\n",
       "                       [-0.0079, -0.0448, -0.0669,  0.0082, -0.0831],\n",
       "                       [-0.0051, -0.0678,  0.0273,  0.0975, -0.0332]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.1019, -0.0582,  0.0298,  0.0016,  0.0277],\n",
       "                       [-0.0739,  0.0210,  0.0897, -0.1061, -0.0875],\n",
       "                       [-0.0612, -0.0955,  0.0348, -0.0267, -0.0703],\n",
       "                       ...,\n",
       "                       [ 0.1012, -0.0401, -0.0834, -0.0760,  0.0175],\n",
       "                       [ 0.0108,  0.0607,  0.0581,  0.0586,  0.0947],\n",
       "                       [-0.0978, -0.0891, -0.0661, -0.0194, -0.0361]],\n",
       "              \n",
       "                      [[-0.0536,  0.0889,  0.0729, -0.0934, -0.0030],\n",
       "                       [ 0.0370,  0.0884, -0.0008,  0.0568,  0.0862],\n",
       "                       [ 0.0050, -0.0138, -0.1114, -0.0632, -0.0974],\n",
       "                       ...,\n",
       "                       [ 0.0837, -0.0449, -0.0784,  0.0733,  0.0275],\n",
       "                       [-0.0410,  0.0236, -0.0552,  0.0535, -0.1117],\n",
       "                       [ 0.1081, -0.0552, -0.0438, -0.0086, -0.0195]],\n",
       "              \n",
       "                      [[-0.0790,  0.0273,  0.1037,  0.0664, -0.0381],\n",
       "                       [-0.0786, -0.0174,  0.0023, -0.0165, -0.0433],\n",
       "                       [-0.0679,  0.0763, -0.0910, -0.0484,  0.1061],\n",
       "                       ...,\n",
       "                       [ 0.0052,  0.0720, -0.0426, -0.0502,  0.1005],\n",
       "                       [ 0.1111, -0.0857, -0.0528,  0.1076,  0.0911],\n",
       "                       [ 0.0325, -0.0065, -0.1051,  0.0388, -0.0601]]], device='cuda:0')),\n",
       "             ('fc_global.5.bias',\n",
       "              tensor([ 0.0661, -0.0618, -0.0829,  0.0677, -0.1032,  0.0669,  0.0557,  0.0662,\n",
       "                      -0.0851, -0.1064, -0.0521,  0.0668,  0.1115,  0.0602,  0.0501, -0.1064,\n",
       "                       0.0394,  0.0304, -0.0290, -0.0936, -0.0312, -0.0876, -0.0917, -0.0957,\n",
       "                      -0.0026, -0.0612, -0.0814, -0.1028, -0.0693, -0.1098, -0.1100, -0.0070],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_global.7.weight',\n",
       "              tensor([[[ 0.0683,  0.0216, -0.0598,  0.0420, -0.0090],\n",
       "                       [-0.0313, -0.0572, -0.0015,  0.0581,  0.0269],\n",
       "                       [ 0.0384,  0.0425, -0.0503,  0.0789, -0.0559],\n",
       "                       ...,\n",
       "                       [ 0.0092,  0.0368, -0.0449,  0.0504, -0.0277],\n",
       "                       [-0.0430,  0.0564, -0.0539,  0.0090,  0.0486],\n",
       "                       [ 0.0361, -0.0485,  0.0628, -0.0659, -0.0656]],\n",
       "              \n",
       "                      [[ 0.0375, -0.0546, -0.0768, -0.0487, -0.0757],\n",
       "                       [-0.0285,  0.0353, -0.0035,  0.0548, -0.0396],\n",
       "                       [ 0.0720,  0.0259,  0.0669, -0.0245,  0.0006],\n",
       "                       ...,\n",
       "                       [ 0.0376,  0.0602,  0.0452,  0.0767,  0.0673],\n",
       "                       [-0.0383,  0.0660,  0.0443,  0.0008, -0.0323],\n",
       "                       [ 0.0388, -0.0586, -0.0133,  0.0125, -0.0109]],\n",
       "              \n",
       "                      [[ 0.0199,  0.0634,  0.0506, -0.0323,  0.0649],\n",
       "                       [-0.0472, -0.0396,  0.0718,  0.0498, -0.0671],\n",
       "                       [ 0.0260,  0.0778, -0.0598,  0.0348,  0.0137],\n",
       "                       ...,\n",
       "                       [-0.0015, -0.0611,  0.0648,  0.0361, -0.0311],\n",
       "                       [ 0.0752, -0.0629,  0.0455,  0.0268, -0.0147],\n",
       "                       [ 0.0233, -0.0629, -0.0077,  0.0387,  0.0068]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0622,  0.0752, -0.0616,  0.0055, -0.0526],\n",
       "                       [ 0.0514,  0.0444, -0.0392, -0.0085, -0.0596],\n",
       "                       [-0.0484, -0.0054,  0.0001,  0.0612,  0.0560],\n",
       "                       ...,\n",
       "                       [-0.0495,  0.0376,  0.0632, -0.0473,  0.0610],\n",
       "                       [-0.0081,  0.0169,  0.0217,  0.0650,  0.0280],\n",
       "                       [-0.0471,  0.0546,  0.0361,  0.0169, -0.0590]],\n",
       "              \n",
       "                      [[ 0.0573, -0.0093,  0.0013, -0.0011,  0.0504],\n",
       "                       [-0.0027,  0.0652,  0.0659, -0.0096, -0.0694],\n",
       "                       [ 0.0279, -0.0183,  0.0027,  0.0689, -0.0569],\n",
       "                       ...,\n",
       "                       [ 0.0736,  0.0213, -0.0764,  0.0711, -0.0623],\n",
       "                       [-0.0486,  0.0675,  0.0180,  0.0592,  0.0144],\n",
       "                       [-0.0521, -0.0032,  0.0563, -0.0652,  0.0189]],\n",
       "              \n",
       "                      [[-0.0270, -0.0565, -0.0265, -0.0352, -0.0787],\n",
       "                       [-0.0154,  0.0576, -0.0363,  0.0601,  0.0203],\n",
       "                       [-0.0572, -0.0726, -0.0627,  0.0058, -0.0065],\n",
       "                       ...,\n",
       "                       [-0.0709,  0.0312, -0.0371,  0.0417,  0.0458],\n",
       "                       [ 0.0112, -0.0265,  0.0302, -0.0417, -0.0131],\n",
       "                       [-0.0609, -0.0354, -0.0178,  0.0209,  0.0474]]], device='cuda:0')),\n",
       "             ('fc_global.7.bias',\n",
       "              tensor([ 0.0363, -0.0579,  0.0697, -0.0725, -0.0512,  0.0360, -0.0298,  0.0622,\n",
       "                      -0.0251,  0.0376,  0.0026, -0.0041,  0.0438,  0.0139,  0.0606,  0.0762,\n",
       "                       0.0193, -0.0053,  0.0184,  0.0167, -0.0588, -0.0015,  0.0379,  0.0623,\n",
       "                       0.0781, -0.0781,  0.0661,  0.0423,  0.0143,  0.0435, -0.0219, -0.0104],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_global.10.weight',\n",
       "              tensor([[[ 0.0695, -0.0606, -0.0653, -0.0622,  0.0401],\n",
       "                       [ 0.0442, -0.0361,  0.0149, -0.0654,  0.0125],\n",
       "                       [ 0.0401, -0.0694, -0.0536,  0.0299,  0.0037],\n",
       "                       ...,\n",
       "                       [-0.0349, -0.0358, -0.0754, -0.0225,  0.0334],\n",
       "                       [ 0.0231, -0.0683,  0.0685, -0.0512, -0.0710],\n",
       "                       [-0.0052,  0.0324, -0.0637,  0.0534, -0.0146]],\n",
       "              \n",
       "                      [[-0.0190, -0.0075,  0.0721, -0.0684,  0.0176],\n",
       "                       [-0.0226, -0.0598, -0.0557, -0.0319,  0.0330],\n",
       "                       [-0.0610, -0.0584,  0.0024, -0.0446, -0.0162],\n",
       "                       ...,\n",
       "                       [ 0.0660, -0.0121,  0.0415, -0.0089, -0.0379],\n",
       "                       [-0.0224, -0.0382,  0.0446, -0.0067,  0.0144],\n",
       "                       [-0.0709, -0.0106, -0.0252,  0.0267, -0.0315]],\n",
       "              \n",
       "                      [[-0.0236, -0.0458, -0.0717, -0.0001,  0.0272],\n",
       "                       [ 0.0233,  0.0620, -0.0149,  0.0735,  0.0106],\n",
       "                       [ 0.0271, -0.0334, -0.0628,  0.0663,  0.0092],\n",
       "                       ...,\n",
       "                       [-0.0010,  0.0002,  0.0093,  0.0063, -0.0732],\n",
       "                       [-0.0371,  0.0028,  0.0532, -0.0644,  0.0524],\n",
       "                       [-0.0646,  0.0186, -0.0641,  0.0080, -0.0025]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0404,  0.0705,  0.0655,  0.0582,  0.0092],\n",
       "                       [-0.0140,  0.0554,  0.0130, -0.0770, -0.0363],\n",
       "                       [-0.0787, -0.0219, -0.0655,  0.0740,  0.0546],\n",
       "                       ...,\n",
       "                       [-0.0111, -0.0508,  0.0660, -0.0361,  0.0514],\n",
       "                       [ 0.0084,  0.0140,  0.0616,  0.0469, -0.0577],\n",
       "                       [-0.0045, -0.0165, -0.0333, -0.0651,  0.0602]],\n",
       "              \n",
       "                      [[-0.0708, -0.0239,  0.0412, -0.0368, -0.0223],\n",
       "                       [-0.0000, -0.0561,  0.0051,  0.0596, -0.0426],\n",
       "                       [ 0.0156,  0.0526,  0.0790, -0.0590,  0.0203],\n",
       "                       ...,\n",
       "                       [-0.0554, -0.0668, -0.0728,  0.0591, -0.0294],\n",
       "                       [ 0.0137, -0.0709, -0.0327, -0.0156, -0.0231],\n",
       "                       [-0.0561, -0.0481,  0.0289,  0.0315, -0.0656]],\n",
       "              \n",
       "                      [[ 0.0769,  0.0376, -0.0731, -0.0771, -0.0264],\n",
       "                       [-0.0008,  0.0130, -0.0482,  0.0544,  0.0260],\n",
       "                       [-0.0617, -0.0542,  0.0274, -0.0132,  0.0542],\n",
       "                       ...,\n",
       "                       [-0.0790, -0.0644, -0.0192,  0.0753, -0.0130],\n",
       "                       [-0.0621,  0.0214,  0.0364, -0.0120,  0.0730],\n",
       "                       [ 0.0457, -0.0494, -0.0229,  0.0558,  0.0632]]], device='cuda:0')),\n",
       "             ('fc_global.10.bias',\n",
       "              tensor([-0.0377,  0.0074, -0.0237, -0.0422,  0.0342,  0.0117, -0.0364,  0.0404,\n",
       "                       0.0638,  0.0628, -0.0588, -0.0079, -0.0017, -0.0120,  0.0496, -0.0387,\n",
       "                      -0.0719, -0.0080, -0.0018, -0.0082, -0.0763,  0.0439,  0.0252,  0.0362,\n",
       "                      -0.0471,  0.0641, -0.0495, -0.0464,  0.0018,  0.0506,  0.0540, -0.0434,\n",
       "                      -0.0453,  0.0647, -0.0046,  0.0156,  0.0333, -0.0384, -0.0581,  0.0330,\n",
       "                       0.0578, -0.0457, -0.0711, -0.0543, -0.0294,  0.0481,  0.0108,  0.0747,\n",
       "                       0.0287,  0.0150,  0.0780,  0.0147, -0.0503, -0.0072, -0.0356, -0.0086,\n",
       "                      -0.0331,  0.0514,  0.0351, -0.0398,  0.0664, -0.0767, -0.0483,  0.0118],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_global.12.weight',\n",
       "              tensor([[[ 0.0072, -0.0463,  0.0287,  0.0538,  0.0258],\n",
       "                       [ 0.0096, -0.0283, -0.0277,  0.0415,  0.0377],\n",
       "                       [-0.0239, -0.0133,  0.0329,  0.0308, -0.0389],\n",
       "                       ...,\n",
       "                       [-0.0362,  0.0444,  0.0454, -0.0500,  0.0248],\n",
       "                       [-0.0168,  0.0395, -0.0442,  0.0549,  0.0415],\n",
       "                       [-0.0341,  0.0136,  0.0520, -0.0049, -0.0379]],\n",
       "              \n",
       "                      [[ 0.0409, -0.0287,  0.0146,  0.0318,  0.0215],\n",
       "                       [ 0.0002, -0.0339,  0.0246,  0.0511, -0.0148],\n",
       "                       [ 0.0083, -0.0510, -0.0422, -0.0511,  0.0511],\n",
       "                       ...,\n",
       "                       [ 0.0268, -0.0546,  0.0428,  0.0029, -0.0028],\n",
       "                       [-0.0258,  0.0499, -0.0340,  0.0385,  0.0028],\n",
       "                       [-0.0150,  0.0556,  0.0039, -0.0513,  0.0243]],\n",
       "              \n",
       "                      [[-0.0157,  0.0119,  0.0323,  0.0350, -0.0239],\n",
       "                       [ 0.0183, -0.0455,  0.0506,  0.0492,  0.0514],\n",
       "                       [-0.0219,  0.0253,  0.0177,  0.0246, -0.0378],\n",
       "                       ...,\n",
       "                       [-0.0461, -0.0079, -0.0383,  0.0281, -0.0331],\n",
       "                       [ 0.0290, -0.0378, -0.0474,  0.0422, -0.0194],\n",
       "                       [-0.0481,  0.0300,  0.0265,  0.0071, -0.0536]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0370,  0.0265,  0.0188, -0.0079,  0.0065],\n",
       "                       [ 0.0540,  0.0259,  0.0320,  0.0211, -0.0005],\n",
       "                       [ 0.0163,  0.0051, -0.0289,  0.0152, -0.0246],\n",
       "                       ...,\n",
       "                       [-0.0155,  0.0129, -0.0488, -0.0003, -0.0126],\n",
       "                       [ 0.0338, -0.0024,  0.0133,  0.0320,  0.0253],\n",
       "                       [-0.0138,  0.0523, -0.0276,  0.0000,  0.0551]],\n",
       "              \n",
       "                      [[-0.0440, -0.0442,  0.0259,  0.0132,  0.0456],\n",
       "                       [ 0.0269,  0.0211, -0.0084, -0.0017,  0.0344],\n",
       "                       [ 0.0339, -0.0017,  0.0057,  0.0018,  0.0019],\n",
       "                       ...,\n",
       "                       [ 0.0100, -0.0383,  0.0440, -0.0088,  0.0073],\n",
       "                       [-0.0466,  0.0128, -0.0038, -0.0509,  0.0152],\n",
       "                       [-0.0238, -0.0070,  0.0072, -0.0142,  0.0325]],\n",
       "              \n",
       "                      [[-0.0162,  0.0345, -0.0398,  0.0268,  0.0489],\n",
       "                       [ 0.0148, -0.0474,  0.0317,  0.0442,  0.0321],\n",
       "                       [ 0.0283, -0.0519,  0.0400,  0.0357, -0.0005],\n",
       "                       ...,\n",
       "                       [ 0.0556, -0.0126, -0.0556,  0.0524,  0.0277],\n",
       "                       [-0.0293, -0.0029, -0.0031,  0.0072, -0.0042],\n",
       "                       [ 0.0408,  0.0518,  0.0000,  0.0112,  0.0409]]], device='cuda:0')),\n",
       "             ('fc_global.12.bias',\n",
       "              tensor([ 0.0323, -0.0272, -0.0415,  0.0508,  0.0424, -0.0309, -0.0130, -0.0108,\n",
       "                       0.0174, -0.0019, -0.0462,  0.0372, -0.0539,  0.0267,  0.0274,  0.0200,\n",
       "                       0.0368,  0.0197, -0.0346,  0.0359,  0.0016, -0.0235,  0.0296,  0.0089,\n",
       "                       0.0255, -0.0172,  0.0470, -0.0065,  0.0190,  0.0414, -0.0191,  0.0248,\n",
       "                      -0.0470, -0.0026,  0.0536,  0.0272, -0.0059, -0.0414,  0.0319, -0.0211,\n",
       "                       0.0044, -0.0156, -0.0525, -0.0543, -0.0395, -0.0372,  0.0527,  0.0534,\n",
       "                       0.0237, -0.0255, -0.0513,  0.0491,  0.0436, -0.0161,  0.0092, -0.0255,\n",
       "                       0.0439, -0.0377, -0.0471,  0.0049, -0.0473, -0.0214, -0.0164, -0.0543],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_global.15.weight',\n",
       "              tensor([[[ 0.0188, -0.0040,  0.0393, -0.0324, -0.0556],\n",
       "                       [-0.0397, -0.0250, -0.0016, -0.0234,  0.0132],\n",
       "                       [ 0.0160,  0.0149,  0.0033, -0.0495, -0.0021],\n",
       "                       ...,\n",
       "                       [-0.0463,  0.0115, -0.0253, -0.0004,  0.0495],\n",
       "                       [ 0.0072,  0.0046, -0.0442,  0.0485, -0.0513],\n",
       "                       [-0.0013,  0.0090, -0.0519,  0.0123,  0.0414]],\n",
       "              \n",
       "                      [[-0.0447, -0.0321,  0.0382,  0.0070, -0.0063],\n",
       "                       [ 0.0501, -0.0050, -0.0350, -0.0414, -0.0330],\n",
       "                       [-0.0101,  0.0395, -0.0474,  0.0270, -0.0164],\n",
       "                       ...,\n",
       "                       [ 0.0463, -0.0425, -0.0153,  0.0390, -0.0529],\n",
       "                       [ 0.0125, -0.0233, -0.0020,  0.0408,  0.0384],\n",
       "                       [-0.0254,  0.0371,  0.0123,  0.0540,  0.0479]],\n",
       "              \n",
       "                      [[ 0.0512,  0.0097,  0.0380,  0.0113, -0.0441],\n",
       "                       [ 0.0437, -0.0489,  0.0479, -0.0148, -0.0307],\n",
       "                       [ 0.0228,  0.0516, -0.0116, -0.0285,  0.0512],\n",
       "                       ...,\n",
       "                       [ 0.0199, -0.0228, -0.0466,  0.0522, -0.0197],\n",
       "                       [ 0.0098,  0.0191,  0.0555, -0.0285,  0.0162],\n",
       "                       [ 0.0193, -0.0429, -0.0547,  0.0418, -0.0021]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0079, -0.0329,  0.0204, -0.0467,  0.0430],\n",
       "                       [-0.0028,  0.0106, -0.0503,  0.0235,  0.0432],\n",
       "                       [ 0.0254,  0.0068,  0.0158, -0.0312, -0.0228],\n",
       "                       ...,\n",
       "                       [ 0.0467, -0.0111,  0.0524, -0.0156, -0.0554],\n",
       "                       [ 0.0119,  0.0307,  0.0179,  0.0087, -0.0421],\n",
       "                       [-0.0006,  0.0038, -0.0119, -0.0041, -0.0167]],\n",
       "              \n",
       "                      [[ 0.0231, -0.0474,  0.0163,  0.0024, -0.0226],\n",
       "                       [-0.0348,  0.0532,  0.0058,  0.0194,  0.0094],\n",
       "                       [-0.0489, -0.0250, -0.0259,  0.0083, -0.0421],\n",
       "                       ...,\n",
       "                       [-0.0245, -0.0398, -0.0353, -0.0263,  0.0229],\n",
       "                       [-0.0072,  0.0083, -0.0123, -0.0482, -0.0313],\n",
       "                       [ 0.0406, -0.0034, -0.0092, -0.0009, -0.0495]],\n",
       "              \n",
       "                      [[ 0.0341,  0.0218, -0.0492,  0.0010,  0.0370],\n",
       "                       [ 0.0288,  0.0492,  0.0512,  0.0022, -0.0173],\n",
       "                       [ 0.0537, -0.0449,  0.0412,  0.0472,  0.0254],\n",
       "                       ...,\n",
       "                       [ 0.0269, -0.0529,  0.0284, -0.0348, -0.0266],\n",
       "                       [ 0.0496, -0.0330, -0.0375, -0.0259,  0.0182],\n",
       "                       [ 0.0176,  0.0445,  0.0315, -0.0282,  0.0178]]], device='cuda:0')),\n",
       "             ('fc_global.15.bias',\n",
       "              tensor([-0.0507, -0.0210,  0.0524,  0.0269, -0.0005, -0.0501,  0.0290,  0.0304,\n",
       "                       0.0459, -0.0063,  0.0254,  0.0179,  0.0531,  0.0454,  0.0001,  0.0146,\n",
       "                      -0.0179,  0.0060,  0.0327, -0.0521, -0.0017,  0.0314,  0.0322,  0.0141,\n",
       "                      -0.0319,  0.0538,  0.0441, -0.0088, -0.0176,  0.0373, -0.0059,  0.0350,\n",
       "                       0.0153,  0.0184, -0.0036, -0.0528, -0.0336,  0.0363, -0.0355, -0.0291,\n",
       "                       0.0313,  0.0379,  0.0333,  0.0166,  0.0397, -0.0232,  0.0457,  0.0464,\n",
       "                       0.0319, -0.0060, -0.0335, -0.0363,  0.0549,  0.0390, -0.0265, -0.0366,\n",
       "                       0.0280,  0.0326, -0.0443,  0.0292,  0.0247, -0.0089,  0.0254,  0.0536,\n",
       "                      -0.0426, -0.0430, -0.0427,  0.0352,  0.0530, -0.0358,  0.0374, -0.0204,\n",
       "                      -0.0467, -0.0260, -0.0476, -0.0252, -0.0523,  0.0437,  0.0521, -0.0069,\n",
       "                      -0.0176, -0.0046,  0.0132,  0.0045,  0.0065,  0.0167, -0.0146, -0.0047,\n",
       "                       0.0154,  0.0355,  0.0473,  0.0394, -0.0442, -0.0037,  0.0224,  0.0383,\n",
       "                      -0.0554,  0.0051,  0.0159, -0.0245, -0.0446,  0.0354,  0.0230, -0.0100,\n",
       "                      -0.0309, -0.0497,  0.0219,  0.0518,  0.0145, -0.0446,  0.0498, -0.0431,\n",
       "                       0.0513, -0.0328, -0.0335,  0.0436, -0.0452, -0.0233, -0.0154, -0.0182,\n",
       "                      -0.0489, -0.0349, -0.0398, -0.0065,  0.0152,  0.0315, -0.0392, -0.0180],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_global.17.weight',\n",
       "              tensor([[[-0.0159,  0.0286,  0.0049,  0.0167, -0.0326],\n",
       "                       [-0.0140,  0.0370,  0.0043,  0.0220,  0.0298],\n",
       "                       [-0.0132,  0.0161, -0.0250, -0.0163,  0.0023],\n",
       "                       ...,\n",
       "                       [ 0.0303,  0.0325, -0.0321, -0.0112, -0.0127],\n",
       "                       [ 0.0185,  0.0043,  0.0128, -0.0147, -0.0353],\n",
       "                       [-0.0136,  0.0272,  0.0151, -0.0163, -0.0206]],\n",
       "              \n",
       "                      [[-0.0025,  0.0232,  0.0319,  0.0359,  0.0274],\n",
       "                       [ 0.0125,  0.0261,  0.0186,  0.0374, -0.0131],\n",
       "                       [-0.0369,  0.0299, -0.0254,  0.0185,  0.0329],\n",
       "                       ...,\n",
       "                       [ 0.0217,  0.0058, -0.0315, -0.0326,  0.0009],\n",
       "                       [-0.0375,  0.0096,  0.0250,  0.0173,  0.0168],\n",
       "                       [ 0.0173, -0.0076,  0.0071, -0.0192, -0.0156]],\n",
       "              \n",
       "                      [[-0.0106, -0.0227,  0.0375,  0.0390,  0.0112],\n",
       "                       [ 0.0244,  0.0123, -0.0086, -0.0220,  0.0089],\n",
       "                       [ 0.0121,  0.0252,  0.0391,  0.0353,  0.0073],\n",
       "                       ...,\n",
       "                       [-0.0316, -0.0236,  0.0094, -0.0189, -0.0371],\n",
       "                       [ 0.0289, -0.0143, -0.0165, -0.0102,  0.0007],\n",
       "                       [-0.0262,  0.0217, -0.0153, -0.0159, -0.0342]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0308,  0.0170, -0.0195,  0.0317, -0.0258],\n",
       "                       [-0.0285, -0.0000,  0.0206, -0.0347,  0.0241],\n",
       "                       [-0.0131, -0.0096,  0.0266, -0.0357,  0.0155],\n",
       "                       ...,\n",
       "                       [-0.0295, -0.0051,  0.0003, -0.0234, -0.0221],\n",
       "                       [ 0.0290,  0.0034,  0.0353,  0.0045,  0.0013],\n",
       "                       [-0.0165, -0.0332,  0.0040,  0.0045,  0.0252]],\n",
       "              \n",
       "                      [[ 0.0021,  0.0168,  0.0385,  0.0002, -0.0025],\n",
       "                       [-0.0324,  0.0038, -0.0333,  0.0346,  0.0040],\n",
       "                       [ 0.0205,  0.0088,  0.0311,  0.0392,  0.0199],\n",
       "                       ...,\n",
       "                       [-0.0067,  0.0295,  0.0355, -0.0352,  0.0035],\n",
       "                       [-0.0279,  0.0040,  0.0106, -0.0098, -0.0162],\n",
       "                       [ 0.0194, -0.0303, -0.0379,  0.0389,  0.0091]],\n",
       "              \n",
       "                      [[-0.0166, -0.0010, -0.0307,  0.0194,  0.0265],\n",
       "                       [ 0.0128, -0.0155,  0.0058, -0.0232, -0.0065],\n",
       "                       [ 0.0224,  0.0018, -0.0142,  0.0037,  0.0194],\n",
       "                       ...,\n",
       "                       [ 0.0017, -0.0074,  0.0082,  0.0009, -0.0091],\n",
       "                       [ 0.0108, -0.0352, -0.0252,  0.0285, -0.0017],\n",
       "                       [ 0.0373,  0.0107,  0.0265, -0.0364,  0.0372]]], device='cuda:0')),\n",
       "             ('fc_global.17.bias',\n",
       "              tensor([ 0.0073,  0.0143,  0.0067,  0.0348,  0.0306, -0.0303, -0.0371,  0.0309,\n",
       "                       0.0171,  0.0101,  0.0318,  0.0273, -0.0264,  0.0027, -0.0160,  0.0283,\n",
       "                       0.0259,  0.0248,  0.0046,  0.0340,  0.0306,  0.0326, -0.0166,  0.0049,\n",
       "                       0.0358, -0.0056,  0.0374,  0.0215, -0.0004,  0.0037, -0.0351, -0.0280,\n",
       "                       0.0162,  0.0342,  0.0314,  0.0339, -0.0104, -0.0130,  0.0195, -0.0082,\n",
       "                      -0.0162,  0.0211, -0.0022, -0.0159, -0.0079,  0.0019, -0.0255,  0.0331,\n",
       "                      -0.0288,  0.0291,  0.0362, -0.0285,  0.0354, -0.0192, -0.0174,  0.0189,\n",
       "                       0.0146, -0.0338, -0.0389,  0.0368, -0.0380, -0.0378,  0.0342, -0.0008,\n",
       "                       0.0157,  0.0322, -0.0323,  0.0078,  0.0292, -0.0334,  0.0118,  0.0144,\n",
       "                      -0.0275,  0.0175, -0.0282, -0.0000, -0.0072,  0.0137, -0.0259, -0.0021,\n",
       "                      -0.0011, -0.0126,  0.0076,  0.0367, -0.0022,  0.0368, -0.0209,  0.0280,\n",
       "                      -0.0318,  0.0135,  0.0330, -0.0284, -0.0187, -0.0269, -0.0120,  0.0012,\n",
       "                       0.0352,  0.0143,  0.0385, -0.0264,  0.0233, -0.0069, -0.0058, -0.0071,\n",
       "                       0.0091, -0.0268,  0.0090, -0.0218, -0.0146,  0.0065,  0.0304,  0.0070,\n",
       "                      -0.0356, -0.0293, -0.0029,  0.0352, -0.0308,  0.0331,  0.0233,  0.0174,\n",
       "                      -0.0128,  0.0198,  0.0393,  0.0068, -0.0343, -0.0083,  0.0153,  0.0287],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_global.20.weight',\n",
       "              tensor([[[-0.0334, -0.0238,  0.0115, -0.0140, -0.0005],\n",
       "                       [ 0.0076,  0.0082,  0.0075,  0.0317, -0.0066],\n",
       "                       [ 0.0285, -0.0228,  0.0388, -0.0171, -0.0201],\n",
       "                       ...,\n",
       "                       [ 0.0242,  0.0090,  0.0077, -0.0014, -0.0097],\n",
       "                       [-0.0139, -0.0070,  0.0395,  0.0054, -0.0170],\n",
       "                       [-0.0222,  0.0250, -0.0280,  0.0147,  0.0381]],\n",
       "              \n",
       "                      [[-0.0037,  0.0189, -0.0146, -0.0054, -0.0222],\n",
       "                       [ 0.0222,  0.0180,  0.0179,  0.0275,  0.0118],\n",
       "                       [-0.0226, -0.0318, -0.0029, -0.0095,  0.0127],\n",
       "                       ...,\n",
       "                       [ 0.0274,  0.0384, -0.0270,  0.0395,  0.0336],\n",
       "                       [-0.0309,  0.0180, -0.0162,  0.0215, -0.0034],\n",
       "                       [ 0.0308, -0.0348,  0.0214,  0.0086, -0.0273]],\n",
       "              \n",
       "                      [[ 0.0033,  0.0336, -0.0048,  0.0351,  0.0183],\n",
       "                       [-0.0092, -0.0023,  0.0389, -0.0223,  0.0195],\n",
       "                       [-0.0241,  0.0121, -0.0202, -0.0363,  0.0282],\n",
       "                       ...,\n",
       "                       [ 0.0258,  0.0028,  0.0059,  0.0320,  0.0203],\n",
       "                       [ 0.0237,  0.0315, -0.0302,  0.0320,  0.0292],\n",
       "                       [ 0.0220, -0.0012,  0.0050,  0.0164,  0.0281]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0016,  0.0060,  0.0018,  0.0146,  0.0328],\n",
       "                       [-0.0031,  0.0219, -0.0366, -0.0253, -0.0331],\n",
       "                       [ 0.0096,  0.0013,  0.0321,  0.0209,  0.0066],\n",
       "                       ...,\n",
       "                       [-0.0395,  0.0117, -0.0012, -0.0301, -0.0059],\n",
       "                       [ 0.0105, -0.0174, -0.0088, -0.0184,  0.0271],\n",
       "                       [ 0.0247,  0.0217,  0.0175, -0.0184, -0.0085]],\n",
       "              \n",
       "                      [[ 0.0237, -0.0142,  0.0228,  0.0012,  0.0333],\n",
       "                       [ 0.0168,  0.0328, -0.0256,  0.0161, -0.0137],\n",
       "                       [-0.0123, -0.0065,  0.0046, -0.0310,  0.0308],\n",
       "                       ...,\n",
       "                       [ 0.0375,  0.0220, -0.0082,  0.0083, -0.0271],\n",
       "                       [-0.0098,  0.0242,  0.0125,  0.0040,  0.0181],\n",
       "                       [ 0.0318, -0.0367,  0.0062, -0.0347, -0.0207]],\n",
       "              \n",
       "                      [[ 0.0337,  0.0298,  0.0088,  0.0146,  0.0315],\n",
       "                       [-0.0137, -0.0175,  0.0272,  0.0297, -0.0345],\n",
       "                       [-0.0195, -0.0226,  0.0235,  0.0215, -0.0238],\n",
       "                       ...,\n",
       "                       [-0.0243, -0.0368, -0.0177, -0.0208,  0.0265],\n",
       "                       [-0.0019,  0.0016, -0.0337, -0.0127, -0.0043],\n",
       "                       [-0.0048,  0.0368,  0.0378,  0.0124, -0.0190]]], device='cuda:0')),\n",
       "             ('fc_global.20.bias',\n",
       "              tensor([-0.0173,  0.0333,  0.0243, -0.0235,  0.0132, -0.0093, -0.0062, -0.0191,\n",
       "                      -0.0168, -0.0136,  0.0239, -0.0111, -0.0301,  0.0133,  0.0302,  0.0020,\n",
       "                       0.0171, -0.0141, -0.0215,  0.0215,  0.0085, -0.0174,  0.0228,  0.0097,\n",
       "                       0.0334,  0.0254, -0.0252,  0.0180, -0.0359, -0.0106, -0.0302,  0.0146,\n",
       "                      -0.0310, -0.0251, -0.0132, -0.0021, -0.0353, -0.0338, -0.0188, -0.0315,\n",
       "                      -0.0055, -0.0095, -0.0195, -0.0381,  0.0070, -0.0200,  0.0295, -0.0322,\n",
       "                       0.0124, -0.0264,  0.0224,  0.0220, -0.0136,  0.0227,  0.0283, -0.0388,\n",
       "                       0.0286, -0.0317, -0.0253,  0.0075,  0.0123, -0.0385,  0.0215,  0.0324,\n",
       "                      -0.0062,  0.0251, -0.0033, -0.0183, -0.0222,  0.0142, -0.0389,  0.0038,\n",
       "                      -0.0141,  0.0242, -0.0139, -0.0063, -0.0339,  0.0096, -0.0299,  0.0145,\n",
       "                      -0.0145, -0.0317, -0.0191,  0.0319,  0.0246, -0.0329,  0.0202, -0.0393,\n",
       "                       0.0349,  0.0083, -0.0063, -0.0361,  0.0134, -0.0119, -0.0092, -0.0032,\n",
       "                       0.0332, -0.0271, -0.0138, -0.0346,  0.0328,  0.0139,  0.0337, -0.0319,\n",
       "                       0.0268,  0.0102,  0.0266, -0.0144, -0.0362, -0.0190, -0.0165,  0.0248,\n",
       "                       0.0334,  0.0185, -0.0164, -0.0287, -0.0205,  0.0052,  0.0161,  0.0045,\n",
       "                      -0.0300, -0.0196, -0.0071,  0.0334,  0.0279, -0.0375, -0.0068,  0.0051,\n",
       "                       0.0384, -0.0136, -0.0172, -0.0301, -0.0190, -0.0015,  0.0194,  0.0267,\n",
       "                       0.0028,  0.0097, -0.0102, -0.0013, -0.0001, -0.0205, -0.0373,  0.0264,\n",
       "                      -0.0234, -0.0293,  0.0339,  0.0056, -0.0038,  0.0364, -0.0231,  0.0191,\n",
       "                      -0.0199, -0.0183,  0.0180, -0.0211, -0.0372,  0.0092, -0.0097, -0.0016,\n",
       "                       0.0091, -0.0261,  0.0296,  0.0366,  0.0388, -0.0103, -0.0146,  0.0133,\n",
       "                       0.0134, -0.0257,  0.0217,  0.0377, -0.0040, -0.0262, -0.0286,  0.0046,\n",
       "                       0.0338, -0.0087, -0.0297,  0.0180, -0.0299,  0.0299, -0.0385, -0.0385,\n",
       "                       0.0349,  0.0393,  0.0068, -0.0150, -0.0138,  0.0363, -0.0091,  0.0297,\n",
       "                       0.0031, -0.0344,  0.0034, -0.0220, -0.0188,  0.0071, -0.0367,  0.0004,\n",
       "                      -0.0132,  0.0318, -0.0046,  0.0224, -0.0020, -0.0234, -0.0144,  0.0010,\n",
       "                      -0.0110, -0.0198,  0.0252,  0.0150, -0.0059,  0.0363, -0.0390, -0.0179,\n",
       "                      -0.0194, -0.0039, -0.0384,  0.0385,  0.0277, -0.0272, -0.0173,  0.0008,\n",
       "                       0.0265,  0.0309, -0.0367,  0.0374, -0.0142, -0.0327,  0.0162, -0.0114,\n",
       "                      -0.0045, -0.0311,  0.0385,  0.0078,  0.0034, -0.0163,  0.0071, -0.0253,\n",
       "                      -0.0390,  0.0116, -0.0099, -0.0272,  0.0070, -0.0347, -0.0231,  0.0001,\n",
       "                       0.0261,  0.0020, -0.0378,  0.0001, -0.0011, -0.0056,  0.0116,  0.0198],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_global.22.weight',\n",
       "              tensor([[[ 0.0181,  0.0205,  0.0075,  0.0177, -0.0153],\n",
       "                       [ 0.0126,  0.0004, -0.0248, -0.0205, -0.0037],\n",
       "                       [ 0.0085,  0.0056, -0.0014, -0.0001,  0.0178],\n",
       "                       ...,\n",
       "                       [-0.0172,  0.0047, -0.0183, -0.0146, -0.0216],\n",
       "                       [ 0.0198, -0.0021,  0.0118, -0.0059, -0.0167],\n",
       "                       [-0.0029, -0.0063, -0.0080,  0.0236, -0.0040]],\n",
       "              \n",
       "                      [[ 0.0120,  0.0133, -0.0186, -0.0149, -0.0181],\n",
       "                       [ 0.0072, -0.0118,  0.0113,  0.0277,  0.0120],\n",
       "                       [-0.0054, -0.0197, -0.0247,  0.0103, -0.0113],\n",
       "                       ...,\n",
       "                       [-0.0148,  0.0135,  0.0278, -0.0179,  0.0175],\n",
       "                       [ 0.0020,  0.0226, -0.0015, -0.0072,  0.0150],\n",
       "                       [-0.0233, -0.0239, -0.0190, -0.0194, -0.0005]],\n",
       "              \n",
       "                      [[-0.0095,  0.0097,  0.0162,  0.0257, -0.0261],\n",
       "                       [-0.0127, -0.0175,  0.0148, -0.0065, -0.0266],\n",
       "                       [ 0.0103, -0.0068, -0.0248, -0.0192, -0.0134],\n",
       "                       ...,\n",
       "                       [ 0.0229,  0.0185,  0.0088, -0.0022, -0.0150],\n",
       "                       [ 0.0178,  0.0127,  0.0160, -0.0143, -0.0167],\n",
       "                       [ 0.0137,  0.0081,  0.0126, -0.0155, -0.0124]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0036,  0.0059, -0.0014, -0.0059, -0.0144],\n",
       "                       [-0.0275, -0.0087,  0.0233, -0.0272,  0.0127],\n",
       "                       [-0.0073, -0.0084, -0.0015,  0.0016, -0.0252],\n",
       "                       ...,\n",
       "                       [ 0.0089,  0.0179,  0.0087, -0.0039,  0.0208],\n",
       "                       [-0.0145,  0.0049,  0.0234, -0.0046, -0.0053],\n",
       "                       [-0.0102,  0.0198, -0.0131,  0.0071, -0.0023]],\n",
       "              \n",
       "                      [[-0.0211, -0.0240,  0.0120, -0.0030,  0.0236],\n",
       "                       [-0.0264,  0.0181, -0.0243, -0.0157, -0.0053],\n",
       "                       [ 0.0271, -0.0015,  0.0171, -0.0050,  0.0254],\n",
       "                       ...,\n",
       "                       [ 0.0035,  0.0063,  0.0022,  0.0149, -0.0249],\n",
       "                       [-0.0021, -0.0010,  0.0219, -0.0018,  0.0026],\n",
       "                       [ 0.0153,  0.0104,  0.0100,  0.0027,  0.0053]],\n",
       "              \n",
       "                      [[ 0.0035, -0.0109, -0.0199,  0.0008, -0.0068],\n",
       "                       [-0.0142,  0.0265,  0.0161, -0.0125,  0.0222],\n",
       "                       [ 0.0216,  0.0090,  0.0259, -0.0043,  0.0015],\n",
       "                       ...,\n",
       "                       [-0.0080, -0.0148,  0.0213,  0.0200, -0.0257],\n",
       "                       [ 0.0066, -0.0269,  0.0067, -0.0194, -0.0133],\n",
       "                       [ 0.0014,  0.0213,  0.0125, -0.0038,  0.0042]]], device='cuda:0')),\n",
       "             ('fc_global.22.bias',\n",
       "              tensor([-0.0079, -0.0135, -0.0042, -0.0000,  0.0203,  0.0155, -0.0079,  0.0222,\n",
       "                      -0.0127,  0.0210,  0.0248,  0.0075,  0.0271,  0.0137,  0.0047, -0.0173,\n",
       "                       0.0232,  0.0085, -0.0213, -0.0062, -0.0094, -0.0154,  0.0055,  0.0213,\n",
       "                       0.0186,  0.0273,  0.0212, -0.0084, -0.0133,  0.0033,  0.0188, -0.0094,\n",
       "                      -0.0245, -0.0039, -0.0273,  0.0096, -0.0073, -0.0155,  0.0171,  0.0002,\n",
       "                       0.0111, -0.0159, -0.0116,  0.0214,  0.0209,  0.0179,  0.0204,  0.0095,\n",
       "                       0.0160, -0.0228, -0.0092, -0.0254, -0.0129,  0.0233,  0.0100,  0.0002,\n",
       "                       0.0198, -0.0013,  0.0030,  0.0213, -0.0221, -0.0063,  0.0183,  0.0141,\n",
       "                      -0.0260, -0.0253, -0.0277, -0.0185, -0.0207,  0.0028,  0.0179, -0.0198,\n",
       "                      -0.0203,  0.0122,  0.0161,  0.0279,  0.0151,  0.0272,  0.0263,  0.0063,\n",
       "                       0.0181,  0.0138,  0.0190,  0.0103, -0.0102,  0.0234,  0.0266, -0.0243,\n",
       "                       0.0178,  0.0167,  0.0184,  0.0190,  0.0014,  0.0012, -0.0184,  0.0198,\n",
       "                      -0.0215, -0.0269,  0.0189, -0.0001, -0.0201, -0.0038,  0.0134,  0.0096,\n",
       "                       0.0032, -0.0267,  0.0149,  0.0190, -0.0180, -0.0085,  0.0160, -0.0235,\n",
       "                       0.0167, -0.0252, -0.0020, -0.0057, -0.0105,  0.0031,  0.0203, -0.0104,\n",
       "                       0.0264, -0.0094,  0.0226, -0.0159, -0.0006,  0.0233, -0.0189,  0.0170,\n",
       "                       0.0007, -0.0244, -0.0093,  0.0090,  0.0242,  0.0043, -0.0180, -0.0169,\n",
       "                       0.0134,  0.0155, -0.0061, -0.0085, -0.0027, -0.0147, -0.0001,  0.0219,\n",
       "                       0.0190,  0.0216, -0.0205,  0.0256, -0.0264,  0.0052, -0.0112, -0.0105,\n",
       "                      -0.0040, -0.0181, -0.0214, -0.0083, -0.0248,  0.0127, -0.0145, -0.0182,\n",
       "                      -0.0163, -0.0257, -0.0256,  0.0059,  0.0039, -0.0056,  0.0238,  0.0222,\n",
       "                      -0.0026, -0.0043, -0.0175, -0.0112, -0.0226, -0.0122,  0.0003, -0.0128,\n",
       "                       0.0148, -0.0254,  0.0156, -0.0103,  0.0161, -0.0036, -0.0066, -0.0022,\n",
       "                      -0.0097, -0.0023,  0.0206, -0.0117,  0.0155,  0.0063, -0.0034,  0.0107,\n",
       "                      -0.0016, -0.0270, -0.0068, -0.0258,  0.0092,  0.0128,  0.0257, -0.0150,\n",
       "                      -0.0089, -0.0006,  0.0202, -0.0224, -0.0013,  0.0264, -0.0091,  0.0072,\n",
       "                      -0.0012, -0.0110, -0.0092,  0.0033,  0.0251,  0.0212,  0.0233,  0.0096,\n",
       "                      -0.0192,  0.0086,  0.0199, -0.0032, -0.0241, -0.0098,  0.0141,  0.0063,\n",
       "                       0.0245,  0.0021,  0.0082,  0.0259, -0.0067,  0.0131,  0.0227, -0.0191,\n",
       "                      -0.0017,  0.0133,  0.0163,  0.0085,  0.0183,  0.0117,  0.0236, -0.0061,\n",
       "                      -0.0251,  0.0177,  0.0034, -0.0063, -0.0124, -0.0083,  0.0175,  0.0166,\n",
       "                      -0.0144, -0.0023,  0.0252, -0.0045, -0.0254, -0.0073, -0.0075, -0.0125],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_local.0.weight',\n",
       "              tensor([[[-0.1176, -0.2082, -0.2670, -0.2398,  0.0261],\n",
       "                       [ 0.2685, -0.1877, -0.1145,  0.2940, -0.0203]],\n",
       "              \n",
       "                      [[-0.2642, -0.1347, -0.0717,  0.2934,  0.0767],\n",
       "                       [ 0.2322, -0.1336,  0.0934, -0.2092, -0.0301]],\n",
       "              \n",
       "                      [[-0.0797,  0.1939, -0.0142,  0.1226, -0.0378],\n",
       "                       [ 0.2131, -0.2783, -0.0064,  0.0275,  0.0432]],\n",
       "              \n",
       "                      [[-0.0471,  0.1659, -0.1998, -0.0269, -0.2732],\n",
       "                       [-0.0477, -0.2361, -0.0121,  0.1824,  0.0212]],\n",
       "              \n",
       "                      [[ 0.0698,  0.0371,  0.0931,  0.0128, -0.1141],\n",
       "                       [-0.0076,  0.0130,  0.1703, -0.0189,  0.0654]],\n",
       "              \n",
       "                      [[ 0.1088,  0.2571, -0.0270,  0.0329,  0.0708],\n",
       "                       [-0.0232,  0.0686, -0.2627,  0.1910,  0.1149]],\n",
       "              \n",
       "                      [[-0.1405, -0.1636,  0.2394,  0.0976, -0.0282],\n",
       "                       [-0.0882,  0.2122, -0.2826,  0.1730,  0.1558]],\n",
       "              \n",
       "                      [[ 0.1459,  0.0359,  0.0490,  0.0857, -0.2840],\n",
       "                       [ 0.2590, -0.0615,  0.1282, -0.1599, -0.1190]],\n",
       "              \n",
       "                      [[-0.1676, -0.0162,  0.0174,  0.0448, -0.2428],\n",
       "                       [ 0.2617,  0.2919, -0.1839, -0.2282, -0.1035]],\n",
       "              \n",
       "                      [[-0.1277,  0.2548, -0.0974,  0.2188, -0.2445],\n",
       "                       [ 0.1828, -0.1902, -0.2948,  0.2535, -0.0731]],\n",
       "              \n",
       "                      [[-0.2781,  0.0004,  0.1687, -0.0653, -0.2606],\n",
       "                       [-0.2385, -0.0133,  0.0144, -0.0201, -0.3062]],\n",
       "              \n",
       "                      [[ 0.1355,  0.2210, -0.0416, -0.0060, -0.2275],\n",
       "                       [-0.0649,  0.0345,  0.0415, -0.1863,  0.1311]],\n",
       "              \n",
       "                      [[ 0.1560,  0.1463, -0.0691,  0.2022,  0.0521],\n",
       "                       [ 0.2039,  0.1875, -0.2083, -0.0538,  0.2322]],\n",
       "              \n",
       "                      [[ 0.0883, -0.1960,  0.2210,  0.1322, -0.1470],\n",
       "                       [ 0.0404, -0.0897, -0.2570, -0.1835, -0.2003]],\n",
       "              \n",
       "                      [[-0.1696,  0.3025,  0.0897,  0.0056,  0.2467],\n",
       "                       [-0.1083, -0.0625, -0.1655, -0.0035,  0.3111]],\n",
       "              \n",
       "                      [[ 0.0183,  0.2829, -0.0882, -0.0714,  0.2406],\n",
       "                       [-0.0898, -0.2242, -0.2599, -0.1521,  0.1328]]], device='cuda:0')),\n",
       "             ('fc_local.0.bias',\n",
       "              tensor([ 0.0650,  0.1726, -0.1951,  0.1278,  0.0080,  0.2168,  0.1635, -0.2111,\n",
       "                       0.2241,  0.1316,  0.0298,  0.0391, -0.0337, -0.0875,  0.2913,  0.2889],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_local.2.weight',\n",
       "              tensor([[[-0.0425,  0.0462,  0.0485,  0.1116, -0.0122],\n",
       "                       [ 0.0085,  0.0796, -0.0597,  0.0107,  0.0828],\n",
       "                       [ 0.0646, -0.0865, -0.1115, -0.0716,  0.0595],\n",
       "                       ...,\n",
       "                       [-0.0670, -0.0572, -0.0556,  0.0631,  0.0604],\n",
       "                       [-0.1087, -0.0041, -0.0854, -0.0711, -0.0984],\n",
       "                       [ 0.0042, -0.0452,  0.0237, -0.0157, -0.0586]],\n",
       "              \n",
       "                      [[-0.0850,  0.0643,  0.0910, -0.1105, -0.0528],\n",
       "                       [ 0.0884, -0.0936,  0.0760, -0.0765,  0.0450],\n",
       "                       [-0.0154, -0.1049, -0.0427, -0.0554,  0.0700],\n",
       "                       ...,\n",
       "                       [-0.0060,  0.0822,  0.1073, -0.0704,  0.0362],\n",
       "                       [-0.0360, -0.0221, -0.0851,  0.0455, -0.0452],\n",
       "                       [-0.0859, -0.0986, -0.0181,  0.0342,  0.0258]],\n",
       "              \n",
       "                      [[ 0.0951,  0.0324, -0.0020,  0.0891,  0.0044],\n",
       "                       [ 0.0318,  0.1000, -0.0013,  0.0745,  0.1008],\n",
       "                       [ 0.0877, -0.0051,  0.0358,  0.0925,  0.1077],\n",
       "                       ...,\n",
       "                       [-0.0571,  0.0457, -0.0073, -0.0781, -0.0415],\n",
       "                       [-0.0980,  0.0574,  0.0101,  0.0040,  0.0044],\n",
       "                       [ 0.0455,  0.0966, -0.0286,  0.0885,  0.0907]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0519, -0.0423, -0.1020, -0.0873,  0.0230],\n",
       "                       [-0.0842, -0.0724,  0.0362,  0.0297, -0.0590],\n",
       "                       [ 0.0803,  0.0039, -0.0489,  0.0991,  0.1070],\n",
       "                       ...,\n",
       "                       [ 0.0960, -0.0330, -0.0935,  0.0636,  0.0406],\n",
       "                       [ 0.0893, -0.0178, -0.0195,  0.0381, -0.0305],\n",
       "                       [ 0.0407,  0.0839,  0.0331, -0.1037, -0.0105]],\n",
       "              \n",
       "                      [[ 0.0868, -0.0832, -0.0174,  0.0206, -0.0688],\n",
       "                       [-0.0418,  0.0381, -0.0518,  0.1059, -0.0969],\n",
       "                       [ 0.0909, -0.0072, -0.0603, -0.0486, -0.1026],\n",
       "                       ...,\n",
       "                       [ 0.1041,  0.1052, -0.0369, -0.0011,  0.0105],\n",
       "                       [-0.1077, -0.0688,  0.0845,  0.0339, -0.0231],\n",
       "                       [ 0.0402,  0.0659,  0.0165, -0.0520,  0.0250]],\n",
       "              \n",
       "                      [[-0.1036, -0.0534,  0.0338,  0.1087,  0.0890],\n",
       "                       [ 0.0173,  0.1003, -0.0109, -0.0219, -0.0192],\n",
       "                       [ 0.0640, -0.0971,  0.0116,  0.0819, -0.0479],\n",
       "                       ...,\n",
       "                       [ 0.1069, -0.0404, -0.0142,  0.0574, -0.0626],\n",
       "                       [ 0.0338,  0.0190, -0.0264,  0.0207,  0.0599],\n",
       "                       [ 0.0693, -0.0638,  0.0771, -0.0876,  0.0073]]], device='cuda:0')),\n",
       "             ('fc_local.2.bias',\n",
       "              tensor([-0.0663,  0.0055,  0.0479,  0.0752, -0.0986,  0.0750,  0.0541, -0.0159,\n",
       "                       0.0173,  0.0614,  0.0176,  0.0711, -0.1001, -0.0116, -0.0446,  0.0986],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_local.5.weight',\n",
       "              tensor([[[ 0.0761, -0.0769,  0.0397, -0.0372,  0.0417],\n",
       "                       [-0.0189,  0.0374,  0.0405,  0.0536, -0.0805],\n",
       "                       [-0.0503, -0.1031,  0.0726, -0.0669,  0.0001],\n",
       "                       ...,\n",
       "                       [-0.0478,  0.0159, -0.0503,  0.0526, -0.0961],\n",
       "                       [-0.0203, -0.0827, -0.0406,  0.0389,  0.0491],\n",
       "                       [ 0.0924, -0.0251, -0.0033,  0.0968, -0.0962]],\n",
       "              \n",
       "                      [[-0.0309,  0.0602,  0.0882, -0.0848, -0.0338],\n",
       "                       [ 0.0950, -0.0177, -0.0918, -0.0843,  0.1024],\n",
       "                       [ 0.0758,  0.0833,  0.0015,  0.0239,  0.1025],\n",
       "                       ...,\n",
       "                       [-0.0151, -0.0915, -0.0613,  0.0319, -0.0510],\n",
       "                       [-0.1068, -0.0552,  0.0635,  0.1024,  0.0544],\n",
       "                       [-0.0500, -0.0455,  0.0605, -0.0707, -0.0953]],\n",
       "              \n",
       "                      [[-0.0398, -0.0000, -0.0749, -0.0796, -0.0127],\n",
       "                       [-0.0543,  0.0601, -0.0247, -0.0260,  0.0757],\n",
       "                       [ 0.0398,  0.0050,  0.1026, -0.0505, -0.0447],\n",
       "                       ...,\n",
       "                       [ 0.0728, -0.0937,  0.0422, -0.0981,  0.0743],\n",
       "                       [-0.0057, -0.1083,  0.0633,  0.0786, -0.0220],\n",
       "                       [ 0.0376,  0.0082,  0.0771, -0.0051, -0.0167]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0253,  0.0269,  0.1062,  0.0170, -0.0597],\n",
       "                       [ 0.0694, -0.0864, -0.0637, -0.1035,  0.0375],\n",
       "                       [ 0.0414,  0.0352, -0.1029,  0.0441,  0.0742],\n",
       "                       ...,\n",
       "                       [ 0.0759, -0.0761, -0.0424,  0.1081,  0.0685],\n",
       "                       [-0.0301,  0.0187,  0.0874,  0.0824, -0.0158],\n",
       "                       [-0.0043,  0.0550, -0.0848,  0.0073, -0.0184]],\n",
       "              \n",
       "                      [[-0.0161, -0.1077,  0.0583,  0.0230,  0.0506],\n",
       "                       [ 0.0016, -0.0567, -0.0439, -0.0506,  0.0637],\n",
       "                       [ 0.0766, -0.0913,  0.0045,  0.0513, -0.1081],\n",
       "                       ...,\n",
       "                       [ 0.0264, -0.0752,  0.0436, -0.0951, -0.1092],\n",
       "                       [ 0.0088, -0.0020,  0.0083, -0.0309,  0.0592],\n",
       "                       [ 0.0193,  0.1045,  0.0673, -0.0319,  0.0772]],\n",
       "              \n",
       "                      [[ 0.0585, -0.0430,  0.0580, -0.0040, -0.0064],\n",
       "                       [-0.0622,  0.0485,  0.0872, -0.1016, -0.1034],\n",
       "                       [-0.0741, -0.0889,  0.0629,  0.0841,  0.0995],\n",
       "                       ...,\n",
       "                       [-0.0489,  0.0891,  0.1007, -0.0874, -0.0753],\n",
       "                       [-0.1019,  0.0027, -0.0277, -0.0285,  0.0076],\n",
       "                       [ 0.0050,  0.0075,  0.0024,  0.1116,  0.0967]]], device='cuda:0')),\n",
       "             ('fc_local.5.bias',\n",
       "              tensor([-0.1089,  0.0232,  0.0878,  0.0059, -0.0604, -0.0362,  0.0319,  0.0823,\n",
       "                      -0.1098,  0.1022, -0.1063, -0.0765, -0.0556,  0.0009,  0.0926,  0.0987,\n",
       "                      -0.0278,  0.1017, -0.0582,  0.0572, -0.0771, -0.0419, -0.0904, -0.0978,\n",
       "                      -0.0316,  0.0296, -0.0914,  0.0099, -0.0357,  0.0382, -0.0331, -0.0630],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_local.7.weight',\n",
       "              tensor([[[ 0.0782,  0.0533, -0.0590,  0.0484, -0.0552],\n",
       "                       [ 0.0060, -0.0236,  0.0711, -0.0471,  0.0297],\n",
       "                       [-0.0059, -0.0714,  0.0785,  0.0588, -0.0375],\n",
       "                       ...,\n",
       "                       [-0.0286,  0.0275,  0.0040,  0.0033,  0.0424],\n",
       "                       [-0.0070,  0.0609,  0.0499,  0.0346, -0.0688],\n",
       "                       [ 0.0750, -0.0347,  0.0564, -0.0734,  0.0445]],\n",
       "              \n",
       "                      [[ 0.0267,  0.0351,  0.0261, -0.0466,  0.0004],\n",
       "                       [ 0.0212, -0.0649,  0.0486,  0.0695,  0.0286],\n",
       "                       [ 0.0776,  0.0356, -0.0762,  0.0593,  0.0370],\n",
       "                       ...,\n",
       "                       [ 0.0392,  0.0250, -0.0234, -0.0495, -0.0657],\n",
       "                       [ 0.0072, -0.0118, -0.0750,  0.0664,  0.0430],\n",
       "                       [ 0.0280, -0.0086,  0.0230, -0.0657,  0.0332]],\n",
       "              \n",
       "                      [[ 0.0710, -0.0755,  0.0135,  0.0603, -0.0522],\n",
       "                       [-0.0044, -0.0512,  0.0731,  0.0562, -0.0746],\n",
       "                       [ 0.0247, -0.0773,  0.0603,  0.0201,  0.0759],\n",
       "                       ...,\n",
       "                       [-0.0004, -0.0390,  0.0489, -0.0054,  0.0172],\n",
       "                       [-0.0524, -0.0049,  0.0196, -0.0718, -0.0740],\n",
       "                       [-0.0673, -0.0244, -0.0665, -0.0672,  0.0712]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0401, -0.0775,  0.0524, -0.0011,  0.0545],\n",
       "                       [ 0.0500,  0.0121,  0.0123,  0.0329,  0.0066],\n",
       "                       [ 0.0375,  0.0680,  0.0449,  0.0264, -0.0226],\n",
       "                       ...,\n",
       "                       [-0.0518,  0.0503,  0.0001, -0.0776, -0.0490],\n",
       "                       [ 0.0283, -0.0038,  0.0191, -0.0263, -0.0527],\n",
       "                       [ 0.0036, -0.0315,  0.0556, -0.0444, -0.0055]],\n",
       "              \n",
       "                      [[-0.0118,  0.0725, -0.0709,  0.0649, -0.0725],\n",
       "                       [ 0.0719, -0.0452,  0.0734,  0.0375,  0.0703],\n",
       "                       [ 0.0598, -0.0513, -0.0754, -0.0731,  0.0075],\n",
       "                       ...,\n",
       "                       [-0.0276,  0.0207,  0.0485,  0.0408, -0.0151],\n",
       "                       [ 0.0445,  0.0230, -0.0783, -0.0751,  0.0156],\n",
       "                       [ 0.0308,  0.0454,  0.0434,  0.0086,  0.0465]],\n",
       "              \n",
       "                      [[-0.0006,  0.0100,  0.0590, -0.0095, -0.0613],\n",
       "                       [-0.0732,  0.0186,  0.0587, -0.0296, -0.0221],\n",
       "                       [ 0.0488,  0.0366,  0.0480,  0.0538,  0.0263],\n",
       "                       ...,\n",
       "                       [ 0.0497,  0.0514,  0.0064, -0.0084,  0.0366],\n",
       "                       [ 0.0703,  0.0234,  0.0300, -0.0244,  0.0710],\n",
       "                       [ 0.0032, -0.0534, -0.0764, -0.0293,  0.0496]]], device='cuda:0')),\n",
       "             ('fc_local.7.bias',\n",
       "              tensor([ 0.0388, -0.0436,  0.0172,  0.0071, -0.0312,  0.0287, -0.0689, -0.0589,\n",
       "                      -0.0437,  0.0736, -0.0666,  0.0328, -0.0455,  0.0661,  0.0692,  0.0168,\n",
       "                      -0.0136,  0.0030,  0.0045,  0.0417, -0.0106, -0.0254, -0.0783, -0.0194,\n",
       "                      -0.0110, -0.0118,  0.0366, -0.0118,  0.0143, -0.0763, -0.0015,  0.0109],\n",
       "                     device='cuda:0')),\n",
       "             ('final_layer.0.weight',\n",
       "              tensor([[ 0.0016, -0.0084,  0.0020,  ..., -0.0051, -0.0002,  0.0107],\n",
       "                      [ 0.0112, -0.0009,  0.0070,  ...,  0.0100, -0.0099,  0.0097],\n",
       "                      [ 0.0037, -0.0109, -0.0090,  ...,  0.0033,  0.0030, -0.0012],\n",
       "                      ...,\n",
       "                      [ 0.0002,  0.0098,  0.0062,  ..., -0.0108, -0.0029, -0.0095],\n",
       "                      [ 0.0099,  0.0073,  0.0085,  ..., -0.0036, -0.0027, -0.0019],\n",
       "                      [-0.0101, -0.0106, -0.0078,  ..., -0.0084, -0.0020, -0.0113]],\n",
       "                     device='cuda:0')),\n",
       "             ('final_layer.0.bias',\n",
       "              tensor([ 0.0090, -0.0035,  0.0025,  0.0040, -0.0046, -0.0093,  0.0094,  0.0075,\n",
       "                       0.0082,  0.0052, -0.0047,  0.0097,  0.0045,  0.0001, -0.0062,  0.0081,\n",
       "                      -0.0046, -0.0071, -0.0043,  0.0056,  0.0091,  0.0041, -0.0025,  0.0032,\n",
       "                      -0.0052, -0.0019, -0.0004,  0.0069, -0.0056,  0.0043, -0.0104,  0.0038,\n",
       "                       0.0089, -0.0097, -0.0092,  0.0024,  0.0109,  0.0107,  0.0092, -0.0006,\n",
       "                       0.0095, -0.0004,  0.0049,  0.0112,  0.0086, -0.0096, -0.0103,  0.0069,\n",
       "                       0.0109,  0.0039,  0.0020, -0.0027,  0.0058, -0.0012,  0.0058, -0.0032,\n",
       "                       0.0031,  0.0003,  0.0025, -0.0030, -0.0055,  0.0029, -0.0099, -0.0062,\n",
       "                      -0.0111,  0.0003,  0.0086, -0.0018, -0.0026, -0.0074,  0.0004, -0.0029,\n",
       "                      -0.0077,  0.0016,  0.0035,  0.0035, -0.0101, -0.0006,  0.0006,  0.0055,\n",
       "                      -0.0102, -0.0073,  0.0034, -0.0076,  0.0065,  0.0088, -0.0050, -0.0025,\n",
       "                       0.0014,  0.0113, -0.0077, -0.0062, -0.0028, -0.0103,  0.0054, -0.0006,\n",
       "                       0.0049, -0.0011,  0.0058,  0.0059, -0.0005,  0.0088, -0.0087,  0.0067,\n",
       "                       0.0016, -0.0023,  0.0004, -0.0088,  0.0077, -0.0102, -0.0020,  0.0034,\n",
       "                      -0.0060, -0.0071,  0.0066,  0.0103, -0.0080, -0.0004, -0.0027,  0.0033,\n",
       "                      -0.0035,  0.0051, -0.0032, -0.0088,  0.0072, -0.0052,  0.0043,  0.0097,\n",
       "                       0.0003,  0.0100, -0.0031, -0.0015,  0.0005, -0.0017, -0.0072, -0.0016,\n",
       "                       0.0005,  0.0091,  0.0066,  0.0100, -0.0063,  0.0076,  0.0077,  0.0110,\n",
       "                      -0.0037, -0.0102, -0.0078,  0.0015,  0.0028,  0.0112,  0.0103, -0.0013,\n",
       "                      -0.0081, -0.0101, -0.0025,  0.0065,  0.0077, -0.0084,  0.0103,  0.0057,\n",
       "                      -0.0090, -0.0033, -0.0065, -0.0016,  0.0079,  0.0003, -0.0068,  0.0044,\n",
       "                      -0.0008, -0.0059, -0.0035,  0.0110,  0.0069,  0.0015,  0.0109, -0.0022,\n",
       "                      -0.0003, -0.0020, -0.0043, -0.0108,  0.0044, -0.0096,  0.0064,  0.0071,\n",
       "                      -0.0052,  0.0088,  0.0059, -0.0069,  0.0012, -0.0044, -0.0091, -0.0086,\n",
       "                       0.0077,  0.0066,  0.0005,  0.0102, -0.0005,  0.0018, -0.0110,  0.0022,\n",
       "                      -0.0065,  0.0029,  0.0074,  0.0098, -0.0064,  0.0031, -0.0069, -0.0043,\n",
       "                      -0.0073,  0.0029, -0.0098,  0.0100, -0.0080,  0.0075, -0.0059, -0.0010,\n",
       "                       0.0099, -0.0037, -0.0015,  0.0098,  0.0019, -0.0076, -0.0024,  0.0070,\n",
       "                       0.0002, -0.0021,  0.0065,  0.0032,  0.0080,  0.0057,  0.0029, -0.0011,\n",
       "                      -0.0039,  0.0015, -0.0018,  0.0021, -0.0108, -0.0040,  0.0092,  0.0077,\n",
       "                      -0.0026, -0.0022,  0.0071,  0.0095, -0.0038,  0.0031, -0.0007, -0.0045,\n",
       "                      -0.0039,  0.0112, -0.0034,  0.0026, -0.0052, -0.0100, -0.0000,  0.0100],\n",
       "                     device='cuda:0')),\n",
       "             ('final_layer.3.weight',\n",
       "              tensor([[-0.0528,  0.0366,  0.0199,  ...,  0.0323,  0.0034, -0.0053],\n",
       "                      [ 0.0427, -0.0386, -0.0156,  ...,  0.0588,  0.0075,  0.0202],\n",
       "                      [ 0.0314, -0.0611,  0.0149,  ..., -0.0132,  0.0382, -0.0234],\n",
       "                      ...,\n",
       "                      [-0.0357,  0.0061,  0.0261,  ..., -0.0438, -0.0366,  0.0000],\n",
       "                      [ 0.0293, -0.0303, -0.0205,  ..., -0.0361, -0.0036,  0.0184],\n",
       "                      [-0.0478,  0.0451,  0.0338,  ..., -0.0605,  0.0086,  0.0427]],\n",
       "                     device='cuda:0')),\n",
       "             ('final_layer.3.bias',\n",
       "              tensor([ 0.0507, -0.0368,  0.0281, -0.0220, -0.0166,  0.0199,  0.0501, -0.0187,\n",
       "                       0.0038,  0.0156,  0.0272, -0.0065,  0.0004,  0.0117, -0.0535, -0.0450,\n",
       "                       0.0303, -0.0100,  0.0327, -0.0125,  0.0506,  0.0428, -0.0451, -0.0095,\n",
       "                       0.0504,  0.0278,  0.0117,  0.0052,  0.0068, -0.0330,  0.0464,  0.0200,\n",
       "                       0.0524,  0.0613, -0.0091, -0.0025,  0.0511,  0.0183,  0.0152,  0.0335,\n",
       "                       0.0146, -0.0138,  0.0217,  0.0419, -0.0149,  0.0508,  0.0329,  0.0047,\n",
       "                       0.0085, -0.0132,  0.0508,  0.0535, -0.0270, -0.0376,  0.0227, -0.0545,\n",
       "                      -0.0295,  0.0096, -0.0151, -0.0087,  0.0569,  0.0205, -0.0534,  0.0009,\n",
       "                      -0.0578, -0.0386,  0.0380,  0.0251, -0.0236,  0.0189,  0.0613, -0.0172,\n",
       "                      -0.0013, -0.0012,  0.0472,  0.0405,  0.0488, -0.0006,  0.0380,  0.0066,\n",
       "                      -0.0217, -0.0073, -0.0425, -0.0535,  0.0315,  0.0400,  0.0347, -0.0141,\n",
       "                       0.0533, -0.0231,  0.0488, -0.0603,  0.0554, -0.0245, -0.0045, -0.0403,\n",
       "                       0.0518, -0.0235, -0.0017, -0.0051, -0.0313,  0.0485,  0.0135,  0.0459,\n",
       "                       0.0303,  0.0228, -0.0224,  0.0050, -0.0469,  0.0600,  0.0187, -0.0452,\n",
       "                      -0.0253, -0.0429, -0.0268,  0.0607,  0.0521,  0.0607, -0.0072, -0.0264,\n",
       "                      -0.0232, -0.0563,  0.0015, -0.0609,  0.0446,  0.0006, -0.0206,  0.0528,\n",
       "                      -0.0396,  0.0525,  0.0239,  0.0374,  0.0549, -0.0136, -0.0136,  0.0269,\n",
       "                       0.0129, -0.0278, -0.0519, -0.0013, -0.0274,  0.0392, -0.0565,  0.0619,\n",
       "                      -0.0076, -0.0426, -0.0240, -0.0274,  0.0059,  0.0420,  0.0093, -0.0184,\n",
       "                       0.0215,  0.0002,  0.0147, -0.0353, -0.0224,  0.0263, -0.0452, -0.0079,\n",
       "                      -0.0260,  0.0463,  0.0570, -0.0549, -0.0041, -0.0437,  0.0085,  0.0435,\n",
       "                      -0.0189, -0.0193, -0.0422,  0.0255,  0.0596,  0.0071, -0.0257,  0.0186,\n",
       "                      -0.0040, -0.0444, -0.0558,  0.0452, -0.0476,  0.0613,  0.0006,  0.0424,\n",
       "                       0.0588,  0.0039,  0.0416,  0.0516,  0.0577,  0.0417, -0.0623, -0.0325,\n",
       "                       0.0153, -0.0228,  0.0074,  0.0130,  0.0034,  0.0074, -0.0034, -0.0183,\n",
       "                      -0.0398, -0.0548, -0.0290,  0.0135, -0.0585, -0.0123, -0.0228, -0.0604,\n",
       "                       0.0226, -0.0399,  0.0300,  0.0570,  0.0460,  0.0250, -0.0151,  0.0312,\n",
       "                      -0.0383,  0.0453,  0.0585,  0.0531,  0.0097,  0.0487,  0.0222, -0.0386,\n",
       "                      -0.0093, -0.0129, -0.0448,  0.0046,  0.0033, -0.0327,  0.0121,  0.0533,\n",
       "                       0.0010, -0.0236,  0.0331,  0.0620,  0.0272, -0.0539,  0.0448,  0.0517,\n",
       "                      -0.0182, -0.0584,  0.0603,  0.0590,  0.0499,  0.0132, -0.0476,  0.0176,\n",
       "                      -0.0237, -0.0447, -0.0009,  0.0295, -0.0602,  0.0183, -0.0510, -0.0091],\n",
       "                     device='cuda:0')),\n",
       "             ('final_layer.5.weight',\n",
       "              tensor([[ 0.0327,  0.0185, -0.0007,  ..., -0.0118,  0.0458,  0.0438],\n",
       "                      [ 0.0429, -0.0159,  0.0070,  ..., -0.0448, -0.0555, -0.0614],\n",
       "                      [ 0.0274, -0.0423,  0.0107,  ...,  0.0027,  0.0573,  0.0165],\n",
       "                      ...,\n",
       "                      [-0.0393,  0.0619, -0.0456,  ...,  0.0586,  0.0511, -0.0303],\n",
       "                      [ 0.0392, -0.0398,  0.0007,  ..., -0.0562,  0.0245,  0.0040],\n",
       "                      [ 0.0512, -0.0185,  0.0155,  ..., -0.0198, -0.0103,  0.0198]],\n",
       "                     device='cuda:0')),\n",
       "             ('final_layer.5.bias',\n",
       "              tensor([ 0.0333,  0.0489,  0.0418,  0.0498, -0.0241,  0.0120, -0.0011,  0.0237,\n",
       "                      -0.0243, -0.0163,  0.0474, -0.0520,  0.0217,  0.0387,  0.0085, -0.0620,\n",
       "                       0.0153,  0.0380, -0.0224, -0.0516,  0.0507,  0.0371,  0.0533, -0.0083,\n",
       "                       0.0437,  0.0583,  0.0403, -0.0043, -0.0160,  0.0513, -0.0182, -0.0297,\n",
       "                      -0.0421,  0.0366,  0.0400,  0.0187, -0.0399,  0.0216, -0.0596, -0.0127,\n",
       "                       0.0182, -0.0554,  0.0378,  0.0056, -0.0560,  0.0091,  0.0079,  0.0500,\n",
       "                       0.0091, -0.0208, -0.0079,  0.0211, -0.0049,  0.0314, -0.0015,  0.0285,\n",
       "                       0.0392,  0.0020, -0.0194, -0.0494,  0.0577,  0.0588,  0.0169,  0.0371,\n",
       "                      -0.0535,  0.0272,  0.0069,  0.0300, -0.0515, -0.0275, -0.0022, -0.0203,\n",
       "                       0.0205,  0.0289,  0.0282,  0.0025,  0.0363, -0.0531,  0.0584,  0.0582,\n",
       "                      -0.0028, -0.0068, -0.0423,  0.0052,  0.0223,  0.0374,  0.0204,  0.0601,\n",
       "                      -0.0229,  0.0266,  0.0419, -0.0373, -0.0169,  0.0218,  0.0439,  0.0284,\n",
       "                       0.0239, -0.0411, -0.0341,  0.0075,  0.0585,  0.0170, -0.0615,  0.0140,\n",
       "                      -0.0493,  0.0263,  0.0024, -0.0349, -0.0488,  0.0427,  0.0294, -0.0169,\n",
       "                       0.0333,  0.0265, -0.0271, -0.0607, -0.0462, -0.0216, -0.0614,  0.0414,\n",
       "                       0.0168,  0.0057,  0.0110,  0.0367, -0.0044, -0.0200,  0.0086,  0.0556,\n",
       "                      -0.0163,  0.0485, -0.0107,  0.0396,  0.0195, -0.0186, -0.0302,  0.0588,\n",
       "                       0.0243,  0.0045, -0.0621,  0.0334, -0.0405, -0.0256,  0.0611,  0.0178,\n",
       "                       0.0008, -0.0444, -0.0591, -0.0617,  0.0534, -0.0407,  0.0363, -0.0303,\n",
       "                       0.0379, -0.0516,  0.0548, -0.0313, -0.0594,  0.0293, -0.0504,  0.0220,\n",
       "                       0.0522, -0.0620, -0.0449, -0.0047,  0.0103, -0.0500, -0.0290,  0.0541,\n",
       "                       0.0369, -0.0229, -0.0403,  0.0239,  0.0478,  0.0152, -0.0584,  0.0465,\n",
       "                      -0.0139,  0.0014,  0.0180, -0.0115, -0.0197,  0.0198,  0.0008,  0.0094,\n",
       "                       0.0303,  0.0450,  0.0051, -0.0102,  0.0003,  0.0370, -0.0285, -0.0471,\n",
       "                       0.0257, -0.0179,  0.0269, -0.0584, -0.0571, -0.0322,  0.0474, -0.0000,\n",
       "                      -0.0210,  0.0444, -0.0548,  0.0376,  0.0506,  0.0233,  0.0083, -0.0092,\n",
       "                      -0.0058,  0.0014, -0.0457,  0.0133, -0.0331,  0.0089, -0.0112, -0.0482,\n",
       "                      -0.0536, -0.0053, -0.0582,  0.0438, -0.0021,  0.0247,  0.0407, -0.0369,\n",
       "                      -0.0566, -0.0028, -0.0110, -0.0041,  0.0514,  0.0374, -0.0189,  0.0381,\n",
       "                      -0.0393,  0.0415, -0.0526, -0.0341,  0.0137, -0.0271,  0.0612, -0.0608,\n",
       "                      -0.0042, -0.0590,  0.0410, -0.0514, -0.0514,  0.0236, -0.0230, -0.0212,\n",
       "                       0.0243, -0.0360, -0.0314, -0.0471, -0.0003, -0.0503,  0.0208, -0.0549],\n",
       "                     device='cuda:0')),\n",
       "             ('final_layer.7.weight',\n",
       "              tensor([[-0.0198,  0.0348, -0.0055,  0.0351, -0.0322,  0.0471,  0.0455, -0.0308,\n",
       "                       -0.0510, -0.0354,  0.0217,  0.0220,  0.0207,  0.0007, -0.0395,  0.0564,\n",
       "                        0.0543, -0.0533,  0.0566, -0.0375,  0.0069,  0.0443,  0.0378,  0.0263,\n",
       "                        0.0107,  0.0433,  0.0297, -0.0433, -0.0056,  0.0564,  0.0511, -0.0521,\n",
       "                       -0.0295,  0.0188,  0.0099,  0.0114,  0.0037,  0.0174,  0.0582,  0.0408,\n",
       "                        0.0384,  0.0567, -0.0533,  0.0208,  0.0406,  0.0081,  0.0245, -0.0462,\n",
       "                        0.0071, -0.0600,  0.0185,  0.0147,  0.0176, -0.0091,  0.0363, -0.0444,\n",
       "                        0.0380, -0.0483,  0.0491, -0.0031,  0.0344, -0.0616, -0.0529,  0.0279,\n",
       "                       -0.0167,  0.0069,  0.0449,  0.0235,  0.0125,  0.0203,  0.0512, -0.0057,\n",
       "                        0.0127,  0.0454, -0.0317,  0.0473, -0.0036,  0.0504, -0.0562,  0.0162,\n",
       "                        0.0531,  0.0317, -0.0342,  0.0006, -0.0358,  0.0324,  0.0410,  0.0464,\n",
       "                       -0.0243, -0.0325,  0.0004, -0.0418, -0.0245,  0.0161,  0.0459, -0.0544,\n",
       "                        0.0425, -0.0264, -0.0367, -0.0169, -0.0545,  0.0551, -0.0254,  0.0274,\n",
       "                       -0.0245, -0.0480,  0.0336, -0.0408, -0.0450,  0.0456, -0.0171, -0.0346,\n",
       "                        0.0181, -0.0037,  0.0295, -0.0621,  0.0243, -0.0576, -0.0124,  0.0389,\n",
       "                       -0.0619,  0.0246,  0.0425, -0.0373,  0.0435,  0.0211,  0.0408,  0.0110,\n",
       "                       -0.0345,  0.0595,  0.0389, -0.0285, -0.0002, -0.0220, -0.0180,  0.0512,\n",
       "                        0.0195,  0.0002,  0.0073, -0.0560, -0.0472, -0.0143,  0.0020, -0.0592,\n",
       "                       -0.0445, -0.0042,  0.0385,  0.0257, -0.0187,  0.0452, -0.0191, -0.0481,\n",
       "                        0.0001, -0.0033, -0.0022, -0.0422, -0.0483, -0.0122, -0.0605, -0.0096,\n",
       "                       -0.0454,  0.0293, -0.0052,  0.0341, -0.0444,  0.0238, -0.0185, -0.0335,\n",
       "                        0.0391,  0.0470, -0.0448, -0.0504,  0.0175,  0.0290, -0.0146, -0.0571,\n",
       "                       -0.0470,  0.0333, -0.0108,  0.0001,  0.0425,  0.0598,  0.0182, -0.0336,\n",
       "                       -0.0280,  0.0410, -0.0414, -0.0435,  0.0586, -0.0394,  0.0512,  0.0335,\n",
       "                       -0.0319, -0.0129, -0.0587,  0.0447,  0.0597, -0.0553, -0.0349, -0.0279,\n",
       "                       -0.0510,  0.0256, -0.0432, -0.0170,  0.0246, -0.0401,  0.0363,  0.0197,\n",
       "                        0.0177, -0.0274, -0.0241,  0.0559, -0.0305, -0.0136,  0.0027,  0.0101,\n",
       "                        0.0160, -0.0617,  0.0603, -0.0235, -0.0460, -0.0204,  0.0225,  0.0131,\n",
       "                       -0.0322,  0.0155,  0.0022, -0.0563, -0.0468,  0.0549, -0.0011, -0.0253,\n",
       "                        0.0578, -0.0406,  0.0061,  0.0474,  0.0560, -0.0554, -0.0340, -0.0476,\n",
       "                       -0.0181,  0.0135,  0.0105, -0.0260,  0.0567, -0.0407, -0.0589, -0.0010,\n",
       "                       -0.0164, -0.0575,  0.0546, -0.0239,  0.0616, -0.0417,  0.0350, -0.0395]],\n",
       "                     device='cuda:0')),\n",
       "             ('final_layer.7.bias', tensor([-0.0182], device='cuda:0'))])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc_global.0.weight',\n",
       "              tensor([[[ 0.2448,  0.0602, -0.0118,  0.2417, -0.1691]],\n",
       "              \n",
       "                      [[-0.0071, -0.0817, -0.2364, -0.1718, -0.2627]],\n",
       "              \n",
       "                      [[-0.1579, -0.2384,  0.1108,  0.0435, -0.2597]],\n",
       "              \n",
       "                      [[ 0.0535,  0.1065,  0.0533,  0.2609, -0.2373]],\n",
       "              \n",
       "                      [[-0.0868, -0.2028,  0.1422, -0.0817,  0.0970]],\n",
       "              \n",
       "                      [[-0.0048,  0.0712,  0.0546,  0.1678,  0.1545]],\n",
       "              \n",
       "                      [[ 0.1922, -0.0577, -0.2565, -0.1893,  0.1228]],\n",
       "              \n",
       "                      [[ 0.0821,  0.1605, -0.0669,  0.1355, -0.2390]],\n",
       "              \n",
       "                      [[ 0.0829,  0.1810, -0.2465, -0.1306,  0.1469]],\n",
       "              \n",
       "                      [[-0.0402, -0.1287,  0.1146, -0.0897,  0.2022]],\n",
       "              \n",
       "                      [[-0.2242, -0.2585,  0.2228, -0.2027,  0.0526]],\n",
       "              \n",
       "                      [[-0.2237,  0.1343,  0.2598,  0.0977,  0.1847]],\n",
       "              \n",
       "                      [[ 0.0756,  0.0841,  0.1990,  0.2394, -0.2648]],\n",
       "              \n",
       "                      [[ 0.2378, -0.1513, -0.2365,  0.1084,  0.0678]],\n",
       "              \n",
       "                      [[-0.2361, -0.1419,  0.0021, -0.0902, -0.2652]],\n",
       "              \n",
       "                      [[ 0.2337, -0.0199,  0.0286, -0.2138, -0.2228]]], device='cuda:0')),\n",
       "             ('fc_global.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_global.2.weight',\n",
       "              tensor([[[-0.1317,  0.1684, -0.0462, -0.0579,  0.0006],\n",
       "                       [ 0.0847,  0.1463, -0.0129, -0.0580, -0.0189],\n",
       "                       [ 0.0322, -0.1600,  0.0951,  0.0291,  0.1030],\n",
       "                       ...,\n",
       "                       [ 0.0433, -0.1780,  0.1147, -0.0543,  0.0311],\n",
       "                       [ 0.0596,  0.1193,  0.1727,  0.1517,  0.1670],\n",
       "                       [ 0.1865,  0.0702, -0.1727, -0.1004,  0.1067]],\n",
       "              \n",
       "                      [[ 0.1398,  0.1202,  0.0950, -0.0162, -0.0834],\n",
       "                       [ 0.1800,  0.1619,  0.0620, -0.0478, -0.0633],\n",
       "                       [ 0.0363,  0.1738,  0.1614,  0.1153,  0.1618],\n",
       "                       ...,\n",
       "                       [ 0.0800, -0.0162, -0.1001,  0.0095,  0.1716],\n",
       "                       [ 0.1017, -0.0488,  0.1263, -0.0698,  0.1283],\n",
       "                       [-0.1892, -0.0061, -0.0953,  0.0354, -0.1424]],\n",
       "              \n",
       "                      [[-0.1500,  0.1581,  0.1205,  0.1605,  0.1287],\n",
       "                       [-0.0887, -0.0527,  0.0639, -0.1052,  0.0714],\n",
       "                       [-0.0123,  0.0702,  0.1393, -0.1699,  0.0482],\n",
       "                       ...,\n",
       "                       [ 0.1762,  0.1209,  0.1597,  0.1891, -0.0138],\n",
       "                       [ 0.0748,  0.0418, -0.1115,  0.0753,  0.1684],\n",
       "                       [ 0.0732,  0.0653, -0.1166,  0.1175,  0.1221]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.1327, -0.1292, -0.0977, -0.0428,  0.1038],\n",
       "                       [ 0.1176, -0.1567,  0.0068, -0.1546,  0.1795],\n",
       "                       [ 0.1200,  0.1872,  0.0012,  0.0572, -0.1650],\n",
       "                       ...,\n",
       "                       [-0.1843, -0.1259,  0.1791,  0.1301,  0.0777],\n",
       "                       [ 0.0224, -0.0403, -0.0937, -0.1462,  0.1901],\n",
       "                       [-0.1490,  0.0465, -0.1582,  0.1428,  0.0455]],\n",
       "              \n",
       "                      [[ 0.1059, -0.0993,  0.0279, -0.0536,  0.1735],\n",
       "                       [-0.0161,  0.1060,  0.1140, -0.0605,  0.1021],\n",
       "                       [ 0.1491, -0.1768,  0.1504,  0.1169, -0.0299],\n",
       "                       ...,\n",
       "                       [-0.0615, -0.1262,  0.0885, -0.1056,  0.0334],\n",
       "                       [-0.1641, -0.0650,  0.0213, -0.1910, -0.1016],\n",
       "                       [-0.0035,  0.0440, -0.0843,  0.1379,  0.0834]],\n",
       "              \n",
       "                      [[ 0.1352, -0.1771,  0.1790, -0.1135,  0.0700],\n",
       "                       [ 0.1756, -0.1559, -0.0744, -0.0015, -0.0570],\n",
       "                       [-0.1101, -0.0953,  0.1506,  0.1672, -0.0632],\n",
       "                       ...,\n",
       "                       [ 0.0133,  0.0105, -0.1493, -0.0193, -0.1634],\n",
       "                       [-0.0384, -0.0260,  0.0611,  0.1391,  0.0880],\n",
       "                       [-0.0478, -0.1324, -0.0971, -0.0429, -0.1672]]], device='cuda:0')),\n",
       "             ('fc_global.2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_global.5.weight',\n",
       "              tensor([[[-0.0490,  0.0376, -0.0921,  0.0459, -0.0706],\n",
       "                       [-0.1277, -0.0600,  0.0133,  0.1514,  0.1240],\n",
       "                       [-0.0656,  0.1061,  0.1193,  0.0603,  0.1252],\n",
       "                       ...,\n",
       "                       [ 0.0455, -0.0631,  0.1054,  0.0830, -0.0515],\n",
       "                       [-0.1405, -0.1184, -0.1061, -0.0168,  0.0630],\n",
       "                       [ 0.1394,  0.0173, -0.0493,  0.0478, -0.0030]],\n",
       "              \n",
       "                      [[-0.1101,  0.1230,  0.1045, -0.0365, -0.1104],\n",
       "                       [ 0.0691,  0.1189,  0.0826,  0.0818,  0.1248],\n",
       "                       [ 0.0322, -0.0703, -0.0419, -0.1108, -0.1543],\n",
       "                       ...,\n",
       "                       [-0.1014,  0.1223, -0.0402,  0.0223, -0.1171],\n",
       "                       [ 0.0264, -0.0623, -0.0409, -0.1070, -0.0107],\n",
       "                       [ 0.0805,  0.0190, -0.1005,  0.0584,  0.0581]],\n",
       "              \n",
       "                      [[ 0.1187,  0.1164, -0.0533,  0.0874, -0.0980],\n",
       "                       [ 0.0487, -0.0488,  0.0439, -0.1476,  0.1376],\n",
       "                       [-0.0651,  0.0559,  0.1501,  0.1077,  0.0747],\n",
       "                       ...,\n",
       "                       [ 0.0504,  0.1351,  0.0004, -0.1387,  0.1046],\n",
       "                       [-0.1149,  0.1389,  0.1383,  0.0540,  0.0856],\n",
       "                       [ 0.0657, -0.0265, -0.1257, -0.1547,  0.1468]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0060,  0.1106, -0.1393, -0.0137, -0.0900],\n",
       "                       [ 0.0064,  0.0178, -0.1382, -0.1153,  0.0215],\n",
       "                       [ 0.0464,  0.0898,  0.0494, -0.0953,  0.0920],\n",
       "                       ...,\n",
       "                       [-0.0669,  0.1085,  0.0735,  0.1007, -0.1245],\n",
       "                       [-0.0701, -0.1421, -0.0747, -0.1371, -0.0029],\n",
       "                       [-0.0073,  0.0892, -0.1315, -0.0552,  0.1060]],\n",
       "              \n",
       "                      [[ 0.0047,  0.0465, -0.1122, -0.0746,  0.0560],\n",
       "                       [ 0.1535, -0.0153, -0.1425, -0.0767,  0.0267],\n",
       "                       [ 0.0564,  0.1538,  0.0944, -0.0579, -0.0371],\n",
       "                       ...,\n",
       "                       [ 0.0394,  0.1326, -0.0639,  0.1094,  0.0264],\n",
       "                       [-0.1165, -0.0244,  0.0038, -0.0657, -0.0110],\n",
       "                       [ 0.0915, -0.0888,  0.0785, -0.0157, -0.1515]],\n",
       "              \n",
       "                      [[ 0.1103,  0.0733,  0.0153,  0.0838,  0.0789],\n",
       "                       [ 0.1396, -0.0862, -0.0712, -0.1153,  0.1203],\n",
       "                       [-0.0517,  0.0301,  0.1102, -0.0859,  0.0923],\n",
       "                       ...,\n",
       "                       [-0.0497, -0.0079, -0.0811,  0.0095,  0.1090],\n",
       "                       [-0.0002,  0.1502, -0.0565,  0.1171,  0.0939],\n",
       "                       [-0.0982, -0.1334,  0.0526,  0.0212,  0.0892]]], device='cuda:0')),\n",
       "             ('fc_global.5.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_global.7.weight',\n",
       "              tensor([[[ 0.0747, -0.0041,  0.1149,  0.0592,  0.1205],\n",
       "                       [-0.0008, -0.0975, -0.0531, -0.1035,  0.0617],\n",
       "                       [ 0.1361, -0.0344,  0.1302,  0.0816,  0.0363],\n",
       "                       ...,\n",
       "                       [-0.0056,  0.0404, -0.1218, -0.0148,  0.0575],\n",
       "                       [ 0.1257, -0.1227, -0.0905, -0.0526, -0.0502],\n",
       "                       [-0.0528,  0.0066, -0.1203,  0.1232,  0.0935]],\n",
       "              \n",
       "                      [[-0.0251,  0.0596, -0.0511,  0.0795,  0.1362],\n",
       "                       [ 0.0853,  0.0837, -0.1149,  0.0190, -0.0742],\n",
       "                       [-0.0474,  0.0992,  0.0173,  0.1368,  0.1264],\n",
       "                       ...,\n",
       "                       [ 0.0942,  0.1330,  0.0759,  0.1062,  0.1257],\n",
       "                       [-0.0972,  0.1321,  0.0627,  0.0552,  0.1234],\n",
       "                       [-0.0156,  0.0965, -0.0257,  0.1119,  0.0395]],\n",
       "              \n",
       "                      [[-0.0697,  0.0362, -0.0952, -0.1151,  0.0563],\n",
       "                       [-0.0495, -0.0132,  0.1334, -0.0747, -0.0513],\n",
       "                       [-0.1058,  0.0603,  0.0905, -0.0168,  0.1085],\n",
       "                       ...,\n",
       "                       [-0.0018,  0.0618, -0.1002,  0.0522, -0.0667],\n",
       "                       [-0.0377,  0.0139, -0.1298,  0.0132, -0.0262],\n",
       "                       [-0.0508,  0.0465, -0.1263,  0.0485, -0.1018]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0938,  0.0663,  0.0864, -0.1064,  0.0791],\n",
       "                       [-0.0181,  0.0322,  0.0630, -0.0782,  0.0338],\n",
       "                       [ 0.0557,  0.0763,  0.0123, -0.0318, -0.0365],\n",
       "                       ...,\n",
       "                       [ 0.0253, -0.0209, -0.0156,  0.0293, -0.0551],\n",
       "                       [ 0.1057, -0.0506,  0.0640, -0.0867, -0.0907],\n",
       "                       [-0.0388,  0.0863,  0.1188,  0.1312,  0.0516]],\n",
       "              \n",
       "                      [[-0.1145, -0.0245, -0.1020, -0.0888, -0.0643],\n",
       "                       [-0.0962,  0.0497,  0.0642, -0.1299,  0.1352],\n",
       "                       [-0.1163,  0.1363, -0.1306,  0.1222,  0.0302],\n",
       "                       ...,\n",
       "                       [-0.0084,  0.0009, -0.0486,  0.0622,  0.0126],\n",
       "                       [-0.1015,  0.1199, -0.0502, -0.0406,  0.0453],\n",
       "                       [-0.1305, -0.0065,  0.0010,  0.0444,  0.0077]],\n",
       "              \n",
       "                      [[ 0.1112,  0.0562,  0.0443, -0.1329, -0.0832],\n",
       "                       [ 0.1061,  0.0548, -0.0570, -0.0900,  0.0794],\n",
       "                       [-0.1278, -0.0372, -0.0984,  0.0539, -0.0813],\n",
       "                       ...,\n",
       "                       [ 0.1132,  0.0710,  0.0546, -0.0796, -0.0467],\n",
       "                       [ 0.0769, -0.0631, -0.0199, -0.1059, -0.0646],\n",
       "                       [-0.1291,  0.1031,  0.0709,  0.0264,  0.0497]]], device='cuda:0')),\n",
       "             ('fc_global.7.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_global.10.weight',\n",
       "              tensor([[[-0.0988, -0.0219, -0.0387,  0.0376,  0.1114],\n",
       "                       [-0.0103,  0.0731,  0.1060, -0.0274, -0.0161],\n",
       "                       [-0.0998, -0.0557, -0.0288, -0.0066,  0.0521],\n",
       "                       ...,\n",
       "                       [-0.0116,  0.0779, -0.0622, -0.0053,  0.0967],\n",
       "                       [-0.0721, -0.0586, -0.0268,  0.0349, -0.0404],\n",
       "                       [-0.0756,  0.0273, -0.0638,  0.1014, -0.1048]],\n",
       "              \n",
       "                      [[-0.0630,  0.0086, -0.0813, -0.0057, -0.0099],\n",
       "                       [-0.0120, -0.1085,  0.0775, -0.0020,  0.1053],\n",
       "                       [-0.0911,  0.0053,  0.0586,  0.0489,  0.0282],\n",
       "                       ...,\n",
       "                       [-0.0684,  0.0490, -0.0646,  0.0760, -0.0424],\n",
       "                       [-0.0471, -0.0326,  0.0045, -0.1058,  0.0048],\n",
       "                       [ 0.0165,  0.0237,  0.0125, -0.0884, -0.0707]],\n",
       "              \n",
       "                      [[-0.0338,  0.0919, -0.0159,  0.0417,  0.0399],\n",
       "                       [ 0.0037, -0.1027, -0.0628,  0.0490, -0.0940],\n",
       "                       [ 0.0764, -0.1079, -0.0882, -0.0430,  0.1072],\n",
       "                       ...,\n",
       "                       [-0.0268,  0.0314, -0.0086,  0.0622, -0.0300],\n",
       "                       [ 0.0446,  0.0408, -0.0024,  0.0436, -0.0445],\n",
       "                       [-0.0459, -0.1103, -0.0993, -0.1008, -0.0529]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0608,  0.0813, -0.0886, -0.0434, -0.0850],\n",
       "                       [-0.0333, -0.0263,  0.0362,  0.0933,  0.0710],\n",
       "                       [ 0.0636, -0.0480,  0.0013,  0.1006,  0.0849],\n",
       "                       ...,\n",
       "                       [-0.0427,  0.0445, -0.0111, -0.0521,  0.0513],\n",
       "                       [ 0.0492, -0.0967,  0.0876, -0.0952,  0.0058],\n",
       "                       [ 0.0886,  0.0507, -0.1109,  0.0584,  0.0757]],\n",
       "              \n",
       "                      [[-0.0280, -0.0550, -0.0818, -0.0685,  0.0360],\n",
       "                       [ 0.0810,  0.0528,  0.0127,  0.0458,  0.0083],\n",
       "                       [-0.0663, -0.0258,  0.0791,  0.0635, -0.0791],\n",
       "                       ...,\n",
       "                       [-0.0446,  0.0015, -0.0309,  0.0752,  0.0353],\n",
       "                       [-0.0196, -0.0472,  0.0364, -0.0102,  0.0612],\n",
       "                       [-0.0941, -0.0616, -0.0086,  0.0725, -0.0656]],\n",
       "              \n",
       "                      [[-0.0634, -0.0844, -0.0425,  0.0638, -0.0551],\n",
       "                       [ 0.0397,  0.1005, -0.0119,  0.0277,  0.0989],\n",
       "                       [-0.0157,  0.1013, -0.0326,  0.0069,  0.0111],\n",
       "                       ...,\n",
       "                       [-0.0387, -0.0020,  0.0228, -0.0797,  0.0082],\n",
       "                       [-0.0779, -0.0715, -0.0178, -0.0650,  0.0766],\n",
       "                       [-0.0764, -0.0167,  0.1106, -0.0508,  0.0420]]], device='cuda:0')),\n",
       "             ('fc_global.10.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('fc_global.12.weight',\n",
       "              tensor([[[-0.0687, -0.0925,  0.0097,  0.0045,  0.0203],\n",
       "                       [-0.0283, -0.0803,  0.0195, -0.0868,  0.0790],\n",
       "                       [-0.0797, -0.0300, -0.0265,  0.0717, -0.0602],\n",
       "                       ...,\n",
       "                       [-0.0593,  0.0681,  0.0357,  0.0810, -0.0706],\n",
       "                       [ 0.0814,  0.0627,  0.0278, -0.0873,  0.0105],\n",
       "                       [ 0.0582, -0.0539,  0.0454,  0.0189, -0.0209]],\n",
       "              \n",
       "                      [[-0.0093,  0.0642,  0.0013, -0.0071,  0.0101],\n",
       "                       [ 0.0272, -0.0732, -0.0766,  0.0076, -0.0524],\n",
       "                       [-0.0548, -0.0160, -0.0009,  0.0155,  0.0220],\n",
       "                       ...,\n",
       "                       [-0.0279,  0.0277,  0.0466,  0.0638, -0.0078],\n",
       "                       [-0.0337,  0.0454, -0.0672, -0.0661, -0.0789],\n",
       "                       [-0.0339, -0.0605,  0.0613,  0.0647, -0.0908]],\n",
       "              \n",
       "                      [[ 0.0516, -0.0815,  0.0289,  0.0364, -0.0418],\n",
       "                       [-0.0609, -0.0236,  0.0877, -0.0790, -0.0487],\n",
       "                       [ 0.0413,  0.0687,  0.0558, -0.0597, -0.0464],\n",
       "                       ...,\n",
       "                       [ 0.0070, -0.0105, -0.0576, -0.0360,  0.0517],\n",
       "                       [-0.0174,  0.0651, -0.0856, -0.0760,  0.0321],\n",
       "                       [-0.0442,  0.0543,  0.0015, -0.0432, -0.0732]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0067, -0.0004, -0.0486,  0.0033, -0.0303],\n",
       "                       [ 0.0906, -0.0831, -0.0056,  0.0909,  0.0400],\n",
       "                       [ 0.0373, -0.0223, -0.0679,  0.0575,  0.0335],\n",
       "                       ...,\n",
       "                       [-0.0967,  0.0220,  0.0325,  0.0782, -0.0231],\n",
       "                       [-0.0511, -0.0510, -0.0570, -0.0292,  0.0534],\n",
       "                       [ 0.0461, -0.0846,  0.0411,  0.0818,  0.0319]],\n",
       "              \n",
       "                      [[-0.0756,  0.0275,  0.0738, -0.0675, -0.0350],\n",
       "                       [-0.0691,  0.0802,  0.0947, -0.0483, -0.0159],\n",
       "                       [ 0.0280,  0.0204, -0.0442,  0.0597,  0.0071],\n",
       "                       ...,\n",
       "                       [ 0.0949, -0.0149, -0.0207, -0.0247,  0.0500],\n",
       "                       [-0.0951, -0.0064,  0.0601,  0.0816, -0.0818],\n",
       "                       [ 0.0021, -0.0393,  0.0716, -0.0690,  0.0809]],\n",
       "              \n",
       "                      [[ 0.0360, -0.0830,  0.0207,  0.0333,  0.0428],\n",
       "                       [-0.0609,  0.0683,  0.0124, -0.0905,  0.0240],\n",
       "                       [ 0.0469,  0.0428, -0.0031, -0.0810, -0.0360],\n",
       "                       ...,\n",
       "                       [-0.0229, -0.0371, -0.0018,  0.0004, -0.0476],\n",
       "                       [ 0.0149, -0.0712, -0.0963, -0.0086,  0.0229],\n",
       "                       [-0.0749, -0.0762,  0.0149, -0.0381, -0.0160]]], device='cuda:0')),\n",
       "             ('fc_global.12.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('fc_global.15.weight',\n",
       "              tensor([[[ 0.0509, -0.0094,  0.0366, -0.0637,  0.0320],\n",
       "                       [ 0.0151, -0.0568,  0.0364, -0.0701,  0.0327],\n",
       "                       [-0.0039,  0.0595, -0.0783, -0.0395, -0.0268],\n",
       "                       ...,\n",
       "                       [ 0.0531, -0.0635,  0.0630,  0.0144, -0.0306],\n",
       "                       [-0.0470,  0.0443,  0.0400,  0.0305, -0.0260],\n",
       "                       [-0.0126,  0.0505,  0.0040, -0.0257, -0.0720]],\n",
       "              \n",
       "                      [[-0.0018, -0.0417,  0.0429,  0.0582,  0.0124],\n",
       "                       [-0.0542,  0.0338,  0.0565, -0.0762, -0.0278],\n",
       "                       [-0.0209,  0.0703, -0.0473,  0.0227,  0.0706],\n",
       "                       ...,\n",
       "                       [-0.0515,  0.0604,  0.0434,  0.0481, -0.0107],\n",
       "                       [ 0.0769, -0.0238,  0.0265, -0.0292,  0.0484],\n",
       "                       [-0.0745,  0.0114, -0.0601,  0.0786,  0.0725]],\n",
       "              \n",
       "                      [[ 0.0442,  0.0573,  0.0459, -0.0582, -0.0205],\n",
       "                       [-0.0151,  0.0066,  0.0730,  0.0257,  0.0210],\n",
       "                       [-0.0552,  0.0129, -0.0776, -0.0410, -0.0337],\n",
       "                       ...,\n",
       "                       [-0.0507, -0.0590,  0.0640, -0.0513, -0.0453],\n",
       "                       [ 0.0211, -0.0447,  0.0695,  0.0688, -0.0381],\n",
       "                       [ 0.0631,  0.0651,  0.0026,  0.0483,  0.0014]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0752, -0.0071,  0.0377, -0.0202,  0.0518],\n",
       "                       [ 0.0430, -0.0547,  0.0002, -0.0567,  0.0594],\n",
       "                       [ 0.0282,  0.0692, -0.0563,  0.0079, -0.0210],\n",
       "                       ...,\n",
       "                       [ 0.0289, -0.0303,  0.0692, -0.0300,  0.0510],\n",
       "                       [-0.0311,  0.0720, -0.0089, -0.0409, -0.0730],\n",
       "                       [ 0.0416, -0.0220,  0.0465,  0.0285,  0.0264]],\n",
       "              \n",
       "                      [[-0.0681, -0.0149, -0.0181,  0.0788,  0.0421],\n",
       "                       [ 0.0682,  0.0656, -0.0661,  0.0479,  0.0148],\n",
       "                       [ 0.0184,  0.0044, -0.0124,  0.0099,  0.0101],\n",
       "                       ...,\n",
       "                       [-0.0729,  0.0532, -0.0212,  0.0126, -0.0369],\n",
       "                       [-0.0641, -0.0640,  0.0267,  0.0550, -0.0014],\n",
       "                       [-0.0260, -0.0118, -0.0216,  0.0504,  0.0150]],\n",
       "              \n",
       "                      [[-0.0706,  0.0563,  0.0556, -0.0496,  0.0345],\n",
       "                       [-0.0617,  0.0084, -0.0423, -0.0519, -0.0269],\n",
       "                       [-0.0100,  0.0493,  0.0647, -0.0302, -0.0023],\n",
       "                       ...,\n",
       "                       [ 0.0467,  0.0540,  0.0681, -0.0263, -0.0048],\n",
       "                       [-0.0559, -0.0186, -0.0282,  0.0135,  0.0515],\n",
       "                       [-0.0506, -0.0034,  0.0100, -0.0286, -0.0277]]], device='cuda:0')),\n",
       "             ('fc_global.15.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0.], device='cuda:0')),\n",
       "             ('fc_global.17.weight',\n",
       "              tensor([[[-0.0541, -0.0530,  0.0157, -0.0237,  0.0074],\n",
       "                       [-0.0407,  0.0231,  0.0193,  0.0053, -0.0546],\n",
       "                       [ 0.0289,  0.0051,  0.0258, -0.0644,  0.0411],\n",
       "                       ...,\n",
       "                       [ 0.0368,  0.0608,  0.0652,  0.0311, -0.0440],\n",
       "                       [ 0.0627,  0.0524, -0.0494,  0.0599, -0.0576],\n",
       "                       [-0.0608, -0.0191,  0.0326, -0.0683, -0.0645]],\n",
       "              \n",
       "                      [[ 0.0155,  0.0396,  0.0240, -0.0255,  0.0495],\n",
       "                       [-0.0464,  0.0096,  0.0098,  0.0243, -0.0136],\n",
       "                       [-0.0012, -0.0174,  0.0323, -0.0142,  0.0628],\n",
       "                       ...,\n",
       "                       [ 0.0563,  0.0668, -0.0235,  0.0505,  0.0343],\n",
       "                       [-0.0585, -0.0653,  0.0404,  0.0038, -0.0673],\n",
       "                       [-0.0366,  0.0449,  0.0173, -0.0565, -0.0602]],\n",
       "              \n",
       "                      [[ 0.0559, -0.0394, -0.0602, -0.0613, -0.0325],\n",
       "                       [ 0.0304, -0.0566,  0.0670,  0.0143, -0.0641],\n",
       "                       [ 0.0206,  0.0432, -0.0404, -0.0536,  0.0208],\n",
       "                       ...,\n",
       "                       [-0.0514,  0.0601,  0.0334,  0.0206, -0.0655],\n",
       "                       [ 0.0110,  0.0272, -0.0660, -0.0479, -0.0024],\n",
       "                       [ 0.0216, -0.0163,  0.0677,  0.0075,  0.0436]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0630,  0.0353, -0.0515,  0.0660, -0.0222],\n",
       "                       [ 0.0430, -0.0158,  0.0394, -0.0547, -0.0570],\n",
       "                       [-0.0473,  0.0335, -0.0493,  0.0484,  0.0591],\n",
       "                       ...,\n",
       "                       [ 0.0006,  0.0231,  0.0093,  0.0630, -0.0060],\n",
       "                       [ 0.0523,  0.0424,  0.0208,  0.0113, -0.0461],\n",
       "                       [ 0.0554,  0.0374, -0.0097,  0.0646, -0.0445]],\n",
       "              \n",
       "                      [[-0.0368, -0.0232,  0.0506,  0.0630, -0.0062],\n",
       "                       [-0.0015,  0.0199,  0.0203, -0.0048,  0.0024],\n",
       "                       [ 0.0333,  0.0051,  0.0235,  0.0602,  0.0466],\n",
       "                       ...,\n",
       "                       [ 0.0276, -0.0500,  0.0510, -0.0252, -0.0554],\n",
       "                       [-0.0169,  0.0249,  0.0441, -0.0242,  0.0404],\n",
       "                       [ 0.0260, -0.0176, -0.0110,  0.0380, -0.0264]],\n",
       "              \n",
       "                      [[ 0.0556,  0.0238, -0.0384, -0.0379, -0.0222],\n",
       "                       [-0.0304,  0.0137,  0.0248,  0.0292,  0.0009],\n",
       "                       [ 0.0216,  0.0178, -0.0493, -0.0541,  0.0305],\n",
       "                       ...,\n",
       "                       [ 0.0644, -0.0023, -0.0684,  0.0303, -0.0241],\n",
       "                       [-0.0008,  0.0338,  0.0026,  0.0317, -0.0045],\n",
       "                       [-0.0274, -0.0408, -0.0206, -0.0286,  0.0449]]], device='cuda:0')),\n",
       "             ('fc_global.17.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0.], device='cuda:0')),\n",
       "             ('fc_global.20.weight',\n",
       "              tensor([[[ 0.0276, -0.0249, -0.0052, -0.0266,  0.0321],\n",
       "                       [ 0.0244, -0.0170,  0.0489,  0.0546, -0.0293],\n",
       "                       [-0.0028, -0.0358,  0.0141, -0.0178,  0.0306],\n",
       "                       ...,\n",
       "                       [ 0.0453, -0.0319,  0.0465, -0.0214,  0.0048],\n",
       "                       [-0.0057,  0.0469,  0.0486, -0.0250, -0.0084],\n",
       "                       [ 0.0362, -0.0328,  0.0103,  0.0280,  0.0210]],\n",
       "              \n",
       "                      [[-0.0187,  0.0159,  0.0464, -0.0355, -0.0263],\n",
       "                       [-0.0384, -0.0324,  0.0230,  0.0150, -0.0329],\n",
       "                       [ 0.0024, -0.0018, -0.0016, -0.0477,  0.0224],\n",
       "                       ...,\n",
       "                       [ 0.0026, -0.0196,  0.0164, -0.0482, -0.0474],\n",
       "                       [-0.0490,  0.0431,  0.0272,  0.0109, -0.0442],\n",
       "                       [ 0.0449, -0.0370,  0.0439, -0.0507, -0.0245]],\n",
       "              \n",
       "                      [[-0.0260, -0.0528, -0.0491,  0.0125,  0.0054],\n",
       "                       [-0.0054, -0.0291,  0.0248,  0.0280, -0.0236],\n",
       "                       [-0.0277, -0.0158, -0.0341,  0.0299, -0.0348],\n",
       "                       ...,\n",
       "                       [ 0.0056,  0.0125,  0.0311,  0.0267,  0.0002],\n",
       "                       [-0.0282, -0.0476, -0.0360, -0.0397,  0.0416],\n",
       "                       [-0.0158, -0.0510,  0.0018,  0.0111, -0.0068]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0266, -0.0514,  0.0104, -0.0182,  0.0468],\n",
       "                       [-0.0101, -0.0379, -0.0438,  0.0501, -0.0313],\n",
       "                       [-0.0446, -0.0558, -0.0185, -0.0354, -0.0276],\n",
       "                       ...,\n",
       "                       [ 0.0064,  0.0343, -0.0218,  0.0107,  0.0391],\n",
       "                       [-0.0430,  0.0327,  0.0374, -0.0069, -0.0473],\n",
       "                       [-0.0435,  0.0322, -0.0246, -0.0389, -0.0141]],\n",
       "              \n",
       "                      [[-0.0123, -0.0371, -0.0522, -0.0177,  0.0266],\n",
       "                       [-0.0091,  0.0286,  0.0451,  0.0254,  0.0212],\n",
       "                       [ 0.0257, -0.0550,  0.0440, -0.0183,  0.0544],\n",
       "                       ...,\n",
       "                       [-0.0258,  0.0394,  0.0470,  0.0377, -0.0433],\n",
       "                       [ 0.0334, -0.0299, -0.0101, -0.0286,  0.0251],\n",
       "                       [ 0.0291,  0.0275,  0.0117, -0.0532,  0.0108]],\n",
       "              \n",
       "                      [[ 0.0465, -0.0343, -0.0054,  0.0119, -0.0194],\n",
       "                       [-0.0136, -0.0211,  0.0036,  0.0394, -0.0487],\n",
       "                       [-0.0454,  0.0097, -0.0099,  0.0243, -0.0190],\n",
       "                       ...,\n",
       "                       [ 0.0549,  0.0405,  0.0328, -0.0306, -0.0505],\n",
       "                       [-0.0223,  0.0006, -0.0103, -0.0315, -0.0225],\n",
       "                       [ 0.0288,  0.0360,  0.0132,  0.0547, -0.0084]]], device='cuda:0')),\n",
       "             ('fc_global.20.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('fc_global.22.weight',\n",
       "              tensor([[[ 0.0077,  0.0428, -0.0355,  0.0130,  0.0466],\n",
       "                       [ 0.0141,  0.0410,  0.0467, -0.0012,  0.0338],\n",
       "                       [-0.0445, -0.0402, -0.0140,  0.0181, -0.0109],\n",
       "                       ...,\n",
       "                       [-0.0358, -0.0159, -0.0143, -0.0079,  0.0064],\n",
       "                       [-0.0106,  0.0377, -0.0062, -0.0122,  0.0007],\n",
       "                       [-0.0386,  0.0215, -0.0318, -0.0368,  0.0297]],\n",
       "              \n",
       "                      [[ 0.0067, -0.0400,  0.0448, -0.0317, -0.0258],\n",
       "                       [ 0.0373, -0.0382, -0.0054, -0.0351, -0.0258],\n",
       "                       [-0.0175,  0.0012, -0.0144, -0.0402,  0.0153],\n",
       "                       ...,\n",
       "                       [ 0.0011,  0.0089, -0.0056, -0.0162, -0.0427],\n",
       "                       [-0.0055, -0.0122, -0.0124, -0.0067,  0.0141],\n",
       "                       [-0.0154, -0.0220,  0.0063, -0.0412, -0.0404]],\n",
       "              \n",
       "                      [[-0.0155, -0.0161, -0.0470, -0.0126,  0.0383],\n",
       "                       [-0.0179,  0.0352,  0.0118,  0.0480, -0.0476],\n",
       "                       [ 0.0121, -0.0095, -0.0206,  0.0068,  0.0106],\n",
       "                       ...,\n",
       "                       [ 0.0383, -0.0291, -0.0474,  0.0366,  0.0083],\n",
       "                       [ 0.0061, -0.0326,  0.0200, -0.0400, -0.0415],\n",
       "                       [ 0.0462,  0.0315,  0.0369, -0.0093,  0.0275]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.0185, -0.0114, -0.0062,  0.0180,  0.0148],\n",
       "                       [-0.0385,  0.0432,  0.0385,  0.0232, -0.0134],\n",
       "                       [ 0.0268, -0.0215, -0.0186,  0.0407, -0.0038],\n",
       "                       ...,\n",
       "                       [ 0.0332,  0.0066, -0.0185,  0.0378, -0.0283],\n",
       "                       [ 0.0130,  0.0478, -0.0138, -0.0069,  0.0226],\n",
       "                       [-0.0425,  0.0335,  0.0441,  0.0340,  0.0208]],\n",
       "              \n",
       "                      [[ 0.0396,  0.0343,  0.0006, -0.0450, -0.0221],\n",
       "                       [-0.0426,  0.0481, -0.0039,  0.0425,  0.0268],\n",
       "                       [-0.0246, -0.0301, -0.0362,  0.0358,  0.0128],\n",
       "                       ...,\n",
       "                       [-0.0141, -0.0037,  0.0033,  0.0057, -0.0322],\n",
       "                       [-0.0091,  0.0230,  0.0328, -0.0023,  0.0372],\n",
       "                       [-0.0361,  0.0085, -0.0435, -0.0256,  0.0433]],\n",
       "              \n",
       "                      [[-0.0311,  0.0032,  0.0039,  0.0363,  0.0442],\n",
       "                       [ 0.0025,  0.0003,  0.0109, -0.0254, -0.0194],\n",
       "                       [-0.0010, -0.0362, -0.0081,  0.0364, -0.0305],\n",
       "                       ...,\n",
       "                       [-0.0348,  0.0005, -0.0301,  0.0177,  0.0449],\n",
       "                       [ 0.0214, -0.0388, -0.0455, -0.0061,  0.0295],\n",
       "                       [ 0.0091, -0.0361, -0.0114, -0.0348,  0.0280]]], device='cuda:0')),\n",
       "             ('fc_global.22.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('fc_local.0.weight',\n",
       "              tensor([[[ 0.0830, -0.1198,  0.1774,  0.0663, -0.2503],\n",
       "                       [-0.1881, -0.2552, -0.1255, -0.1883,  0.0249]],\n",
       "              \n",
       "                      [[ 0.1022, -0.1167, -0.0258, -0.0795,  0.1114],\n",
       "                       [ 0.0500, -0.1737, -0.1858,  0.0470,  0.1581]],\n",
       "              \n",
       "                      [[ 0.1513, -0.0130,  0.1708, -0.1277,  0.0656],\n",
       "                       [ 0.0038, -0.1205, -0.1893,  0.1668, -0.0797]],\n",
       "              \n",
       "                      [[-0.0098, -0.0721, -0.2005, -0.1980, -0.0678],\n",
       "                       [-0.0888, -0.1364, -0.0712, -0.2042,  0.0668]],\n",
       "              \n",
       "                      [[ 0.2444, -0.2318, -0.1584, -0.0373, -0.1669],\n",
       "                       [-0.0341,  0.1233,  0.2029, -0.0199, -0.0089]],\n",
       "              \n",
       "                      [[ 0.2089,  0.0709, -0.0700,  0.2184,  0.1194],\n",
       "                       [-0.2269, -0.0652, -0.2284, -0.0449,  0.1867]],\n",
       "              \n",
       "                      [[ 0.0682, -0.1013, -0.0431, -0.2068, -0.0232],\n",
       "                       [ 0.0237, -0.0090, -0.2039,  0.1508,  0.2459]],\n",
       "              \n",
       "                      [[ 0.1899, -0.1308,  0.2359, -0.2528,  0.2085],\n",
       "                       [-0.0698, -0.0936, -0.1461,  0.1061, -0.2140]],\n",
       "              \n",
       "                      [[ 0.1001, -0.0738, -0.2364,  0.0746,  0.2205],\n",
       "                       [ 0.2513, -0.0142,  0.2559, -0.1732,  0.1062]],\n",
       "              \n",
       "                      [[-0.1873, -0.1746,  0.0656,  0.1956,  0.2312],\n",
       "                       [ 0.1936,  0.1118,  0.2017, -0.2181,  0.2087]],\n",
       "              \n",
       "                      [[ 0.0367, -0.1990,  0.1877,  0.0202,  0.2111],\n",
       "                       [-0.1077,  0.1923, -0.1144, -0.1095, -0.1514]],\n",
       "              \n",
       "                      [[ 0.1024,  0.0266,  0.2557, -0.1833, -0.2160],\n",
       "                       [ 0.1484,  0.0170,  0.0820, -0.2334,  0.1463]],\n",
       "              \n",
       "                      [[ 0.2258, -0.0530,  0.1532, -0.0454, -0.2208],\n",
       "                       [-0.0801,  0.2450,  0.1897, -0.1738,  0.0465]],\n",
       "              \n",
       "                      [[ 0.2262,  0.1101, -0.1178,  0.0370,  0.1100],\n",
       "                       [ 0.0401,  0.1529, -0.1866,  0.2547,  0.0614]],\n",
       "              \n",
       "                      [[ 0.2527,  0.1311,  0.0827, -0.0729,  0.2281],\n",
       "                       [-0.2142,  0.1223,  0.0518, -0.1555, -0.0402]],\n",
       "              \n",
       "                      [[-0.0270,  0.0527, -0.0614, -0.0179, -0.1609],\n",
       "                       [-0.1639, -0.2166,  0.0933,  0.0480,  0.0047]]], device='cuda:0')),\n",
       "             ('fc_local.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_local.2.weight',\n",
       "              tensor([[[-0.0389, -0.0631,  0.0383,  0.1134, -0.0165],\n",
       "                       [-0.1360,  0.0777,  0.1506,  0.1926,  0.0438],\n",
       "                       [ 0.1851, -0.1724,  0.1653,  0.1285, -0.1202],\n",
       "                       ...,\n",
       "                       [-0.0601, -0.1782,  0.0649, -0.0995,  0.0513],\n",
       "                       [ 0.1002,  0.1621,  0.1474, -0.0579,  0.1716],\n",
       "                       [ 0.0936, -0.0667, -0.0859, -0.0960,  0.0864]],\n",
       "              \n",
       "                      [[ 0.0839,  0.1420, -0.1779,  0.1423, -0.1726],\n",
       "                       [-0.0982,  0.1158,  0.0766, -0.0795,  0.1362],\n",
       "                       [ 0.0962,  0.1150, -0.0339,  0.1875, -0.1852],\n",
       "                       ...,\n",
       "                       [-0.1383,  0.0242, -0.1630, -0.1585, -0.0142],\n",
       "                       [ 0.0713,  0.0685,  0.1471, -0.0533, -0.0632],\n",
       "                       [ 0.0964,  0.0977, -0.1382,  0.1633,  0.1214]],\n",
       "              \n",
       "                      [[-0.0394, -0.1442,  0.1904, -0.1293, -0.0257],\n",
       "                       [ 0.0518,  0.1175, -0.0713,  0.1506, -0.1471],\n",
       "                       [-0.0445,  0.0627,  0.1588, -0.0338,  0.0021],\n",
       "                       ...,\n",
       "                       [ 0.1290, -0.1003,  0.0385, -0.0369,  0.1696],\n",
       "                       [-0.0454, -0.0139, -0.1502, -0.1281,  0.0957],\n",
       "                       [-0.0917, -0.0924,  0.1131, -0.0667, -0.1528]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.1703, -0.1227,  0.0140, -0.0983, -0.1665],\n",
       "                       [ 0.1103, -0.1139, -0.1591, -0.1434, -0.0674],\n",
       "                       [-0.1261, -0.1846,  0.1030,  0.0839,  0.0569],\n",
       "                       ...,\n",
       "                       [-0.0236, -0.0896, -0.1704,  0.0969, -0.1495],\n",
       "                       [ 0.0818,  0.0694, -0.0757,  0.0143, -0.1415],\n",
       "                       [ 0.0025, -0.1879, -0.1775,  0.1869,  0.1061]],\n",
       "              \n",
       "                      [[-0.1447,  0.1662, -0.1807,  0.1400, -0.1007],\n",
       "                       [-0.1800,  0.0047, -0.0346,  0.1625,  0.1125],\n",
       "                       [-0.1932,  0.0478, -0.0652,  0.1259,  0.1066],\n",
       "                       ...,\n",
       "                       [-0.0234,  0.0582, -0.0000,  0.0317,  0.0173],\n",
       "                       [-0.0354,  0.1285, -0.1371,  0.1038, -0.0830],\n",
       "                       [ 0.0355,  0.1055,  0.1789, -0.1149,  0.0525]],\n",
       "              \n",
       "                      [[-0.1481, -0.0113, -0.0843, -0.1366, -0.1032],\n",
       "                       [-0.1677,  0.0272, -0.1773,  0.1617, -0.1194],\n",
       "                       [ 0.1863, -0.1593,  0.0748, -0.1337,  0.1766],\n",
       "                       ...,\n",
       "                       [-0.0969, -0.0899,  0.1355, -0.0327, -0.0753],\n",
       "                       [ 0.0111, -0.0524, -0.1811,  0.0610,  0.0180],\n",
       "                       [ 0.0844, -0.1593, -0.1487,  0.0250, -0.1770]]], device='cuda:0')),\n",
       "             ('fc_local.2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_local.5.weight',\n",
       "              tensor([[[-0.0088,  0.0120, -0.0653,  0.0224,  0.1191],\n",
       "                       [-0.1086,  0.0458, -0.0541, -0.0649,  0.0839],\n",
       "                       [ 0.1182,  0.0671, -0.0730,  0.0997, -0.0183],\n",
       "                       ...,\n",
       "                       [ 0.1500, -0.0866, -0.1115, -0.0360, -0.0968],\n",
       "                       [-0.1260, -0.0959,  0.1574,  0.0137, -0.1409],\n",
       "                       [ 0.1095,  0.0667, -0.0039, -0.1520, -0.1558]],\n",
       "              \n",
       "                      [[ 0.0543, -0.1009,  0.0002, -0.0196,  0.0575],\n",
       "                       [-0.0897,  0.0435, -0.1418, -0.0020,  0.1096],\n",
       "                       [ 0.0320, -0.1246, -0.1393,  0.0117,  0.0338],\n",
       "                       ...,\n",
       "                       [-0.0769,  0.0912,  0.1205,  0.0789, -0.1217],\n",
       "                       [ 0.1364,  0.0507,  0.0568, -0.1193,  0.0453],\n",
       "                       [-0.0350,  0.1543, -0.0374,  0.0723,  0.0494]],\n",
       "              \n",
       "                      [[ 0.0356, -0.1140,  0.0320,  0.0715,  0.0772],\n",
       "                       [-0.0353,  0.0821,  0.0460,  0.0356, -0.1574],\n",
       "                       [ 0.1087, -0.0076, -0.0530,  0.1142,  0.0006],\n",
       "                       ...,\n",
       "                       [-0.0039,  0.0282, -0.0929, -0.0885,  0.1217],\n",
       "                       [ 0.0512, -0.0378,  0.0674, -0.0805, -0.1197],\n",
       "                       [ 0.0060, -0.0732,  0.0552,  0.0472, -0.0920]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0283, -0.0103,  0.1069,  0.1194, -0.0573],\n",
       "                       [ 0.1285,  0.0924, -0.1084, -0.1044, -0.0726],\n",
       "                       [ 0.0072, -0.0037,  0.0504, -0.1186, -0.0214],\n",
       "                       ...,\n",
       "                       [ 0.1099, -0.0634,  0.0508, -0.1561, -0.1363],\n",
       "                       [ 0.1462,  0.1238, -0.1580, -0.0305, -0.0757],\n",
       "                       [ 0.1530,  0.0186, -0.0545, -0.0504,  0.0215]],\n",
       "              \n",
       "                      [[-0.0918,  0.0711, -0.0079, -0.1272, -0.0795],\n",
       "                       [ 0.0421,  0.0425, -0.0696,  0.0849,  0.1424],\n",
       "                       [ 0.0043,  0.1206, -0.0900, -0.0594,  0.0107],\n",
       "                       ...,\n",
       "                       [-0.0149, -0.0356,  0.1497, -0.1169,  0.0844],\n",
       "                       [-0.0816, -0.0162,  0.0737,  0.1520, -0.1090],\n",
       "                       [-0.0927, -0.1396, -0.0586, -0.0558,  0.0226]],\n",
       "              \n",
       "                      [[-0.0695, -0.1241, -0.1096, -0.0115, -0.1039],\n",
       "                       [-0.0381,  0.0217, -0.1361,  0.0521, -0.0932],\n",
       "                       [-0.1192,  0.0640,  0.0553, -0.1493, -0.1205],\n",
       "                       ...,\n",
       "                       [ 0.0051, -0.0123, -0.0932, -0.0315,  0.1536],\n",
       "                       [ 0.0212, -0.0672, -0.1560,  0.0407, -0.1224],\n",
       "                       [-0.0351,  0.1267, -0.0659,  0.0898, -0.1276]]], device='cuda:0')),\n",
       "             ('fc_local.5.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('fc_local.7.weight',\n",
       "              tensor([[[ 0.0140,  0.0347,  0.0900, -0.0476, -0.1115],\n",
       "                       [-0.0354, -0.0638, -0.0694,  0.0371, -0.0715],\n",
       "                       [ 0.0929, -0.0011, -0.1285,  0.0649, -0.0493],\n",
       "                       ...,\n",
       "                       [ 0.0739, -0.1347, -0.0614,  0.0623,  0.0434],\n",
       "                       [-0.0705, -0.0618,  0.0572, -0.0243,  0.0244],\n",
       "                       [-0.0873,  0.0071, -0.0903,  0.0701, -0.0478]],\n",
       "              \n",
       "                      [[ 0.0816, -0.0907, -0.0429,  0.0536, -0.0107],\n",
       "                       [ 0.1334,  0.1160, -0.0422,  0.0273,  0.0352],\n",
       "                       [ 0.0173,  0.1230,  0.0849, -0.1110, -0.0017],\n",
       "                       ...,\n",
       "                       [-0.0092,  0.0041, -0.1155,  0.0562,  0.0081],\n",
       "                       [-0.1073,  0.1188, -0.0175,  0.1333, -0.0897],\n",
       "                       [ 0.0683,  0.0551,  0.0239, -0.1212,  0.0208]],\n",
       "              \n",
       "                      [[ 0.0878, -0.1337, -0.0051,  0.0770, -0.0851],\n",
       "                       [-0.0637,  0.0469, -0.0465,  0.0772,  0.0252],\n",
       "                       [-0.0446, -0.1017, -0.1215,  0.0876, -0.0263],\n",
       "                       ...,\n",
       "                       [-0.0399,  0.0925, -0.0870, -0.0744, -0.0289],\n",
       "                       [-0.0576, -0.1367,  0.1135, -0.0908, -0.0458],\n",
       "                       [ 0.0724,  0.0974, -0.1243,  0.0750,  0.0753]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0079, -0.0072,  0.1257, -0.0715, -0.1289],\n",
       "                       [ 0.0300, -0.0301,  0.1116,  0.0706, -0.0211],\n",
       "                       [ 0.0554, -0.0871,  0.0884,  0.1234,  0.1047],\n",
       "                       ...,\n",
       "                       [ 0.0387, -0.0606, -0.0287, -0.0290,  0.0658],\n",
       "                       [-0.0832,  0.0039,  0.0227,  0.1007,  0.1195],\n",
       "                       [ 0.0623,  0.0974,  0.1030, -0.0090, -0.0145]],\n",
       "              \n",
       "                      [[ 0.0616, -0.0250,  0.0346,  0.0213,  0.0018],\n",
       "                       [-0.0221, -0.0790,  0.1099, -0.1092,  0.0181],\n",
       "                       [-0.1173, -0.0754, -0.0141, -0.0242, -0.0567],\n",
       "                       ...,\n",
       "                       [ 0.0757,  0.0866,  0.0147, -0.0099, -0.0275],\n",
       "                       [ 0.0604, -0.0973,  0.0395,  0.0879,  0.0837],\n",
       "                       [-0.0627,  0.0138, -0.0326, -0.1213, -0.1025]],\n",
       "              \n",
       "                      [[ 0.1074, -0.0763,  0.1338,  0.0320,  0.0471],\n",
       "                       [-0.0540, -0.0014, -0.0273, -0.1067, -0.0888],\n",
       "                       [ 0.0512, -0.0423,  0.0224,  0.1346,  0.1111],\n",
       "                       ...,\n",
       "                       [ 0.0789,  0.0303, -0.0013,  0.0198, -0.0223],\n",
       "                       [-0.0994, -0.0928,  0.0976,  0.1133,  0.1185],\n",
       "                       [ 0.0657,  0.1271,  0.0710, -0.0323,  0.0512]]], device='cuda:0')),\n",
       "             ('fc_local.7.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('final_layer.0.weight',\n",
       "              tensor([[-0.0218,  0.0239, -0.0220,  ...,  0.0120, -0.0161, -0.0082],\n",
       "                      [-0.0013,  0.0118,  0.0195,  ...,  0.0103,  0.0222,  0.0193],\n",
       "                      [ 0.0054, -0.0177,  0.0164,  ..., -0.0019, -0.0167,  0.0085],\n",
       "                      ...,\n",
       "                      [-0.0116, -0.0165, -0.0127,  ...,  0.0237,  0.0126,  0.0106],\n",
       "                      [ 0.0016,  0.0019, -0.0111,  ...,  0.0202, -0.0011,  0.0058],\n",
       "                      [ 0.0102,  0.0187,  0.0034,  ...,  0.0238,  0.0190, -0.0204]],\n",
       "                     device='cuda:0')),\n",
       "             ('final_layer.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('final_layer.3.weight',\n",
       "              tensor([[ 0.0956, -0.0767, -0.0821,  ...,  0.0427, -0.0600,  0.0037],\n",
       "                      [-0.0830, -0.0341, -0.0379,  ..., -0.0541,  0.1046, -0.0312],\n",
       "                      [-0.0751, -0.1006, -0.1037,  ...,  0.0018,  0.0062, -0.0011],\n",
       "                      ...,\n",
       "                      [ 0.0178,  0.0694, -0.0805,  ..., -0.0278, -0.0332, -0.0715],\n",
       "                      [-0.0079, -0.0187,  0.0722,  ...,  0.0105, -0.0142, -0.1080],\n",
       "                      [ 0.0485, -0.0096,  0.0962,  ...,  0.1068,  0.1081, -0.0776]],\n",
       "                     device='cuda:0')),\n",
       "             ('final_layer.3.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('final_layer.5.weight',\n",
       "              tensor([[ 0.0719,  0.0028,  0.0572,  ...,  0.0568, -0.1060,  0.0999],\n",
       "                      [ 0.0750,  0.0152,  0.0757,  ..., -0.0254, -0.0568,  0.0403],\n",
       "                      [ 0.0058,  0.0088, -0.0078,  ...,  0.0381,  0.0579,  0.0721],\n",
       "                      ...,\n",
       "                      [ 0.0356,  0.1014, -0.0444,  ..., -0.0912,  0.0099, -0.0156],\n",
       "                      [ 0.0635, -0.0443, -0.0081,  ..., -0.0348,  0.0423, -0.0967],\n",
       "                      [-0.0488, -0.0354,  0.0386,  ...,  0.0320, -0.1050, -0.0232]],\n",
       "                     device='cuda:0')),\n",
       "             ('final_layer.5.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('final_layer.7.weight',\n",
       "              tensor([[ 0.0913, -0.1035,  0.1104, -0.1084, -0.0720, -0.1052,  0.1315,  0.0681,\n",
       "                       -0.0098,  0.0083, -0.0339,  0.0246, -0.0209, -0.1178,  0.0152, -0.0322,\n",
       "                       -0.0824,  0.0115,  0.1427, -0.0260, -0.1142,  0.0431,  0.0371, -0.0532,\n",
       "                       -0.0304,  0.1034, -0.1039,  0.0922,  0.1280, -0.0336,  0.0807,  0.0855,\n",
       "                       -0.1357,  0.0285,  0.0339, -0.0503,  0.0136, -0.0471, -0.0465,  0.1066,\n",
       "                        0.0713, -0.1026,  0.1317,  0.0570, -0.1065, -0.0976,  0.0876, -0.0392,\n",
       "                       -0.0813, -0.1168, -0.1380,  0.0822,  0.0698, -0.0879, -0.1036, -0.0907,\n",
       "                        0.1482,  0.1482,  0.0330, -0.1440,  0.0252, -0.0160,  0.0698,  0.0879,\n",
       "                       -0.1097, -0.0466, -0.0548, -0.1468,  0.1215, -0.1157,  0.1022,  0.1326,\n",
       "                        0.1317, -0.0710,  0.0670,  0.0779, -0.1363,  0.0618, -0.0059, -0.1507,\n",
       "                        0.0983,  0.0644,  0.0027,  0.0080,  0.1082,  0.0135, -0.1239, -0.0372,\n",
       "                       -0.1444, -0.1093,  0.0218, -0.0717, -0.0942, -0.0450,  0.1251,  0.0143,\n",
       "                        0.0164,  0.0874,  0.0079,  0.1023,  0.0315, -0.0512, -0.0590,  0.0125,\n",
       "                        0.0956,  0.0293,  0.1036, -0.1238,  0.0139,  0.0041, -0.0541, -0.1186,\n",
       "                        0.0492, -0.0915, -0.1074,  0.0543,  0.1352,  0.1359, -0.1240,  0.1013,\n",
       "                        0.1101, -0.0331,  0.1043,  0.0910,  0.1033,  0.0158,  0.0660,  0.1032,\n",
       "                       -0.1107, -0.0261,  0.0068,  0.0099, -0.1275,  0.0413,  0.0704,  0.0735,\n",
       "                       -0.0425,  0.1436,  0.1385,  0.0812,  0.0322,  0.1517, -0.1058, -0.0438,\n",
       "                        0.1125,  0.0464, -0.1367,  0.1189,  0.0125, -0.1372,  0.0948,  0.0950,\n",
       "                        0.0110,  0.0827, -0.0840,  0.0987,  0.0902,  0.0816, -0.0381,  0.0233,\n",
       "                        0.0201, -0.0470, -0.0782,  0.0289, -0.0807, -0.1416,  0.0179, -0.0394,\n",
       "                       -0.0368, -0.1140, -0.0307,  0.0792, -0.0523,  0.1288,  0.0110,  0.1404,\n",
       "                        0.0123,  0.0746, -0.0915,  0.0723,  0.0072,  0.0302, -0.0815, -0.0968,\n",
       "                        0.0889, -0.1082, -0.0492, -0.1138, -0.0742, -0.0697, -0.0430, -0.1171,\n",
       "                        0.1303, -0.1009, -0.0421,  0.0147, -0.0867,  0.0929, -0.1473,  0.0470,\n",
       "                       -0.0881,  0.1145,  0.0576, -0.0835,  0.0254, -0.0178, -0.0452,  0.1168,\n",
       "                        0.0672,  0.0668, -0.0606, -0.0750,  0.0171,  0.1248,  0.1396,  0.0966,\n",
       "                       -0.0912,  0.1053,  0.0201, -0.0612,  0.1029, -0.0628,  0.0945, -0.1103,\n",
       "                       -0.0048,  0.1316, -0.1349, -0.1397, -0.0265,  0.1147,  0.1338, -0.1155,\n",
       "                        0.0398,  0.1486, -0.0660, -0.0587,  0.1194, -0.0278,  0.0094, -0.0490,\n",
       "                       -0.1237,  0.0948, -0.0298,  0.1181,  0.1300,  0.1488,  0.1276, -0.0111,\n",
       "                       -0.1453, -0.0358,  0.1306, -0.0260, -0.0773, -0.0953, -0.1224,  0.0181]],\n",
       "                     device='cuda:0')),\n",
       "             ('final_layer.7.bias', tensor([0.], device='cuda:0'))])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputvals_cent[savename]={'k':kcount,'fpath':fpath,'aug':aug,'mod':mod,'unqid':fpath+'_'+aug+'_'+mod,'pred_val_final':pred_val_final, 'gt_val_final':gt_val_final}\n",
    "for potkey in ['loss_train_epoch','loss_val_epoch','acc_val_epoch','ap_val_epoch']:\n",
    "    exec('outputvals_cent[\\''+savename+'\\'][\\''+potkey+'\\']='+potkey)\n",
    "pickle.dump(outputvals_cent,open(path.join(foldname,savedicname+'_cent.pickle'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exonet_CV0_101_all_XS',\n",
       " 'exonet_CV1_101_all_XS',\n",
       " 'exonet_CV2_101_all_XS',\n",
       " 'exonet_CV3_101_all_XS',\n",
       " 'exonet_CV4_101_all_XS',\n",
       " 'exonet_CVall_101_all_XS',\n",
       " 'exonet_CV0_101_noise_XS',\n",
       " 'exonet_CV1_101_noise_XS',\n",
       " 'exonet_CV2_101_noise_XS',\n",
       " 'exonet_CV3_101_noise_XS',\n",
       " 'exonet_CV4_101_noise_XS',\n",
       " 'exonet_CVall_101_noise_XS',\n",
       " 'exonet_CV0_101_xshift_XS',\n",
       " 'exonet_CV1_101_xshift_XS',\n",
       " 'exonet_CV2_101_xshift_XS',\n",
       " 'exonet_CV3_101_xshift_XS',\n",
       " 'exonet_CV4_101_xshift_XS',\n",
       " 'exonet_CVall_101_xshift_XS',\n",
       " 'exonet_CV0_101_mirror_XS',\n",
       " 'exonet_CV1_101_mirror_XS',\n",
       " 'exonet_CV2_101_mirror_XS',\n",
       " 'exonet_CV3_101_mirror_XS',\n",
       " 'exonet_CV4_101_mirror_XS',\n",
       " 'exonet_CVall_101_mirror_XS',\n",
       " 'exonet_CV0_101_all_Big',\n",
       " 'exonet_CV1_101_all_Big',\n",
       " 'exonet_CV2_101_all_Big',\n",
       " 'exonet_CV3_101_all_Big',\n",
       " 'exonet_CV4_101_all_Big',\n",
       " 'exonet_CVall_101_all_Big',\n",
       " 'exonet_CV0_101_noise_Big',\n",
       " 'exonet_CV1_101_noise_Big',\n",
       " 'exonet_CV2_101_noise_Big',\n",
       " 'exonet_CV3_101_noise_Big',\n",
       " 'exonet_CV4_101_noise_Big',\n",
       " 'exonet_CVall_101_noise_Big',\n",
       " 'exonet_CV0_101_xshift_Big',\n",
       " 'exonet_CV1_101_xshift_Big',\n",
       " 'exonet_CV2_101_xshift_Big',\n",
       " 'exonet_CV3_101_xshift_Big',\n",
       " 'exonet_CV4_101_xshift_Big',\n",
       " 'exonet_CVall_101_xshift_Big',\n",
       " 'exonet_CV0_101_mirror_Big',\n",
       " 'exonet_CV1_101_mirror_Big',\n",
       " 'exonet_CV2_101_mirror_Big',\n",
       " 'exonet_CV3_101_mirror_Big',\n",
       " 'exonet_CV4_101_mirror_Big',\n",
       " 'exonet_CVall_101_mirror_Big',\n",
       " 'exonet_CV0_201_all_XS',\n",
       " 'exonet_CV1_201_all_XS',\n",
       " 'exonet_CV2_201_all_XS',\n",
       " 'exonet_CV3_201_all_XS',\n",
       " 'exonet_CV4_201_all_XS',\n",
       " 'exonet_CVall_201_all_XS',\n",
       " 'exonet_CV0_201_noise_XS',\n",
       " 'exonet_CV1_201_noise_XS',\n",
       " 'exonet_CV2_201_noise_XS',\n",
       " 'exonet_CV3_201_noise_XS',\n",
       " 'exonet_CV4_201_noise_XS',\n",
       " 'exonet_CVall_201_noise_XS',\n",
       " 'exonet_CV0_201_xshift_XS',\n",
       " 'exonet_CV1_201_xshift_XS',\n",
       " 'exonet_CV2_201_xshift_XS',\n",
       " 'exonet_CV3_201_xshift_XS',\n",
       " 'exonet_CV4_201_xshift_XS',\n",
       " 'exonet_CVall_201_xshift_XS',\n",
       " 'exonet_CV0_201_mirror_XS',\n",
       " 'exonet_CV1_201_mirror_XS',\n",
       " 'exonet_CV2_201_mirror_XS',\n",
       " 'exonet_CV3_201_mirror_XS',\n",
       " 'exonet_CV4_201_mirror_XS',\n",
       " 'exonet_CVall_201_mirror_XS',\n",
       " 'exonet_CV0_201_all_Big',\n",
       " 'exonet_CV1_201_all_Big',\n",
       " 'exonet_CV2_201_all_Big',\n",
       " 'exonet_CV3_201_all_Big',\n",
       " 'exonet_CV4_201_all_Big',\n",
       " 'exonet_CVall_201_all_Big',\n",
       " 'exonet_CV0_201_noise_Big',\n",
       " 'exonet_CV1_201_noise_Big',\n",
       " 'exonet_CV2_201_noise_Big',\n",
       " 'exonet_CV3_201_noise_Big',\n",
       " 'exonet_CV4_201_noise_Big',\n",
       " 'exonet_CVall_201_noise_Big',\n",
       " 'exonet_CV0_201_xshift_Big',\n",
       " 'exonet_CV1_201_xshift_Big',\n",
       " 'exonet_CV2_201_xshift_Big',\n",
       " 'exonet_CV3_201_xshift_Big',\n",
       " 'exonet_CV4_201_xshift_Big',\n",
       " 'exonet_CVall_201_xshift_Big',\n",
       " 'exonet_CV0_201_mirror_Big',\n",
       " 'exonet_CV1_201_mirror_Big',\n",
       " 'exonet_CV2_201_mirror_Big',\n",
       " 'exonet_CV3_201_mirror_Big',\n",
       " 'exonet_CV4_201_mirror_Big',\n",
       " 'exonet_CVall_201_mirror_Big']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in outputvals_contd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['k', 'loss_train_epoch', 'loss_val_epoch', 'acc_val_epoch', 'ap_val_epoch'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'unqid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-66b096f70513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputvals_contd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputvals_contd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputvals_contd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unqid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'unqid'"
     ]
    }
   ],
   "source": [
    "for key in outputvals_contd:\n",
    "    print(outputvals_contd[key].keys())\n",
    "    print(outputvals_contd[key]['unqid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loss_train_epoch)#, loss_val_epoch, acc_val_epoch, ap_val_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputvals['exonet_CV0_101_all_XS']['loss_train_epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputvals['exonet_CV0_101_all_XS']['loss_train_epoch']+outputvals['exonet_CV0_101_all_XS']['loss_train_epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exonet_CV4_201_mirror_Big'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_epoch=outputvals[savename]['loss_train_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputvals_contd['exonet_CV4_201_mirror_Big']['loss_train_epoch']=outputvals['exonet_CV4_201_mirror_Big']['loss_train_epoch']+loss_train_epoch\n",
      "outputvals_contd['exonet_CV4_201_mirror_Big']['loss_val_epoch']=outputvals['exonet_CV4_201_mirror_Big']['loss_val_epoch']+loss_val_epoch\n",
      "outputvals_contd['exonet_CV4_201_mirror_Big']['acc_val_epoch']=outputvals['exonet_CV4_201_mirror_Big']['acc_val_epoch']+acc_val_epoch\n",
      "outputvals_contd['exonet_CV4_201_mirror_Big']['ap_val_epoch']=outputvals['exonet_CV4_201_mirror_Big']['ap_val_epoch']+ap_val_epoch\n"
     ]
    }
   ],
   "source": [
    "outputvals_contd={}\n",
    "outputvals_contd[savename]={'k':5}\n",
    "for potkey in ['loss_train_epoch','loss_val_epoch','acc_val_epoch','ap_val_epoch']:\n",
    "    print('outputvals_contd[\\''+savename+'\\'][\\''+potkey+'\\']=outputvals[\\''+savename+'\\'][\\''+potkey+'\\']+'+potkey)\n",
    "    exec('outputvals_contd[\\''+savename+'\\'][\\''+potkey+'\\']=outputvals[\\''+savename+'\\'][\\''+potkey+'\\']+'+potkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exonet_CV4_201_mirror_Big': {'k': 5,\n",
       "  'loss_train_epoch': [array([0.0095215], dtype=float32),\n",
       "   array([0.00503225], dtype=float32),\n",
       "   array([0.00431049], dtype=float32),\n",
       "   array([0.00384206], dtype=float32),\n",
       "   array([0.00327061], dtype=float32),\n",
       "   array([0.00284683], dtype=float32),\n",
       "   array([0.00240302], dtype=float32),\n",
       "   array([0.00213781], dtype=float32),\n",
       "   array([0.00199263], dtype=float32),\n",
       "   array([0.00175735], dtype=float32),\n",
       "   array([0.00147505], dtype=float32),\n",
       "   array([0.00134845], dtype=float32),\n",
       "   array([0.00122727], dtype=float32),\n",
       "   array([0.00112665], dtype=float32),\n",
       "   array([0.00115904], dtype=float32),\n",
       "   array([0.00138941], dtype=float32),\n",
       "   array([0.00120968], dtype=float32),\n",
       "   array([0.00105451], dtype=float32),\n",
       "   array([0.00106463], dtype=float32),\n",
       "   array([0.000921], dtype=float32),\n",
       "   array([0.00098539], dtype=float32),\n",
       "   array([0.00100998], dtype=float32),\n",
       "   array([0.00056352], dtype=float32),\n",
       "   array([0.00085802], dtype=float32),\n",
       "   array([0.0003999], dtype=float32),\n",
       "   array([0.00045191], dtype=float32),\n",
       "   array([0.00048713], dtype=float32),\n",
       "   array([0.00052725], dtype=float32),\n",
       "   array([0.00065487], dtype=float32),\n",
       "   array([0.00040087], dtype=float32),\n",
       "   array([0.00048624], dtype=float32),\n",
       "   array([0.00085401], dtype=float32),\n",
       "   array([0.00036478], dtype=float32),\n",
       "   array([0.00028083], dtype=float32),\n",
       "   array([0.00040782], dtype=float32),\n",
       "   array([0.00059448], dtype=float32),\n",
       "   array([0.0002393], dtype=float32),\n",
       "   array([0.00028559], dtype=float32),\n",
       "   array([0.00034388], dtype=float32),\n",
       "   array([0.00045985], dtype=float32),\n",
       "   array([0.00033655], dtype=float32),\n",
       "   array([0.00017728], dtype=float32),\n",
       "   array([0.00049376], dtype=float32),\n",
       "   array([0.00043621], dtype=float32),\n",
       "   array([0.00020561], dtype=float32),\n",
       "   array([0.0003959], dtype=float32),\n",
       "   array([0.00030522], dtype=float32),\n",
       "   array([0.00033478], dtype=float32),\n",
       "   array([0.00021355], dtype=float32),\n",
       "   array([0.00033244], dtype=float32),\n",
       "   array([0.00018328], dtype=float32),\n",
       "   array([0.00032121], dtype=float32),\n",
       "   array([0.00034473], dtype=float32),\n",
       "   array([0.00021301], dtype=float32),\n",
       "   array([9.708891e-05], dtype=float32),\n",
       "   array([9.477453e-05], dtype=float32),\n",
       "   array([0.0004677], dtype=float32),\n",
       "   array([0.00057659], dtype=float32),\n",
       "   array([0.00044137], dtype=float32),\n",
       "   array([0.00016033], dtype=float32),\n",
       "   array([8.9425936e-05], dtype=float32),\n",
       "   array([8.883979e-05], dtype=float32),\n",
       "   array([9.01739e-05], dtype=float32),\n",
       "   array([0.00046173], dtype=float32),\n",
       "   array([0.00045723], dtype=float32),\n",
       "   array([0.00037591], dtype=float32),\n",
       "   array([0.00017954], dtype=float32),\n",
       "   array([0.00025155], dtype=float32),\n",
       "   array([0.00019139], dtype=float32),\n",
       "   array([0.00028827], dtype=float32),\n",
       "   array([0.00027893], dtype=float32),\n",
       "   array([9.866117e-05], dtype=float32),\n",
       "   array([9.039249e-05], dtype=float32),\n",
       "   array([8.998805e-05], dtype=float32),\n",
       "   array([9.728803e-05], dtype=float32),\n",
       "   array([0.00068237], dtype=float32),\n",
       "   array([0.00030328], dtype=float32),\n",
       "   array([0.00013114], dtype=float32),\n",
       "   array([9.4955634e-05], dtype=float32),\n",
       "   array([9.68523e-05], dtype=float32),\n",
       "   array([8.717062e-05], dtype=float32),\n",
       "   array([0.00046846], dtype=float32),\n",
       "   array([0.00032892], dtype=float32),\n",
       "   array([0.00032195], dtype=float32),\n",
       "   array([0.00018322], dtype=float32),\n",
       "   array([8.537915e-05], dtype=float32),\n",
       "   array([0.00027233], dtype=float32),\n",
       "   array([0.0001858], dtype=float32),\n",
       "   array([0.00019575], dtype=float32),\n",
       "   array([9.174855e-05], dtype=float32),\n",
       "   array([8.792445e-05], dtype=float32),\n",
       "   array([0.00041992], dtype=float32),\n",
       "   array([0.00057888], dtype=float32),\n",
       "   array([0.00019234], dtype=float32),\n",
       "   array([9.1626614e-05], dtype=float32),\n",
       "   array([9.128757e-05], dtype=float32),\n",
       "   array([8.813628e-05], dtype=float32),\n",
       "   array([8.315936e-05], dtype=float32),\n",
       "   array([9.4790514e-05], dtype=float32),\n",
       "   array([0.00035602], dtype=float32),\n",
       "   array([0.00057983], dtype=float32),\n",
       "   array([0.00022865], dtype=float32),\n",
       "   array([0.00020132], dtype=float32),\n",
       "   array([0.00020445], dtype=float32),\n",
       "   array([0.0001803], dtype=float32),\n",
       "   array([0.00034422], dtype=float32),\n",
       "   array([0.00013704], dtype=float32),\n",
       "   array([8.827323e-05], dtype=float32),\n",
       "   array([9.0870744e-05], dtype=float32),\n",
       "   array([9.221383e-05], dtype=float32),\n",
       "   array([9.2023925e-05], dtype=float32),\n",
       "   array([8.6806576e-05], dtype=float32),\n",
       "   array([0.0001011], dtype=float32),\n",
       "   array([0.00082137], dtype=float32),\n",
       "   array([0.00025996], dtype=float32),\n",
       "   array([9.729927e-05], dtype=float32),\n",
       "   array([0.00027509], dtype=float32),\n",
       "   array([0.00044693], dtype=float32),\n",
       "   array([0.00071697], dtype=float32),\n",
       "   array([0.00061436], dtype=float32),\n",
       "   array([0.00021165], dtype=float32),\n",
       "   array([0.00015292], dtype=float32),\n",
       "   array([9.012702e-05], dtype=float32),\n",
       "   array([8.4250314e-05], dtype=float32),\n",
       "   array([0.00013682], dtype=float32),\n",
       "   array([0.00053448], dtype=float32),\n",
       "   array([9.14275e-05], dtype=float32),\n",
       "   array([8.722103e-05], dtype=float32),\n",
       "   array([8.678796e-05], dtype=float32),\n",
       "   array([8.6893444e-05], dtype=float32),\n",
       "   array([8.873664e-05], dtype=float32),\n",
       "   array([8.818633e-05], dtype=float32),\n",
       "   array([9.970537e-05], dtype=float32),\n",
       "   array([0.00108782], dtype=float32),\n",
       "   array([0.00013303], dtype=float32),\n",
       "   array([9.368278e-05], dtype=float32),\n",
       "   array([8.54575e-05], dtype=float32),\n",
       "   array([0.00021623], dtype=float32),\n",
       "   array([0.00042335], dtype=float32),\n",
       "   array([0.00013393], dtype=float32),\n",
       "   array([0.00026407], dtype=float32),\n",
       "   array([0.00015766], dtype=float32),\n",
       "   array([0.00054196], dtype=float32),\n",
       "   array([0.00020154], dtype=float32),\n",
       "   array([0.00018819], dtype=float32),\n",
       "   array([0.00020972], dtype=float32),\n",
       "   array([0.00019792], dtype=float32),\n",
       "   array([0.00018945], dtype=float32),\n",
       "   array([0.00030168], dtype=float32),\n",
       "   array([0.0006822], dtype=float32),\n",
       "   array([0.00028231], dtype=float32),\n",
       "   array([0.00021806], dtype=float32),\n",
       "   array([0.0002106], dtype=float32),\n",
       "   array([0.00021102], dtype=float32),\n",
       "   array([0.00020909], dtype=float32),\n",
       "   array([0.00021357], dtype=float32),\n",
       "   array([0.00021286], dtype=float32),\n",
       "   array([0.00092626], dtype=float32),\n",
       "   array([0.00020507], dtype=float32),\n",
       "   array([0.00023236], dtype=float32),\n",
       "   array([8.88784e-05], dtype=float32),\n",
       "   array([8.098816e-05], dtype=float32),\n",
       "   array([0.0001616], dtype=float32),\n",
       "   array([0.00021391], dtype=float32),\n",
       "   array([0.00047705], dtype=float32),\n",
       "   array([0.00019604], dtype=float32),\n",
       "   array([0.00015238], dtype=float32),\n",
       "   array([0.0001494], dtype=float32),\n",
       "   array([9.814515e-05], dtype=float32),\n",
       "   array([0.0001683], dtype=float32),\n",
       "   array([0.00013691], dtype=float32),\n",
       "   array([0.00037584], dtype=float32),\n",
       "   array([0.00019721], dtype=float32),\n",
       "   array([0.00019116], dtype=float32),\n",
       "   array([0.00018385], dtype=float32),\n",
       "   array([0.00017423], dtype=float32),\n",
       "   array([0.0003197], dtype=float32),\n",
       "   array([0.00037452], dtype=float32),\n",
       "   array([0.00019211], dtype=float32),\n",
       "   array([0.00013899], dtype=float32),\n",
       "   array([0.00013113], dtype=float32),\n",
       "   array([0.00027175], dtype=float32),\n",
       "   array([0.00041329], dtype=float32),\n",
       "   array([0.00028389], dtype=float32),\n",
       "   array([0.00039731], dtype=float32),\n",
       "   array([0.0001488], dtype=float32),\n",
       "   array([0.00013185], dtype=float32),\n",
       "   array([0.00012832], dtype=float32),\n",
       "   array([0.00013777], dtype=float32),\n",
       "   array([0.000132], dtype=float32),\n",
       "   array([0.00026434], dtype=float32),\n",
       "   array([0.00019312], dtype=float32),\n",
       "   array([0.00030071], dtype=float32),\n",
       "   array([0.00011159], dtype=float32),\n",
       "   array([0.00016236], dtype=float32),\n",
       "   array([0.00034385], dtype=float32),\n",
       "   array([0.00015939], dtype=float32),\n",
       "   array([0.00043147], dtype=float32),\n",
       "   array([0.00022379], dtype=float32),\n",
       "   array([0.00021313], dtype=float32),\n",
       "   array([0.0001346], dtype=float32),\n",
       "   array([0.00012559], dtype=float32),\n",
       "   array([0.00012747], dtype=float32),\n",
       "   array([0.0001316], dtype=float32),\n",
       "   array([0.00058235], dtype=float32),\n",
       "   array([0.00010995], dtype=float32),\n",
       "   array([0.00010508], dtype=float32),\n",
       "   array([0.00011202], dtype=float32),\n",
       "   array([0.00034103], dtype=float32),\n",
       "   array([0.00017161], dtype=float32),\n",
       "   array([0.00017965], dtype=float32),\n",
       "   array([8.787254e-05], dtype=float32),\n",
       "   array([0.00018265], dtype=float32),\n",
       "   array([0.00015705], dtype=float32),\n",
       "   array([9.101959e-05], dtype=float32),\n",
       "   array([8.180408e-05], dtype=float32),\n",
       "   array([8.7868524e-05], dtype=float32),\n",
       "   array([0.0002909], dtype=float32),\n",
       "   array([9.5109746e-05], dtype=float32),\n",
       "   array([0.00033085], dtype=float32),\n",
       "   array([0.00013833], dtype=float32),\n",
       "   array([0.00010466], dtype=float32),\n",
       "   array([0.00010322], dtype=float32),\n",
       "   array([0.00011741], dtype=float32),\n",
       "   array([0.00025429], dtype=float32),\n",
       "   array([0.00046185], dtype=float32),\n",
       "   array([8.670516e-05], dtype=float32),\n",
       "   array([8.604908e-05], dtype=float32),\n",
       "   array([7.9898404e-05], dtype=float32),\n",
       "   array([0.0001062], dtype=float32),\n",
       "   array([0.00029357], dtype=float32),\n",
       "   array([0.00022836], dtype=float32),\n",
       "   array([0.00044564], dtype=float32),\n",
       "   array([0.00018744], dtype=float32),\n",
       "   array([0.00012488], dtype=float32),\n",
       "   array([0.00010196], dtype=float32),\n",
       "   array([0.00012189], dtype=float32),\n",
       "   array([0.00053553], dtype=float32),\n",
       "   array([0.00043625], dtype=float32),\n",
       "   array([0.00018743], dtype=float32),\n",
       "   array([0.00016409], dtype=float32),\n",
       "   array([0.00017254], dtype=float32),\n",
       "   array([0.00015181], dtype=float32),\n",
       "   array([0.00018897], dtype=float32),\n",
       "   array([0.00015805], dtype=float32),\n",
       "   array([0.0004576], dtype=float32),\n",
       "   array([0.00017706], dtype=float32),\n",
       "   array([0.00070505], dtype=float32),\n",
       "   array([0.00080209], dtype=float32),\n",
       "   array([0.00089532], dtype=float32),\n",
       "   array([0.00023597], dtype=float32),\n",
       "   array([0.00014424], dtype=float32),\n",
       "   array([0.00040504], dtype=float32),\n",
       "   array([0.00040548], dtype=float32),\n",
       "   array([0.00045441], dtype=float32),\n",
       "   array([0.000386], dtype=float32),\n",
       "   array([0.00029828], dtype=float32),\n",
       "   array([0.00025503], dtype=float32),\n",
       "   array([0.00036545], dtype=float32),\n",
       "   array([0.00018193], dtype=float32),\n",
       "   array([0.00019201], dtype=float32),\n",
       "   array([0.00024342], dtype=float32),\n",
       "   array([0.00030325], dtype=float32),\n",
       "   array([0.00012884], dtype=float32),\n",
       "   array([0.00016786], dtype=float32),\n",
       "   array([0.00014124], dtype=float32),\n",
       "   array([0.00024988], dtype=float32),\n",
       "   array([0.00018299], dtype=float32),\n",
       "   array([0.0001173], dtype=float32),\n",
       "   array([0.00012178], dtype=float32),\n",
       "   array([0.00027691], dtype=float32),\n",
       "   array([0.00027307], dtype=float32),\n",
       "   array([0.00038264], dtype=float32),\n",
       "   array([0.00017524], dtype=float32),\n",
       "   array([8.769739e-05], dtype=float32),\n",
       "   array([0.00010119], dtype=float32),\n",
       "   array([0.00010163], dtype=float32),\n",
       "   array([0.00057921], dtype=float32),\n",
       "   array([9.5935655e-05], dtype=float32),\n",
       "   array([9.861149e-05], dtype=float32),\n",
       "   array([7.83083e-05], dtype=float32),\n",
       "   array([8.081281e-05], dtype=float32),\n",
       "   array([7.6994336e-05], dtype=float32),\n",
       "   array([8.785709e-05], dtype=float32),\n",
       "   array([8.539379e-05], dtype=float32),\n",
       "   array([0.00015632], dtype=float32),\n",
       "   array([0.00145919], dtype=float32),\n",
       "   array([0.00083791], dtype=float32),\n",
       "   array([0.00059577], dtype=float32),\n",
       "   array([0.00041528], dtype=float32),\n",
       "   array([0.00041756], dtype=float32),\n",
       "   array([0.00042003], dtype=float32),\n",
       "   array([0.00039715], dtype=float32),\n",
       "   array([0.00039285], dtype=float32),\n",
       "   array([0.00039505], dtype=float32),\n",
       "   array([0.00040475], dtype=float32),\n",
       "   array([0.00057399], dtype=float32),\n",
       "   array([0.00029214], dtype=float32),\n",
       "   array([0.0004517], dtype=float32),\n",
       "   array([0.00023036], dtype=float32),\n",
       "   array([0.0095215], dtype=float32),\n",
       "   array([0.00503225], dtype=float32),\n",
       "   array([0.00431049], dtype=float32),\n",
       "   array([0.00384206], dtype=float32),\n",
       "   array([0.00327061], dtype=float32),\n",
       "   array([0.00284683], dtype=float32),\n",
       "   array([0.00240302], dtype=float32),\n",
       "   array([0.00213781], dtype=float32),\n",
       "   array([0.00199263], dtype=float32),\n",
       "   array([0.00175735], dtype=float32),\n",
       "   array([0.00147505], dtype=float32),\n",
       "   array([0.00134845], dtype=float32),\n",
       "   array([0.00122727], dtype=float32),\n",
       "   array([0.00112665], dtype=float32),\n",
       "   array([0.00115904], dtype=float32),\n",
       "   array([0.00138941], dtype=float32),\n",
       "   array([0.00120968], dtype=float32),\n",
       "   array([0.00105451], dtype=float32),\n",
       "   array([0.00106463], dtype=float32),\n",
       "   array([0.000921], dtype=float32),\n",
       "   array([0.00098539], dtype=float32),\n",
       "   array([0.00100998], dtype=float32),\n",
       "   array([0.00056352], dtype=float32),\n",
       "   array([0.00085802], dtype=float32),\n",
       "   array([0.0003999], dtype=float32),\n",
       "   array([0.00045191], dtype=float32),\n",
       "   array([0.00048713], dtype=float32),\n",
       "   array([0.00052725], dtype=float32),\n",
       "   array([0.00065487], dtype=float32),\n",
       "   array([0.00040087], dtype=float32),\n",
       "   array([0.00048624], dtype=float32),\n",
       "   array([0.00085401], dtype=float32),\n",
       "   array([0.00036478], dtype=float32),\n",
       "   array([0.00028083], dtype=float32),\n",
       "   array([0.00040782], dtype=float32),\n",
       "   array([0.00059448], dtype=float32),\n",
       "   array([0.0002393], dtype=float32),\n",
       "   array([0.00028559], dtype=float32),\n",
       "   array([0.00034388], dtype=float32),\n",
       "   array([0.00045985], dtype=float32),\n",
       "   array([0.00033655], dtype=float32),\n",
       "   array([0.00017728], dtype=float32),\n",
       "   array([0.00049376], dtype=float32),\n",
       "   array([0.00043621], dtype=float32),\n",
       "   array([0.00020561], dtype=float32),\n",
       "   array([0.0003959], dtype=float32),\n",
       "   array([0.00030522], dtype=float32),\n",
       "   array([0.00033478], dtype=float32),\n",
       "   array([0.00021355], dtype=float32),\n",
       "   array([0.00033244], dtype=float32),\n",
       "   array([0.00018328], dtype=float32),\n",
       "   array([0.00032121], dtype=float32),\n",
       "   array([0.00034473], dtype=float32),\n",
       "   array([0.00021301], dtype=float32),\n",
       "   array([9.708891e-05], dtype=float32),\n",
       "   array([9.477453e-05], dtype=float32),\n",
       "   array([0.0004677], dtype=float32),\n",
       "   array([0.00057659], dtype=float32),\n",
       "   array([0.00044137], dtype=float32),\n",
       "   array([0.00016033], dtype=float32),\n",
       "   array([8.9425936e-05], dtype=float32),\n",
       "   array([8.883979e-05], dtype=float32),\n",
       "   array([9.01739e-05], dtype=float32),\n",
       "   array([0.00046173], dtype=float32),\n",
       "   array([0.00045723], dtype=float32),\n",
       "   array([0.00037591], dtype=float32),\n",
       "   array([0.00017954], dtype=float32),\n",
       "   array([0.00025155], dtype=float32),\n",
       "   array([0.00019139], dtype=float32),\n",
       "   array([0.00028827], dtype=float32),\n",
       "   array([0.00027893], dtype=float32),\n",
       "   array([9.866117e-05], dtype=float32),\n",
       "   array([9.039249e-05], dtype=float32),\n",
       "   array([8.998805e-05], dtype=float32),\n",
       "   array([9.728803e-05], dtype=float32),\n",
       "   array([0.00068237], dtype=float32),\n",
       "   array([0.00030328], dtype=float32),\n",
       "   array([0.00013114], dtype=float32),\n",
       "   array([9.4955634e-05], dtype=float32),\n",
       "   array([9.68523e-05], dtype=float32),\n",
       "   array([8.717062e-05], dtype=float32),\n",
       "   array([0.00046846], dtype=float32),\n",
       "   array([0.00032892], dtype=float32),\n",
       "   array([0.00032195], dtype=float32),\n",
       "   array([0.00018322], dtype=float32),\n",
       "   array([8.537915e-05], dtype=float32),\n",
       "   array([0.00027233], dtype=float32),\n",
       "   array([0.0001858], dtype=float32),\n",
       "   array([0.00019575], dtype=float32),\n",
       "   array([9.174855e-05], dtype=float32),\n",
       "   array([8.792445e-05], dtype=float32),\n",
       "   array([0.00041992], dtype=float32),\n",
       "   array([0.00057888], dtype=float32),\n",
       "   array([0.00019234], dtype=float32),\n",
       "   array([9.1626614e-05], dtype=float32),\n",
       "   array([9.128757e-05], dtype=float32),\n",
       "   array([8.813628e-05], dtype=float32),\n",
       "   array([8.315936e-05], dtype=float32),\n",
       "   array([9.4790514e-05], dtype=float32),\n",
       "   array([0.00035602], dtype=float32),\n",
       "   array([0.00057983], dtype=float32),\n",
       "   array([0.00022865], dtype=float32),\n",
       "   array([0.00020132], dtype=float32),\n",
       "   array([0.00020445], dtype=float32),\n",
       "   array([0.0001803], dtype=float32),\n",
       "   array([0.00034422], dtype=float32),\n",
       "   array([0.00013704], dtype=float32),\n",
       "   array([8.827323e-05], dtype=float32),\n",
       "   array([9.0870744e-05], dtype=float32),\n",
       "   array([9.221383e-05], dtype=float32),\n",
       "   array([9.2023925e-05], dtype=float32),\n",
       "   array([8.6806576e-05], dtype=float32),\n",
       "   array([0.0001011], dtype=float32),\n",
       "   array([0.00082137], dtype=float32),\n",
       "   array([0.00025996], dtype=float32),\n",
       "   array([9.729927e-05], dtype=float32),\n",
       "   array([0.00027509], dtype=float32),\n",
       "   array([0.00044693], dtype=float32),\n",
       "   array([0.00071697], dtype=float32),\n",
       "   array([0.00061436], dtype=float32),\n",
       "   array([0.00021165], dtype=float32),\n",
       "   array([0.00015292], dtype=float32),\n",
       "   array([9.012702e-05], dtype=float32),\n",
       "   array([8.4250314e-05], dtype=float32),\n",
       "   array([0.00013682], dtype=float32),\n",
       "   array([0.00053448], dtype=float32),\n",
       "   array([9.14275e-05], dtype=float32),\n",
       "   array([8.722103e-05], dtype=float32),\n",
       "   array([8.678796e-05], dtype=float32),\n",
       "   array([8.6893444e-05], dtype=float32),\n",
       "   array([8.873664e-05], dtype=float32),\n",
       "   array([8.818633e-05], dtype=float32),\n",
       "   array([9.970537e-05], dtype=float32),\n",
       "   array([0.00108782], dtype=float32),\n",
       "   array([0.00013303], dtype=float32),\n",
       "   array([9.368278e-05], dtype=float32),\n",
       "   array([8.54575e-05], dtype=float32),\n",
       "   array([0.00021623], dtype=float32),\n",
       "   array([0.00042335], dtype=float32),\n",
       "   array([0.00013393], dtype=float32),\n",
       "   array([0.00026407], dtype=float32),\n",
       "   array([0.00015766], dtype=float32),\n",
       "   array([0.00054196], dtype=float32),\n",
       "   array([0.00020154], dtype=float32),\n",
       "   array([0.00018819], dtype=float32),\n",
       "   array([0.00020972], dtype=float32),\n",
       "   array([0.00019792], dtype=float32),\n",
       "   array([0.00018945], dtype=float32),\n",
       "   array([0.00030168], dtype=float32),\n",
       "   array([0.0006822], dtype=float32),\n",
       "   array([0.00028231], dtype=float32),\n",
       "   array([0.00021806], dtype=float32),\n",
       "   array([0.0002106], dtype=float32),\n",
       "   array([0.00021102], dtype=float32),\n",
       "   array([0.00020909], dtype=float32),\n",
       "   array([0.00021357], dtype=float32),\n",
       "   array([0.00021286], dtype=float32),\n",
       "   array([0.00092626], dtype=float32),\n",
       "   array([0.00020507], dtype=float32),\n",
       "   array([0.00023236], dtype=float32),\n",
       "   array([8.88784e-05], dtype=float32),\n",
       "   array([8.098816e-05], dtype=float32),\n",
       "   array([0.0001616], dtype=float32),\n",
       "   array([0.00021391], dtype=float32),\n",
       "   array([0.00047705], dtype=float32),\n",
       "   array([0.00019604], dtype=float32),\n",
       "   array([0.00015238], dtype=float32),\n",
       "   array([0.0001494], dtype=float32),\n",
       "   array([9.814515e-05], dtype=float32),\n",
       "   array([0.0001683], dtype=float32),\n",
       "   array([0.00013691], dtype=float32),\n",
       "   array([0.00037584], dtype=float32),\n",
       "   array([0.00019721], dtype=float32),\n",
       "   array([0.00019116], dtype=float32),\n",
       "   array([0.00018385], dtype=float32),\n",
       "   array([0.00017423], dtype=float32),\n",
       "   array([0.0003197], dtype=float32),\n",
       "   array([0.00037452], dtype=float32),\n",
       "   array([0.00019211], dtype=float32),\n",
       "   array([0.00013899], dtype=float32),\n",
       "   array([0.00013113], dtype=float32),\n",
       "   array([0.00027175], dtype=float32),\n",
       "   array([0.00041329], dtype=float32),\n",
       "   array([0.00028389], dtype=float32),\n",
       "   array([0.00039731], dtype=float32),\n",
       "   array([0.0001488], dtype=float32),\n",
       "   array([0.00013185], dtype=float32),\n",
       "   array([0.00012832], dtype=float32),\n",
       "   array([0.00013777], dtype=float32),\n",
       "   array([0.000132], dtype=float32),\n",
       "   array([0.00026434], dtype=float32),\n",
       "   array([0.00019312], dtype=float32),\n",
       "   array([0.00030071], dtype=float32),\n",
       "   array([0.00011159], dtype=float32),\n",
       "   array([0.00016236], dtype=float32),\n",
       "   array([0.00034385], dtype=float32),\n",
       "   array([0.00015939], dtype=float32),\n",
       "   array([0.00043147], dtype=float32),\n",
       "   array([0.00022379], dtype=float32),\n",
       "   array([0.00021313], dtype=float32),\n",
       "   array([0.0001346], dtype=float32),\n",
       "   array([0.00012559], dtype=float32),\n",
       "   array([0.00012747], dtype=float32),\n",
       "   array([0.0001316], dtype=float32),\n",
       "   array([0.00058235], dtype=float32),\n",
       "   array([0.00010995], dtype=float32),\n",
       "   array([0.00010508], dtype=float32),\n",
       "   array([0.00011202], dtype=float32),\n",
       "   array([0.00034103], dtype=float32),\n",
       "   array([0.00017161], dtype=float32),\n",
       "   array([0.00017965], dtype=float32),\n",
       "   array([8.787254e-05], dtype=float32),\n",
       "   array([0.00018265], dtype=float32),\n",
       "   array([0.00015705], dtype=float32),\n",
       "   array([9.101959e-05], dtype=float32),\n",
       "   array([8.180408e-05], dtype=float32),\n",
       "   array([8.7868524e-05], dtype=float32),\n",
       "   array([0.0002909], dtype=float32),\n",
       "   array([9.5109746e-05], dtype=float32),\n",
       "   array([0.00033085], dtype=float32),\n",
       "   array([0.00013833], dtype=float32),\n",
       "   array([0.00010466], dtype=float32),\n",
       "   array([0.00010322], dtype=float32),\n",
       "   array([0.00011741], dtype=float32),\n",
       "   array([0.00025429], dtype=float32),\n",
       "   array([0.00046185], dtype=float32),\n",
       "   array([8.670516e-05], dtype=float32),\n",
       "   array([8.604908e-05], dtype=float32),\n",
       "   array([7.9898404e-05], dtype=float32),\n",
       "   array([0.0001062], dtype=float32),\n",
       "   array([0.00029357], dtype=float32),\n",
       "   array([0.00022836], dtype=float32),\n",
       "   array([0.00044564], dtype=float32),\n",
       "   array([0.00018744], dtype=float32),\n",
       "   array([0.00012488], dtype=float32),\n",
       "   array([0.00010196], dtype=float32),\n",
       "   array([0.00012189], dtype=float32),\n",
       "   array([0.00053553], dtype=float32),\n",
       "   array([0.00043625], dtype=float32),\n",
       "   array([0.00018743], dtype=float32),\n",
       "   array([0.00016409], dtype=float32),\n",
       "   array([0.00017254], dtype=float32),\n",
       "   array([0.00015181], dtype=float32),\n",
       "   array([0.00018897], dtype=float32),\n",
       "   array([0.00015805], dtype=float32),\n",
       "   array([0.0004576], dtype=float32),\n",
       "   array([0.00017706], dtype=float32),\n",
       "   array([0.00070505], dtype=float32),\n",
       "   array([0.00080209], dtype=float32),\n",
       "   array([0.00089532], dtype=float32),\n",
       "   array([0.00023597], dtype=float32),\n",
       "   array([0.00014424], dtype=float32),\n",
       "   array([0.00040504], dtype=float32),\n",
       "   array([0.00040548], dtype=float32),\n",
       "   array([0.00045441], dtype=float32),\n",
       "   array([0.000386], dtype=float32),\n",
       "   array([0.00029828], dtype=float32),\n",
       "   array([0.00025503], dtype=float32),\n",
       "   array([0.00036545], dtype=float32),\n",
       "   array([0.00018193], dtype=float32),\n",
       "   array([0.00019201], dtype=float32),\n",
       "   array([0.00024342], dtype=float32),\n",
       "   array([0.00030325], dtype=float32),\n",
       "   array([0.00012884], dtype=float32),\n",
       "   array([0.00016786], dtype=float32),\n",
       "   array([0.00014124], dtype=float32),\n",
       "   array([0.00024988], dtype=float32),\n",
       "   array([0.00018299], dtype=float32),\n",
       "   array([0.0001173], dtype=float32),\n",
       "   array([0.00012178], dtype=float32),\n",
       "   array([0.00027691], dtype=float32),\n",
       "   array([0.00027307], dtype=float32),\n",
       "   array([0.00038264], dtype=float32),\n",
       "   array([0.00017524], dtype=float32),\n",
       "   array([8.769739e-05], dtype=float32),\n",
       "   array([0.00010119], dtype=float32),\n",
       "   array([0.00010163], dtype=float32),\n",
       "   array([0.00057921], dtype=float32),\n",
       "   array([9.5935655e-05], dtype=float32),\n",
       "   array([9.861149e-05], dtype=float32),\n",
       "   array([7.83083e-05], dtype=float32),\n",
       "   array([8.081281e-05], dtype=float32),\n",
       "   array([7.6994336e-05], dtype=float32),\n",
       "   array([8.785709e-05], dtype=float32),\n",
       "   array([8.539379e-05], dtype=float32),\n",
       "   array([0.00015632], dtype=float32),\n",
       "   array([0.00145919], dtype=float32),\n",
       "   array([0.00083791], dtype=float32),\n",
       "   array([0.00059577], dtype=float32),\n",
       "   array([0.00041528], dtype=float32),\n",
       "   array([0.00041756], dtype=float32),\n",
       "   array([0.00042003], dtype=float32),\n",
       "   array([0.00039715], dtype=float32),\n",
       "   array([0.00039285], dtype=float32),\n",
       "   array([0.00039505], dtype=float32),\n",
       "   array([0.00040475], dtype=float32),\n",
       "   array([0.00057399], dtype=float32),\n",
       "   array([0.00029214], dtype=float32),\n",
       "   array([0.0004517], dtype=float32),\n",
       "   array([0.00023036], dtype=float32)],\n",
       "  'loss_val_epoch': [0.002734755072132115,\n",
       "   0.003182653933935905,\n",
       "   0.003726666469257677,\n",
       "   0.0030195384792411737,\n",
       "   0.0029941155006652904,\n",
       "   0.0030918853236035497,\n",
       "   0.0035044485843186207,\n",
       "   0.003636318608411619,\n",
       "   0.004542028954532033,\n",
       "   0.0032258454869715718,\n",
       "   0.0049101031061033465,\n",
       "   0.004698990212413642,\n",
       "   0.004128274796232495,\n",
       "   0.0041146191951164715,\n",
       "   0.004678217177679766,\n",
       "   0.0046138494942485255,\n",
       "   0.006094589409936684,\n",
       "   0.005666186255099366,\n",
       "   0.006119016050786711,\n",
       "   0.006769106740249063,\n",
       "   0.0053759687801056555,\n",
       "   0.005578490513611428,\n",
       "   0.0070121557704279705,\n",
       "   0.004848177902863102,\n",
       "   0.007151609215550656,\n",
       "   0.006971391618274788,\n",
       "   0.0058490059525636556,\n",
       "   0.006137615090578714,\n",
       "   0.005596286672172149,\n",
       "   0.007202201398973799,\n",
       "   0.006715353786646516,\n",
       "   0.005183485084824979,\n",
       "   0.006418457649133861,\n",
       "   0.00787428470821947,\n",
       "   0.007891761994573355,\n",
       "   0.005483375016759364,\n",
       "   0.00853436783019337,\n",
       "   0.006634829543614268,\n",
       "   0.00820768086366841,\n",
       "   0.00756376202484378,\n",
       "   0.00674220966731909,\n",
       "   0.009707348253843732,\n",
       "   0.006532075457530641,\n",
       "   0.006815890846590807,\n",
       "   0.009348797605068031,\n",
       "   0.006810309295713143,\n",
       "   0.01001290065001853,\n",
       "   0.007599365375605596,\n",
       "   0.010247493498214091,\n",
       "   0.008235676887179726,\n",
       "   0.010269949163105144,\n",
       "   0.007665532645781988,\n",
       "   0.006982637840120939,\n",
       "   0.010208475603544247,\n",
       "   0.01273032886109387,\n",
       "   0.013247416739384732,\n",
       "   0.008027514021138266,\n",
       "   0.007541544686584825,\n",
       "   0.005684187271583048,\n",
       "   0.009893465170840061,\n",
       "   0.010641963795442357,\n",
       "   0.011399284918888444,\n",
       "   0.012012695006500373,\n",
       "   0.0064323843100150945,\n",
       "   0.008150224284780011,\n",
       "   0.008437672683827628,\n",
       "   0.009396826306044067,\n",
       "   0.009357331022534,\n",
       "   0.007239370055516806,\n",
       "   0.007963891844008357,\n",
       "   0.008529129793319379,\n",
       "   0.010499896113126838,\n",
       "   0.010888030158475795,\n",
       "   0.01119549358116353,\n",
       "   0.012005568010394195,\n",
       "   0.005143387364550442,\n",
       "   0.008615592341601412,\n",
       "   0.0089646194875815,\n",
       "   0.010135788187482299,\n",
       "   0.009432364926115685,\n",
       "   0.01103660768861252,\n",
       "   0.006368789349198111,\n",
       "   0.007936308478647064,\n",
       "   0.008262324599837226,\n",
       "   0.008401981063942812,\n",
       "   0.010373484430927317,\n",
       "   0.00818856692065891,\n",
       "   0.009356204120063709,\n",
       "   0.010094431896628996,\n",
       "   0.011516340809626052,\n",
       "   0.012555332382443418,\n",
       "   0.0085526527790227,\n",
       "   0.007082170838791112,\n",
       "   0.009911053529173722,\n",
       "   0.010605211474929498,\n",
       "   0.011449793982220763,\n",
       "   0.011166881880519291,\n",
       "   0.011326908651485067,\n",
       "   0.011539835843440422,\n",
       "   0.00810788599993334,\n",
       "   0.0090216641438959,\n",
       "   0.010789816783194463,\n",
       "   0.010868269644140324,\n",
       "   0.009441922265408447,\n",
       "   0.009869268397497846,\n",
       "   0.011418342958120043,\n",
       "   0.010488631501880062,\n",
       "   0.011520768241720497,\n",
       "   0.011378716006501503,\n",
       "   0.01089415530003593,\n",
       "   0.011875298967519599,\n",
       "   0.013360162890191893,\n",
       "   0.013721580814313024,\n",
       "   0.008555292761376407,\n",
       "   0.009856999724976952,\n",
       "   0.010635335519881125,\n",
       "   0.00801937413684567,\n",
       "   0.010050345147570909,\n",
       "   0.011617684060893738,\n",
       "   0.008049797561239469,\n",
       "   0.010470341404018556,\n",
       "   0.010184968800513478,\n",
       "   0.013024003175617046,\n",
       "   0.01482838293414662,\n",
       "   0.00938878646009087,\n",
       "   0.007813437859431869,\n",
       "   0.009388441474935148,\n",
       "   0.010569229302514808,\n",
       "   0.010367776360973354,\n",
       "   0.011837508013659447,\n",
       "   0.011024825967420172,\n",
       "   0.011923139361033236,\n",
       "   0.012700983791152714,\n",
       "   0.008120311350147558,\n",
       "   0.00834502909020756,\n",
       "   0.009495753925645669,\n",
       "   0.01096978943365401,\n",
       "   0.0059778512880418735,\n",
       "   0.0076814085831294595,\n",
       "   0.01203686677025774,\n",
       "   0.009480122199538496,\n",
       "   0.011799468435102847,\n",
       "   0.00843511655714077,\n",
       "   0.009732904371680139,\n",
       "   0.010268731784783778,\n",
       "   0.011170817212253017,\n",
       "   0.010969083280670013,\n",
       "   0.012528383819191694,\n",
       "   0.011656963085052456,\n",
       "   0.007834851948671529,\n",
       "   0.009418048409941203,\n",
       "   0.011021852769127143,\n",
       "   0.011542772262932204,\n",
       "   0.01213772895480508,\n",
       "   0.01286470094703589,\n",
       "   0.012462309967170477,\n",
       "   0.013088332241306974,\n",
       "   0.0058249779939191755,\n",
       "   0.008477223871115594,\n",
       "   0.00775672936136089,\n",
       "   0.008967245493990732,\n",
       "   0.010029315764592322,\n",
       "   0.00848605887475181,\n",
       "   0.012849584859722239,\n",
       "   0.008724976909974345,\n",
       "   0.010414452337639423,\n",
       "   0.008939796003465988,\n",
       "   0.009447583994072833,\n",
       "   0.01157385034998825,\n",
       "   0.007378923538978892,\n",
       "   0.011015897545628782,\n",
       "   0.009482082509645108,\n",
       "   0.010967529008529198,\n",
       "   0.011926434006049189,\n",
       "   0.01365965561107278,\n",
       "   0.012463495715722769,\n",
       "   0.006018196529648821,\n",
       "   0.007715845310315633,\n",
       "   0.007872640695803606,\n",
       "   0.009643511288514525,\n",
       "   0.009827856202866643,\n",
       "   0.008422496279279927,\n",
       "   0.009557239989998991,\n",
       "   0.009541855417436216,\n",
       "   0.0076433329613476335,\n",
       "   0.009704547915916576,\n",
       "   0.012071104891917166,\n",
       "   0.012689277834290939,\n",
       "   0.012824079496631556,\n",
       "   0.013990289678547863,\n",
       "   0.008533240927723076,\n",
       "   0.010370527415306835,\n",
       "   0.00663458091891782,\n",
       "   0.010604964321385099,\n",
       "   0.010271098868432213,\n",
       "   0.01039985850930352,\n",
       "   0.010839934520081484,\n",
       "   0.011916816349522377,\n",
       "   0.008136914036607724,\n",
       "   0.00786359825972834,\n",
       "   0.010045608773545964,\n",
       "   0.010523511781409308,\n",
       "   0.010270893642721239,\n",
       "   0.010481191150387235,\n",
       "   0.009362060040798694,\n",
       "   0.010289429422975092,\n",
       "   0.011257382005602523,\n",
       "   0.011679261336674159,\n",
       "   0.008194077856238175,\n",
       "   0.008327588582654925,\n",
       "   0.009005496918442532,\n",
       "   0.009258538013344973,\n",
       "   0.006783083420299997,\n",
       "   0.007946149750285474,\n",
       "   0.008836413735457014,\n",
       "   0.010013079394992604,\n",
       "   0.009957561794506091,\n",
       "   0.009974427817183206,\n",
       "   0.01158677662747549,\n",
       "   0.009503257536676645,\n",
       "   0.010421841198810499,\n",
       "   0.011060775038913737,\n",
       "   0.011672199806834205,\n",
       "   0.010345017638760003,\n",
       "   0.007577005335597793,\n",
       "   0.009480715809390668,\n",
       "   0.010451283364786956,\n",
       "   0.011408394292381984,\n",
       "   0.01174699391263174,\n",
       "   0.011739318176810915,\n",
       "   0.010827330424893191,\n",
       "   0.016354076475238323,\n",
       "   0.008659796782823498,\n",
       "   0.011297388514449962,\n",
       "   0.014330877560793182,\n",
       "   0.01618630041084922,\n",
       "   0.015909770902316635,\n",
       "   0.011702979985600128,\n",
       "   0.010096285548211983,\n",
       "   0.010991861863436038,\n",
       "   0.011530252758986786,\n",
       "   0.010762137792949919,\n",
       "   0.012471919532361014,\n",
       "   0.011645126931234257,\n",
       "   0.012945873232546158,\n",
       "   0.006157598484449758,\n",
       "   0.008700790434696478,\n",
       "   0.011179099062718436,\n",
       "   0.011829516715723899,\n",
       "   0.00883940826545477,\n",
       "   0.00984880908593863,\n",
       "   0.010237066708059786,\n",
       "   0.007505921475270334,\n",
       "   0.008417157468490512,\n",
       "   0.009861684608680145,\n",
       "   0.010329926561031202,\n",
       "   0.010593312797149175,\n",
       "   0.011173188709357601,\n",
       "   0.005717847230486827,\n",
       "   0.008983990882199748,\n",
       "   0.009552040938654324,\n",
       "   0.009474666432161107,\n",
       "   0.00907815932491963,\n",
       "   0.009914766716947897,\n",
       "   0.011965260650952535,\n",
       "   0.01236277034831608,\n",
       "   0.00721125486868944,\n",
       "   0.011595321078581834,\n",
       "   0.010719380965345025,\n",
       "   0.011488919270990256,\n",
       "   0.014211284668345365,\n",
       "   0.009889499680489314,\n",
       "   0.011550596585110076,\n",
       "   0.010047640434527001,\n",
       "   0.011366701107709232,\n",
       "   0.010901487521853081,\n",
       "   0.01114179211788273,\n",
       "   0.009528082491944206,\n",
       "   0.011794081076295782,\n",
       "   0.011277151346850294,\n",
       "   0.011651760355827665,\n",
       "   0.012006644158118763,\n",
       "   0.011605269744319894,\n",
       "   0.011317201254683206,\n",
       "   0.01099788328877665,\n",
       "   0.008948994381658552,\n",
       "   0.006688150714090143,\n",
       "   0.00864408929238575,\n",
       "   0.009475180599802579,\n",
       "   0.010317249643816435,\n",
       "   0.01112448769189472,\n",
       "   0.01138137952728801,\n",
       "   0.011261337197888922,\n",
       "   0.013007744003160552,\n",
       "   0.011895591303321141,\n",
       "   0.011856995629289643,\n",
       "   0.009274007912726647,\n",
       "   0.012217834655079104,\n",
       "   0.011938821841886132,\n",
       "   0.009757632230912403,\n",
       "   array([0.0029218 , 0.00616213, 0.00583233, 0.00354666, 0.00354847,\n",
       "          0.00308683, 0.00374921, 0.00290442, 0.00384016, 0.0039901 ,\n",
       "          0.00417109, 0.00267948, 0.00366226, 0.00311714, 0.00371534,\n",
       "          0.00478862, 0.0030034 , 0.00287774, 0.00341871, 0.00379473,\n",
       "          0.00366237, 0.0037907 , 0.00405239, 0.00429348, 0.00457741,\n",
       "          0.00431171, 0.00427888, 0.00461835, 0.00475794, 0.00529491,\n",
       "          0.0047128 , 0.00497899, 0.00540267, 0.00462159, 0.00500903,\n",
       "          0.00597105, 0.00610203, 0.0059403 , 0.00536288, 0.00613262,\n",
       "          0.00705973, 0.00820656, 0.00612483, 0.0070269 , 0.00716836,\n",
       "          0.00694838, 0.00655843, 0.00647763, 0.00734252, 0.00683237,\n",
       "          0.00775012, 0.00704905, 0.00684297, 0.00868706, 0.00707749,\n",
       "          0.00819429, 0.00827046, 0.00726737, 0.00881009, 0.0083673 ,\n",
       "          0.00779333, 0.00912127, 0.01024361, 0.00861324, 0.00845616,\n",
       "          0.00894415, 0.00861568, 0.0090526 , 0.0094964 , 0.00939017,\n",
       "          0.00939908, 0.01050525, 0.01259025, 0.01011595, 0.01057128,\n",
       "          0.01066657, 0.00893276]),\n",
       "   array([0.00466688, 0.00302209, 0.00276075, 0.00210321, 0.00236342,\n",
       "          0.00346954, 0.0029224 , 0.00309074, 0.00318039, 0.00383531,\n",
       "          0.00441351, 0.00346673, 0.0045137 , 0.00447556, 0.00559169,\n",
       "          0.00630142, 0.00475375, 0.00436227, 0.00576712, 0.00736518,\n",
       "          0.00643943, 0.00673642, 0.0074146 , 0.00526561, 0.00694696,\n",
       "          0.00547946, 0.00862731, 0.0070161 , 0.00781006, 0.0070609 ,\n",
       "          0.00779706, 0.00772406, 0.00719461, 0.00587088, 0.00703087,\n",
       "          0.00647148, 0.00643503, 0.00806901, 0.00701033, 0.00785689,\n",
       "          0.0084656 , 0.00706048, 0.00864923, 0.01047219, 0.00784787,\n",
       "          0.0094803 , 0.00597315, 0.00691371, 0.00786199, 0.00989437,\n",
       "          0.01063943, 0.01072957, 0.0064276 , 0.00678483, 0.00789039,\n",
       "          0.00926209, 0.00746654, 0.00781163, 0.008951  , 0.0061258 ,\n",
       "          0.00994376, 0.00737168, 0.00770935, 0.00692752, 0.00951198,\n",
       "          0.01004647, 0.01030346, 0.0073251 , 0.0078923 , 0.00845866,\n",
       "          0.00927   , 0.01129005, 0.00655966, 0.00832576, 0.00964271,\n",
       "          0.00588142, 0.00810823, 0.00948011, 0.01038205, 0.01069284,\n",
       "          0.01121517, 0.01163478, 0.00750773, 0.00897344, 0.00762627,\n",
       "          0.00856222, 0.01073404, 0.01190527, 0.00722374, 0.00827096,\n",
       "          0.00853533, 0.00954955, 0.01351599, 0.01343279, 0.0106764 ,\n",
       "          0.00994478, 0.01057226, 0.00664034, 0.00763299, 0.008172  ,\n",
       "          0.00837788, 0.00942156, 0.00926332, 0.00556104, 0.00637086,\n",
       "          0.00836075, 0.0129054 , 0.01167348, 0.01293993, 0.01566702,\n",
       "          0.00774422, 0.01080083, 0.01253826, 0.01354967, 0.00799391,\n",
       "          0.00905969, 0.00927217, 0.00880302, 0.00842685, 0.00787355,\n",
       "          0.00835601, 0.00812977, 0.0089749 , 0.01209707, 0.01209829,\n",
       "          0.01415928, 0.00968628, 0.00824486, 0.00864232, 0.0090482 ,\n",
       "          0.00902591, 0.00856017, 0.00814866, 0.00929809, 0.01036838,\n",
       "          0.0134602 , 0.01211331, 0.01200819, 0.01185864, 0.01434998,\n",
       "          0.0137268 , 0.01059959, 0.01060846, 0.00927336, 0.00742903,\n",
       "          0.01014477, 0.01216196, 0.00901168, 0.00971238, 0.00981918,\n",
       "          0.01039778, 0.01051432, 0.0155159 , 0.00986145, 0.00864552,\n",
       "          0.00875193, 0.01099249, 0.01170458, 0.01148892, 0.01188734,\n",
       "          0.01152055, 0.01146999, 0.01305975, 0.01273103, 0.0092817 ,\n",
       "          0.00672041, 0.00685106, 0.00921417, 0.01081132, 0.00786155,\n",
       "          0.00940643, 0.00725701, 0.00924056, 0.01056106, 0.01170498,\n",
       "          0.01199471, 0.01218937, 0.00988943, 0.00777984, 0.01169332,\n",
       "          0.00792862, 0.01003137, 0.01012398, 0.01075731, 0.01205439,\n",
       "          0.01191067, 0.01234832, 0.01229164, 0.0132239 , 0.01235206,\n",
       "          0.01385977, 0.00628493, 0.0088772 , 0.00926906, 0.01093384,\n",
       "          0.01150481, 0.01179529, 0.01221964, 0.01235357, 0.01256784,\n",
       "          0.0127217 , 0.00517849, 0.00812457, 0.00894194, 0.00798527,\n",
       "          0.00865377, 0.00904756, 0.01127995, 0.00775644, 0.01116467,\n",
       "          0.01251551, 0.01315167, 0.0129289 , 0.01301378, 0.01314764,\n",
       "          0.01041341, 0.00749764, 0.00908406, 0.00940296, 0.00946572,\n",
       "          0.00935084, 0.01103239, 0.01114265, 0.01129429, 0.01044326,\n",
       "          0.00824536, 0.00798062, 0.00994627, 0.00944808, 0.01031663,\n",
       "          0.01047955, 0.01046675, 0.01160127, 0.01105333, 0.01282718,\n",
       "          0.01274228, 0.00898267, 0.01007372, 0.01029782, 0.00624238,\n",
       "          0.00868786, 0.00969391, 0.00999026, 0.01129282, 0.01038421,\n",
       "          0.01163145, 0.01156538, 0.01158152, 0.01313416, 0.01290221,\n",
       "          0.01472095, 0.00883082, 0.00799502, 0.01075966, 0.01439529,\n",
       "          0.00884655, 0.00900154, 0.00958336, 0.00890992, 0.01014867,\n",
       "          0.01102544, 0.01037122, 0.01058504, 0.01121826, 0.01132884,\n",
       "          0.00879669, 0.0089262 , 0.0098517 , 0.00975381, 0.00980646,\n",
       "          0.01066344, 0.01049297, 0.01088272, 0.00780078, 0.00878599,\n",
       "          0.00790711, 0.0093365 , 0.00982286, 0.01031876, 0.01053551,\n",
       "          0.01088678, 0.01113503, 0.01085619, 0.01156529, 0.0111815 ,\n",
       "          0.01203214, 0.01291793, 0.00903338, 0.00839572, 0.00807548,\n",
       "          0.01002518, 0.01032923, 0.0105323 , 0.01035282, 0.01089675,\n",
       "          0.01127051, 0.01187309, 0.01259541, 0.00641768, 0.00989378]),\n",
       "   array([0.00285407, 0.00294759, 0.00271549, 0.00254583, 0.00218534,\n",
       "          0.00226702, 0.00277954, 0.00273728, 0.0025313 , 0.00248921,\n",
       "          0.00274239, 0.00290325, 0.0028154 , 0.002703  , 0.00292641,\n",
       "          0.0032405 , 0.00339597, 0.00412566, 0.00513675, 0.00350533,\n",
       "          0.00378911, 0.00344314, 0.00394506, 0.00389771, 0.00403152,\n",
       "          0.00445316, 0.00457491, 0.0046872 , 0.0048364 , 0.00545172,\n",
       "          0.00528166, 0.00503834, 0.00567401, 0.00529189, 0.00552421,\n",
       "          0.00606756, 0.00561473, 0.00656533, 0.00667697, 0.00581247,\n",
       "          0.00607868, 0.00569638, 0.00726961, 0.00678718, 0.00626353,\n",
       "          0.00703485, 0.00648918, 0.00753554, 0.00752722, 0.00557457,\n",
       "          0.00691286, 0.00697269, 0.00691191, 0.00580654, 0.00784304,\n",
       "          0.00679683, 0.00733384, 0.0066597 , 0.00860835, 0.00741242,\n",
       "          0.00764479, 0.00943085, 0.00766137, 0.0082103 , 0.00791454,\n",
       "          0.00724073, 0.00734718, 0.00812315, 0.00899278, 0.00687083,\n",
       "          0.00884887, 0.00849062, 0.00747354, 0.00910479, 0.00919785,\n",
       "          0.00710054, 0.00771254]),\n",
       "   array([0.00408091, 0.00253957, 0.00452637, 0.00219508, 0.00212626,\n",
       "          0.00223217, 0.00285806, 0.00239316, 0.00288186, 0.00301193,\n",
       "          0.00343144, 0.00411142, 0.00364136, 0.00479073, 0.00386373,\n",
       "          0.00418397, 0.00484834, 0.00453013, 0.00532631, 0.00440627,\n",
       "          0.00498875, 0.00561596, 0.00423273, 0.0059729 , 0.00562778,\n",
       "          0.00588967, 0.00594345, 0.00826601, 0.00592485, 0.00796759,\n",
       "          0.00502852, 0.00602841, 0.00689314, 0.00700478, 0.00578388,\n",
       "          0.00714978, 0.0062376 , 0.00622306, 0.00631628, 0.00796225,\n",
       "          0.00858163, 0.00664499, 0.00797624, 0.006017  , 0.00716218,\n",
       "          0.00567394, 0.00950529, 0.00534034, 0.00777006, 0.00976952,\n",
       "          0.00809087, 0.00729289, 0.00696138, 0.00834264, 0.01042605,\n",
       "          0.01017828, 0.01213811, 0.01439711, 0.01402397, 0.01461898,\n",
       "          0.0042567 , 0.0078598 , 0.01102827, 0.00622762, 0.00972254,\n",
       "          0.0086453 , 0.01144642, 0.01225321, 0.01220739, 0.01239181,\n",
       "          0.01249808, 0.00624026, 0.00976725, 0.00880263, 0.01131805,\n",
       "          0.01267124, 0.01248375]),\n",
       "   array([0.00273476, 0.00318265, 0.00372667, 0.00301954, 0.00299412,\n",
       "          0.00309189, 0.00350445, 0.00363632, 0.00454203, 0.00322585,\n",
       "          0.0049101 , 0.00469899, 0.00412827, 0.00411462, 0.00467822,\n",
       "          0.00461385, 0.00609459, 0.00566619, 0.00611902, 0.00676911,\n",
       "          0.00537597, 0.00557849, 0.00701216, 0.00484818, 0.00715161,\n",
       "          0.00697139, 0.00584901, 0.00613762, 0.00559629, 0.0072022 ,\n",
       "          0.00671535, 0.00518349, 0.00641846, 0.00787428, 0.00789176,\n",
       "          0.00548338, 0.00853437, 0.00663483, 0.00820768, 0.00756376,\n",
       "          0.00674221, 0.00970735, 0.00653208, 0.00681589, 0.0093488 ,\n",
       "          0.00681031, 0.0100129 , 0.00759937, 0.01024749, 0.00823568,\n",
       "          0.01026995, 0.00766553, 0.00698264, 0.01020848, 0.01273033,\n",
       "          0.01324742, 0.00802751, 0.00754154, 0.00568419, 0.00989347,\n",
       "          0.01064196, 0.01139928, 0.0120127 , 0.00643238, 0.00815022,\n",
       "          0.00843767, 0.00939683, 0.00935733, 0.00723937, 0.00796389,\n",
       "          0.00852913, 0.0104999 , 0.01088803, 0.01119549, 0.01200557,\n",
       "          0.00514339, 0.00861559, 0.00896462, 0.01013579, 0.00943236,\n",
       "          0.01103661, 0.00636879, 0.00793631, 0.00826232, 0.00840198,\n",
       "          0.01037348, 0.00818857, 0.0093562 , 0.01009443, 0.01151634,\n",
       "          0.01255533, 0.00855265, 0.00708217, 0.00991105, 0.01060521,\n",
       "          0.01144979, 0.01116688, 0.01132691, 0.01153984, 0.00810789,\n",
       "          0.00902166, 0.01078982, 0.01086827, 0.00944192, 0.00986927,\n",
       "          0.01141834, 0.01048863, 0.01152077, 0.01137872, 0.01089416,\n",
       "          0.0118753 , 0.01336016, 0.01372158, 0.00855529, 0.009857  ,\n",
       "          0.01063534, 0.00801937, 0.01005035, 0.01161768, 0.0080498 ,\n",
       "          0.01047034, 0.01018497, 0.013024  , 0.01482838, 0.00938879,\n",
       "          0.00781344, 0.00938844, 0.01056923, 0.01036778, 0.01183751,\n",
       "          0.01102483, 0.01192314, 0.01270098, 0.00812031, 0.00834503,\n",
       "          0.00949575, 0.01096979, 0.00597785, 0.00768141, 0.01203687,\n",
       "          0.00948012, 0.01179947, 0.00843512, 0.0097329 , 0.01026873,\n",
       "          0.01117082, 0.01096908, 0.01252838, 0.01165696, 0.00783485,\n",
       "          0.00941805, 0.01102185, 0.01154277, 0.01213773, 0.0128647 ,\n",
       "          0.01246231, 0.01308833, 0.00582498, 0.00847722, 0.00775673,\n",
       "          0.00896725, 0.01002932, 0.00848606, 0.01284958, 0.00872498,\n",
       "          0.01041445, 0.0089398 , 0.00944758, 0.01157385, 0.00737892,\n",
       "          0.0110159 , 0.00948208, 0.01096753, 0.01192643, 0.01365966,\n",
       "          0.0124635 , 0.0060182 , 0.00771585, 0.00787264, 0.00964351,\n",
       "          0.00982786, 0.0084225 , 0.00955724, 0.00954186, 0.00764333,\n",
       "          0.00970455, 0.0120711 , 0.01268928, 0.01282408, 0.01399029,\n",
       "          0.00853324, 0.01037053, 0.00663458, 0.01060496, 0.0102711 ,\n",
       "          0.01039986, 0.01083993, 0.01191682, 0.00813691, 0.0078636 ,\n",
       "          0.01004561, 0.01052351, 0.01027089, 0.01048119, 0.00936206,\n",
       "          0.01028943, 0.01125738, 0.01167926, 0.00819408, 0.00832759,\n",
       "          0.0090055 , 0.00925854, 0.00678308, 0.00794615, 0.00883641,\n",
       "          0.01001308, 0.00995756, 0.00997443, 0.01158678, 0.00950326,\n",
       "          0.01042184, 0.01106078, 0.0116722 , 0.01034502, 0.00757701,\n",
       "          0.00948072, 0.01045128, 0.01140839, 0.01174699, 0.01173932,\n",
       "          0.01082733, 0.01635408, 0.0086598 , 0.01129739, 0.01433088,\n",
       "          0.0161863 , 0.01590977, 0.01170298, 0.01009629, 0.01099186,\n",
       "          0.01153025, 0.01076214, 0.01247192, 0.01164513, 0.01294587,\n",
       "          0.0061576 , 0.00870079, 0.0111791 , 0.01182952, 0.00883941,\n",
       "          0.00984881, 0.01023707, 0.00750592, 0.00841716, 0.00986168,\n",
       "          0.01032993, 0.01059331, 0.01117319, 0.00571785, 0.00898399,\n",
       "          0.00955204, 0.00947467, 0.00907816, 0.00991477, 0.01196526,\n",
       "          0.01236277, 0.00721125, 0.01159532, 0.01071938, 0.01148892,\n",
       "          0.01421128, 0.0098895 , 0.0115506 , 0.01004764, 0.0113667 ,\n",
       "          0.01090149, 0.01114179, 0.00952808, 0.01179408, 0.01127715,\n",
       "          0.01165176, 0.01200664, 0.01160527, 0.0113172 , 0.01099788,\n",
       "          0.00894899, 0.00668815, 0.00864409, 0.00947518, 0.01031725,\n",
       "          0.01112449, 0.01138138, 0.01126134, 0.01300774, 0.01189559,\n",
       "          0.011857  , 0.00927401, 0.01221783, 0.01193882, 0.00975763])],\n",
       "  'acc_val_epoch': [0.9365599691477053,\n",
       "   0.9309679907443116,\n",
       "   0.9193983802545315,\n",
       "   0.9319321249517933,\n",
       "   0.940416505977632,\n",
       "   0.938102583879676,\n",
       "   0.9442730428075589,\n",
       "   0.9452371770150405,\n",
       "   0.9296182028538372,\n",
       "   0.9444658696490551,\n",
       "   0.9423447743925955,\n",
       "   0.9417662938681064,\n",
       "   0.9496721943694563,\n",
       "   0.9489008870034709,\n",
       "   0.9477439259544929,\n",
       "   0.9425376012340918,\n",
       "   0.9479367527959892,\n",
       "   0.9444658696490551,\n",
       "   0.9496721943694563,\n",
       "   0.9452371770150405,\n",
       "   0.9508291554184343,\n",
       "   0.9510219822599306,\n",
       "   0.9483224064789819,\n",
       "   0.9492865406864636,\n",
       "   0.9483224064789819,\n",
       "   0.9502506748939452,\n",
       "   0.9479367527959892,\n",
       "   0.9496721943694563,\n",
       "   0.9510219822599306,\n",
       "   0.9462013112225222,\n",
       "   0.9498650212109525,\n",
       "   0.9479367527959892,\n",
       "   0.9436945622830698,\n",
       "   0.9477439259544929,\n",
       "   0.9485152333204782,\n",
       "   0.9481295796374856,\n",
       "   0.9500578480524489,\n",
       "   0.9456228306980332,\n",
       "   0.9452371770150405,\n",
       "   0.9471654454300038,\n",
       "   0.9421519475510991,\n",
       "   0.9490937138449672,\n",
       "   0.9454300038565369,\n",
       "   0.9446586964905515,\n",
       "   0.9494793675279599,\n",
       "   0.9436945622830698,\n",
       "   0.9467797917470112,\n",
       "   0.9483224064789819,\n",
       "   0.9465869649055149,\n",
       "   0.9458156575395295,\n",
       "   0.9521789433089086,\n",
       "   0.9496721943694563,\n",
       "   0.9483224064789819,\n",
       "   0.9477439259544929,\n",
       "   0.951214809101427,\n",
       "   0.9500578480524489,\n",
       "   0.9440802159660625,\n",
       "   0.9516004627844196,\n",
       "   0.9475510991129965,\n",
       "   0.9527574238333976,\n",
       "   0.9519861164674123,\n",
       "   0.9525645969919013,\n",
       "   0.9533359043578866,\n",
       "   0.9388738912456613,\n",
       "   0.9446586964905515,\n",
       "   0.9477439259544929,\n",
       "   0.9516004627844196,\n",
       "   0.9506363285769379,\n",
       "   0.9448515233320478,\n",
       "   0.9425376012340918,\n",
       "   0.9529502506748939,\n",
       "   0.9500578480524489,\n",
       "   0.9506363285769379,\n",
       "   0.9502506748939452,\n",
       "   0.9519861164674123,\n",
       "   0.9423447743925955,\n",
       "   0.9502506748939452,\n",
       "   0.9498650212109525,\n",
       "   0.9517932896259159,\n",
       "   0.9531430775163903,\n",
       "   0.9543000385653683,\n",
       "   0.9479367527959892,\n",
       "   0.9504435017354416,\n",
       "   0.9506363285769379,\n",
       "   0.9519861164674123,\n",
       "   0.9514076359429232,\n",
       "   0.9496721943694563,\n",
       "   0.9483224064789819,\n",
       "   0.9479367527959892,\n",
       "   0.9504435017354416,\n",
       "   0.951214809101427,\n",
       "   0.9458156575395295,\n",
       "   0.9481295796374856,\n",
       "   0.9500578480524489,\n",
       "   0.9519861164674123,\n",
       "   0.9510219822599306,\n",
       "   0.9496721943694563,\n",
       "   0.9514076359429232,\n",
       "   0.951214809101427,\n",
       "   0.9490937138449672,\n",
       "   0.9510219822599306,\n",
       "   0.9450443501735442,\n",
       "   0.9473582722715002,\n",
       "   0.9492865406864636,\n",
       "   0.9504435017354416,\n",
       "   0.9500578480524489,\n",
       "   0.9479367527959892,\n",
       "   0.9494793675279599,\n",
       "   0.9498650212109525,\n",
       "   0.9502506748939452,\n",
       "   0.9508291554184343,\n",
       "   0.9492865406864636,\n",
       "   0.9379097570381797,\n",
       "   0.9440802159660625,\n",
       "   0.9508291554184343,\n",
       "   0.9500578480524489,\n",
       "   0.9500578480524489,\n",
       "   0.9361743154647126,\n",
       "   0.9490937138449672,\n",
       "   0.9496721943694563,\n",
       "   0.9489008870034709,\n",
       "   0.9498650212109525,\n",
       "   0.9494793675279599,\n",
       "   0.9496721943694563,\n",
       "   0.9463941380640185,\n",
       "   0.951214809101427,\n",
       "   0.9510219822599306,\n",
       "   0.9508291554184343,\n",
       "   0.951214809101427,\n",
       "   0.9519861164674123,\n",
       "   0.9502506748939452,\n",
       "   0.9527574238333976,\n",
       "   0.9508291554184343,\n",
       "   0.9473582722715002,\n",
       "   0.9517932896259159,\n",
       "   0.9516004627844196,\n",
       "   0.9517932896259159,\n",
       "   0.9519861164674123,\n",
       "   0.9471654454300038,\n",
       "   0.9506363285769379,\n",
       "   0.9481295796374856,\n",
       "   0.9498650212109525,\n",
       "   0.9496721943694563,\n",
       "   0.9502506748939452,\n",
       "   0.9494793675279599,\n",
       "   0.9498650212109525,\n",
       "   0.9490937138449672,\n",
       "   0.951214809101427,\n",
       "   0.9409949865021211,\n",
       "   0.9502506748939452,\n",
       "   0.9490937138449672,\n",
       "   0.951214809101427,\n",
       "   0.9517932896259159,\n",
       "   0.9510219822599306,\n",
       "   0.951214809101427,\n",
       "   0.9490937138449672,\n",
       "   0.9508291554184343,\n",
       "   0.9408021596606247,\n",
       "   0.9454300038565369,\n",
       "   0.9525645969919013,\n",
       "   0.9483224064789819,\n",
       "   0.9492865406864636,\n",
       "   0.9481295796374856,\n",
       "   0.9489008870034709,\n",
       "   0.9463941380640185,\n",
       "   0.9494793675279599,\n",
       "   0.9519861164674123,\n",
       "   0.951214809101427,\n",
       "   0.9519861164674123,\n",
       "   0.9496721943694563,\n",
       "   0.9525645969919013,\n",
       "   0.9500578480524489,\n",
       "   0.9506363285769379,\n",
       "   0.9496721943694563,\n",
       "   0.9510219822599306,\n",
       "   0.9502506748939452,\n",
       "   0.9500578480524489,\n",
       "   0.9506363285769379,\n",
       "   0.9533359043578866,\n",
       "   0.9541072117238719,\n",
       "   0.9550713459313537,\n",
       "   0.9489008870034709,\n",
       "   0.9473582722715002,\n",
       "   0.9492865406864636,\n",
       "   0.9483224064789819,\n",
       "   0.9498650212109525,\n",
       "   0.9500578480524489,\n",
       "   0.9516004627844196,\n",
       "   0.9514076359429232,\n",
       "   0.952371770150405,\n",
       "   0.9435017354415735,\n",
       "   0.9487080601619745,\n",
       "   0.9492865406864636,\n",
       "   0.9527574238333976,\n",
       "   0.9514076359429232,\n",
       "   0.9458156575395295,\n",
       "   0.9481295796374856,\n",
       "   0.9492865406864636,\n",
       "   0.9435017354415735,\n",
       "   0.9504435017354416,\n",
       "   0.9489008870034709,\n",
       "   0.9490937138449672,\n",
       "   0.9502506748939452,\n",
       "   0.9496721943694563,\n",
       "   0.9510219822599306,\n",
       "   0.9517932896259159,\n",
       "   0.9533359043578866,\n",
       "   0.9543000385653683,\n",
       "   0.951214809101427,\n",
       "   0.9500578480524489,\n",
       "   0.953528731199383,\n",
       "   0.9544928654068646,\n",
       "   0.9458156575395295,\n",
       "   0.9500578480524489,\n",
       "   0.9533359043578866,\n",
       "   0.9541072117238719,\n",
       "   0.951214809101427,\n",
       "   0.9506363285769379,\n",
       "   0.9525645969919013,\n",
       "   0.9508291554184343,\n",
       "   0.954685692248361,\n",
       "   0.9539143848823757,\n",
       "   0.9550713459313537,\n",
       "   0.9548785190898573,\n",
       "   0.9519861164674123,\n",
       "   0.9516004627844196,\n",
       "   0.9521789433089086,\n",
       "   0.952371770150405,\n",
       "   0.9516004627844196,\n",
       "   0.951214809101427,\n",
       "   0.9548785190898573,\n",
       "   0.9402236791361358,\n",
       "   0.9517932896259159,\n",
       "   0.9531430775163903,\n",
       "   0.9529502506748939,\n",
       "   0.953528731199383,\n",
       "   0.9510219822599306,\n",
       "   0.9500578480524489,\n",
       "   0.9527574238333976,\n",
       "   0.9521789433089086,\n",
       "   0.952371770150405,\n",
       "   0.951214809101427,\n",
       "   0.9502506748939452,\n",
       "   0.9506363285769379,\n",
       "   0.9444658696490551,\n",
       "   0.9506363285769379,\n",
       "   0.9494793675279599,\n",
       "   0.9517932896259159,\n",
       "   0.9504435017354416,\n",
       "   0.9537215580408793,\n",
       "   0.9517932896259159,\n",
       "   0.9473582722715002,\n",
       "   0.9490937138449672,\n",
       "   0.9460084843810258,\n",
       "   0.9500578480524489,\n",
       "   0.9506363285769379,\n",
       "   0.9514076359429232,\n",
       "   0.9506363285769379,\n",
       "   0.952371770150405,\n",
       "   0.9541072117238719,\n",
       "   0.9514076359429232,\n",
       "   0.9516004627844196,\n",
       "   0.9462013112225222,\n",
       "   0.9539143848823757,\n",
       "   0.951214809101427,\n",
       "   0.9514076359429232,\n",
       "   0.9479367527959892,\n",
       "   0.952371770150405,\n",
       "   0.9531430775163903,\n",
       "   0.9519861164674123,\n",
       "   0.9400308522946395,\n",
       "   0.9490937138449672,\n",
       "   0.9450443501735442,\n",
       "   0.9504435017354416,\n",
       "   0.9504435017354416,\n",
       "   0.9500578480524489,\n",
       "   0.9508291554184343,\n",
       "   0.9514076359429232,\n",
       "   0.9494793675279599,\n",
       "   0.9506363285769379,\n",
       "   0.9508291554184343,\n",
       "   0.9517932896259159,\n",
       "   0.9516004627844196,\n",
       "   0.9525645969919013,\n",
       "   0.952371770150405,\n",
       "   0.9527574238333976,\n",
       "   0.9448515233320478,\n",
       "   0.9483224064789819,\n",
       "   0.9514076359429232,\n",
       "   0.951214809101427,\n",
       "   0.9506363285769379,\n",
       "   0.9508291554184343,\n",
       "   0.9516004627844196,\n",
       "   0.9510219822599306,\n",
       "   0.9510219822599306,\n",
       "   0.9456228306980332,\n",
       "   0.9500578480524489,\n",
       "   0.9500578480524489,\n",
       "   0.9498650212109525,\n",
       "   0.9506363285769379,\n",
       "   array([0.92924619, 0.846925  , 0.86851745, 0.9161365 , 0.91247349,\n",
       "          0.92712551, 0.92018508, 0.93155967, 0.91073838, 0.9142086 ,\n",
       "          0.91208791, 0.94100636, 0.91517255, 0.92963177, 0.91054559,\n",
       "          0.90765375, 0.93811452, 0.94370542, 0.94177752, 0.9329092 ,\n",
       "          0.94486216, 0.93271641, 0.9273183 , 0.94139194, 0.93445151,\n",
       "          0.94081357, 0.93965683, 0.94351263, 0.93445151, 0.94004241,\n",
       "          0.93907847, 0.94621168, 0.93580104, 0.94042799, 0.93310199,\n",
       "          0.944091  , 0.90746096, 0.94370542, 0.93869289, 0.94081357,\n",
       "          0.94235589, 0.94351263, 0.93502988, 0.94544052, 0.94544052,\n",
       "          0.94139194, 0.94505495, 0.93984962, 0.94389821, 0.94544052,\n",
       "          0.93753615, 0.94042799, 0.94004241, 0.93984962, 0.93541546,\n",
       "          0.93946404, 0.94177752, 0.93830731, 0.94235589, 0.94274147,\n",
       "          0.94331984, 0.94139194, 0.94158473, 0.94081357, 0.94081357,\n",
       "          0.94447658, 0.93541546, 0.94197031, 0.94312705, 0.94389821,\n",
       "          0.94331984, 0.93888568, 0.94293426, 0.93252362, 0.94312705,\n",
       "          0.94119915, 0.94370542]),\n",
       "   array([0.87485538, 0.92846124, 0.94022368, 0.95449287, 0.95140764,\n",
       "          0.93000386, 0.94620131, 0.95063633, 0.95430004, 0.94311608,\n",
       "          0.94793675, 0.94793675, 0.93790976, 0.95391438, 0.94138064,\n",
       "          0.94350174, 0.94812958, 0.95005785, 0.94812958, 0.94774393,\n",
       "          0.95005785, 0.95160046, 0.94562283, 0.94639414, 0.9475511 ,\n",
       "          0.94928654, 0.95005785, 0.94581566, 0.94793675, 0.94909371,\n",
       "          0.9533359 , 0.94812958, 0.94234477, 0.95082916, 0.94581566,\n",
       "          0.9533359 , 0.95140764, 0.95102198, 0.94851523, 0.95217894,\n",
       "          0.95198612, 0.94041651, 0.94427304, 0.94890089, 0.95140764,\n",
       "          0.94890089, 0.94947937, 0.95102198, 0.95295025, 0.9533359 ,\n",
       "          0.95352873, 0.95217894, 0.94735827, 0.94446587, 0.95487852,\n",
       "          0.94986502, 0.94697262, 0.95102198, 0.95025067, 0.95661396,\n",
       "          0.94658696, 0.9446587 , 0.95275742, 0.94369456, 0.95160046,\n",
       "          0.95179329, 0.95198612, 0.94890089, 0.93598149, 0.95295025,\n",
       "          0.9533359 , 0.95410721, 0.93790976, 0.94909371, 0.94600848,\n",
       "          0.94890089, 0.95140764, 0.94735827, 0.95063633, 0.95160046,\n",
       "          0.95102198, 0.95102198, 0.94446587, 0.94620131, 0.9525646 ,\n",
       "          0.95198612, 0.95198612, 0.95217894, 0.94986502, 0.95082916,\n",
       "          0.9525646 , 0.95487852, 0.95314308, 0.93096799, 0.94774393,\n",
       "          0.94928654, 0.94658696, 0.94986502, 0.95179329, 0.95082916,\n",
       "          0.95082916, 0.94967219, 0.95102198, 0.94928654, 0.95005785,\n",
       "          0.95198612, 0.95314308, 0.95468569, 0.95140764, 0.9533359 ,\n",
       "          0.95372156, 0.9533359 , 0.955457  , 0.95198612, 0.95082916,\n",
       "          0.95025067, 0.94562283, 0.95295025, 0.9504435 , 0.95217894,\n",
       "          0.95082916, 0.95372156, 0.95275742, 0.95102198, 0.95275742,\n",
       "          0.95237177, 0.95063633, 0.95025067, 0.95179329, 0.95082916,\n",
       "          0.9504435 , 0.95005785, 0.95140764, 0.955457  , 0.95410721,\n",
       "          0.95295025, 0.95487852, 0.95410721, 0.9533359 , 0.9533359 ,\n",
       "          0.95410721, 0.95102198, 0.94967219, 0.95237177, 0.95140764,\n",
       "          0.94369456, 0.93848824, 0.94677979, 0.94909371, 0.94870806,\n",
       "          0.94851523, 0.94832241, 0.93077516, 0.94639414, 0.94967219,\n",
       "          0.94851523, 0.94986502, 0.94967219, 0.94967219, 0.94967219,\n",
       "          0.95005785, 0.94928654, 0.95005785, 0.94909371, 0.95025067,\n",
       "          0.95063633, 0.95217894, 0.95140764, 0.95102198, 0.94639414,\n",
       "          0.95160046, 0.95063633, 0.95121481, 0.95198612, 0.95198612,\n",
       "          0.95237177, 0.95198612, 0.93829541, 0.95468569, 0.95237177,\n",
       "          0.95025067, 0.95198612, 0.95217894, 0.95314308, 0.95217894,\n",
       "          0.95237177, 0.95217894, 0.95179329, 0.95160046, 0.95179329,\n",
       "          0.95102198, 0.94870806, 0.95217894, 0.94986502, 0.95121481,\n",
       "          0.95102198, 0.95121481, 0.95082916, 0.95140764, 0.95140764,\n",
       "          0.95179329, 0.94909371, 0.95102198, 0.95160046, 0.95468569,\n",
       "          0.95449287, 0.95430004, 0.95314308, 0.95025067, 0.95102198,\n",
       "          0.95121481, 0.95179329, 0.95160046, 0.95198612, 0.95160046,\n",
       "          0.94292325, 0.94909371, 0.95121481, 0.95140764, 0.9504435 ,\n",
       "          0.95160046, 0.95102198, 0.95160046, 0.95140764, 0.95121481,\n",
       "          0.95198612, 0.95179329, 0.95179329, 0.9525646 , 0.95314308,\n",
       "          0.95314308, 0.95275742, 0.95295025, 0.95295025, 0.95217894,\n",
       "          0.95198612, 0.95102198, 0.94967219, 0.95295025, 0.9533359 ,\n",
       "          0.95449287, 0.95487852, 0.95564983, 0.95564983, 0.95526417,\n",
       "          0.95584265, 0.95526417, 0.95622831, 0.955457  , 0.9525646 ,\n",
       "          0.95642113, 0.95468569, 0.9533359 , 0.95352873, 0.93540301,\n",
       "          0.95217894, 0.95295025, 0.95410721, 0.95275742, 0.95082916,\n",
       "          0.95102198, 0.95160046, 0.95198612, 0.95198612, 0.95160046,\n",
       "          0.95217894, 0.9525646 , 0.95352873, 0.95352873, 0.95314308,\n",
       "          0.95410721, 0.95468569, 0.95449287, 0.95025067, 0.95468569,\n",
       "          0.9525646 , 0.95275742, 0.9525646 , 0.95237177, 0.95275742,\n",
       "          0.95352873, 0.95237177, 0.95295025, 0.95295025, 0.95372156,\n",
       "          0.95449287, 0.95275742, 0.94774393, 0.95372156, 0.94986502,\n",
       "          0.95063633, 0.95121481, 0.95082916, 0.95140764, 0.95121481,\n",
       "          0.95121481, 0.95025067, 0.95082916, 0.94793675, 0.95314308]),\n",
       "   array([0.93077516, 0.93154647, 0.93308909, 0.94176629, 0.94600848,\n",
       "          0.94600848, 0.92537601, 0.93578866, 0.94060933, 0.94176629,\n",
       "          0.9396452 , 0.93983803, 0.94523718, 0.9475511 , 0.94793675,\n",
       "          0.94870806, 0.9446587 , 0.93251061, 0.92306209, 0.95121481,\n",
       "          0.9425376 , 0.94986502, 0.95468569, 0.94735827, 0.94716545,\n",
       "          0.94986502, 0.94234477, 0.94870806, 0.95217894, 0.9533359 ,\n",
       "          0.95391438, 0.95372156, 0.94870806, 0.94793675, 0.95198612,\n",
       "          0.95140764, 0.9504435 , 0.95468569, 0.95217894, 0.95005785,\n",
       "          0.95102198, 0.95005785, 0.94639414, 0.95487852, 0.95661396,\n",
       "          0.95410721, 0.95564983, 0.94890089, 0.95564983, 0.95430004,\n",
       "          0.95526417, 0.94947937, 0.95314308, 0.95275742, 0.955457  ,\n",
       "          0.95449287, 0.95160046, 0.94697262, 0.94677979, 0.95642113,\n",
       "          0.95025067, 0.94041651, 0.95314308, 0.94832241, 0.95526417,\n",
       "          0.94677979, 0.95063633, 0.95295025, 0.95179329, 0.95237177,\n",
       "          0.95179329, 0.94986502, 0.95295025, 0.95140764, 0.95198612,\n",
       "          0.94947937, 0.95198612]),\n",
       "   array([0.90281527, 0.94022368, 0.89278828, 0.9533359 , 0.95622831,\n",
       "          0.9583494 , 0.93713845, 0.95372156, 0.94639414, 0.95063633,\n",
       "          0.94003085, 0.94600848, 0.95217894, 0.9533359 , 0.95526417,\n",
       "          0.95969919, 0.9367528 , 0.95372156, 0.95719244, 0.95102198,\n",
       "          0.94947937, 0.95121481, 0.95738527, 0.955457  , 0.955457  ,\n",
       "          0.94774393, 0.95584265, 0.9504435 , 0.95487852, 0.95430004,\n",
       "          0.95468569, 0.95603548, 0.9583494 , 0.95526417, 0.95391438,\n",
       "          0.95410721, 0.95564983, 0.95661396, 0.95815658, 0.95622831,\n",
       "          0.95487852, 0.95680679, 0.95082916, 0.95295025, 0.95237177,\n",
       "          0.94793675, 0.94890089, 0.95622831, 0.95796375, 0.95526417,\n",
       "          0.95507135, 0.955457  , 0.95449287, 0.95584265, 0.95449287,\n",
       "          0.95410721, 0.95507135, 0.95468569, 0.95507135, 0.95391438,\n",
       "          0.94677979, 0.95468569, 0.95642113, 0.94735827, 0.95564983,\n",
       "          0.95352873, 0.95352873, 0.955457  , 0.95487852, 0.95468569,\n",
       "          0.95507135, 0.95468569, 0.955457  , 0.95102198, 0.9533359 ,\n",
       "          0.95430004, 0.95430004]),\n",
       "   array([0.93655997, 0.93096799, 0.91939838, 0.93193212, 0.94041651,\n",
       "          0.93810258, 0.94427304, 0.94523718, 0.9296182 , 0.94446587,\n",
       "          0.94234477, 0.94176629, 0.94967219, 0.94890089, 0.94774393,\n",
       "          0.9425376 , 0.94793675, 0.94446587, 0.94967219, 0.94523718,\n",
       "          0.95082916, 0.95102198, 0.94832241, 0.94928654, 0.94832241,\n",
       "          0.95025067, 0.94793675, 0.94967219, 0.95102198, 0.94620131,\n",
       "          0.94986502, 0.94793675, 0.94369456, 0.94774393, 0.94851523,\n",
       "          0.94812958, 0.95005785, 0.94562283, 0.94523718, 0.94716545,\n",
       "          0.94215195, 0.94909371, 0.94543   , 0.9446587 , 0.94947937,\n",
       "          0.94369456, 0.94677979, 0.94832241, 0.94658696, 0.94581566,\n",
       "          0.95217894, 0.94967219, 0.94832241, 0.94774393, 0.95121481,\n",
       "          0.95005785, 0.94408022, 0.95160046, 0.9475511 , 0.95275742,\n",
       "          0.95198612, 0.9525646 , 0.9533359 , 0.93887389, 0.9446587 ,\n",
       "          0.94774393, 0.95160046, 0.95063633, 0.94485152, 0.9425376 ,\n",
       "          0.95295025, 0.95005785, 0.95063633, 0.95025067, 0.95198612,\n",
       "          0.94234477, 0.95025067, 0.94986502, 0.95179329, 0.95314308,\n",
       "          0.95430004, 0.94793675, 0.9504435 , 0.95063633, 0.95198612,\n",
       "          0.95140764, 0.94967219, 0.94832241, 0.94793675, 0.9504435 ,\n",
       "          0.95121481, 0.94581566, 0.94812958, 0.95005785, 0.95198612,\n",
       "          0.95102198, 0.94967219, 0.95140764, 0.95121481, 0.94909371,\n",
       "          0.95102198, 0.94504435, 0.94735827, 0.94928654, 0.9504435 ,\n",
       "          0.95005785, 0.94793675, 0.94947937, 0.94986502, 0.95025067,\n",
       "          0.95082916, 0.94928654, 0.93790976, 0.94408022, 0.95082916,\n",
       "          0.95005785, 0.95005785, 0.93617432, 0.94909371, 0.94967219,\n",
       "          0.94890089, 0.94986502, 0.94947937, 0.94967219, 0.94639414,\n",
       "          0.95121481, 0.95102198, 0.95082916, 0.95121481, 0.95198612,\n",
       "          0.95025067, 0.95275742, 0.95082916, 0.94735827, 0.95179329,\n",
       "          0.95160046, 0.95179329, 0.95198612, 0.94716545, 0.95063633,\n",
       "          0.94812958, 0.94986502, 0.94967219, 0.95025067, 0.94947937,\n",
       "          0.94986502, 0.94909371, 0.95121481, 0.94099499, 0.95025067,\n",
       "          0.94909371, 0.95121481, 0.95179329, 0.95102198, 0.95121481,\n",
       "          0.94909371, 0.95082916, 0.94080216, 0.94543   , 0.9525646 ,\n",
       "          0.94832241, 0.94928654, 0.94812958, 0.94890089, 0.94639414,\n",
       "          0.94947937, 0.95198612, 0.95121481, 0.95198612, 0.94967219,\n",
       "          0.9525646 , 0.95005785, 0.95063633, 0.94967219, 0.95102198,\n",
       "          0.95025067, 0.95005785, 0.95063633, 0.9533359 , 0.95410721,\n",
       "          0.95507135, 0.94890089, 0.94735827, 0.94928654, 0.94832241,\n",
       "          0.94986502, 0.95005785, 0.95160046, 0.95140764, 0.95237177,\n",
       "          0.94350174, 0.94870806, 0.94928654, 0.95275742, 0.95140764,\n",
       "          0.94581566, 0.94812958, 0.94928654, 0.94350174, 0.9504435 ,\n",
       "          0.94890089, 0.94909371, 0.95025067, 0.94967219, 0.95102198,\n",
       "          0.95179329, 0.9533359 , 0.95430004, 0.95121481, 0.95005785,\n",
       "          0.95352873, 0.95449287, 0.94581566, 0.95005785, 0.9533359 ,\n",
       "          0.95410721, 0.95121481, 0.95063633, 0.9525646 , 0.95082916,\n",
       "          0.95468569, 0.95391438, 0.95507135, 0.95487852, 0.95198612,\n",
       "          0.95160046, 0.95217894, 0.95237177, 0.95160046, 0.95121481,\n",
       "          0.95487852, 0.94022368, 0.95179329, 0.95314308, 0.95295025,\n",
       "          0.95352873, 0.95102198, 0.95005785, 0.95275742, 0.95217894,\n",
       "          0.95237177, 0.95121481, 0.95025067, 0.95063633, 0.94446587,\n",
       "          0.95063633, 0.94947937, 0.95179329, 0.9504435 , 0.95372156,\n",
       "          0.95179329, 0.94735827, 0.94909371, 0.94600848, 0.95005785,\n",
       "          0.95063633, 0.95140764, 0.95063633, 0.95237177, 0.95410721,\n",
       "          0.95140764, 0.95160046, 0.94620131, 0.95391438, 0.95121481,\n",
       "          0.95140764, 0.94793675, 0.95237177, 0.95314308, 0.95198612,\n",
       "          0.94003085, 0.94909371, 0.94504435, 0.9504435 , 0.9504435 ,\n",
       "          0.95005785, 0.95082916, 0.95140764, 0.94947937, 0.95063633,\n",
       "          0.95082916, 0.95179329, 0.95160046, 0.9525646 , 0.95237177,\n",
       "          0.95275742, 0.94485152, 0.94832241, 0.95140764, 0.95121481,\n",
       "          0.95063633, 0.95082916, 0.95160046, 0.95102198, 0.95102198,\n",
       "          0.94562283, 0.95005785, 0.95005785, 0.94986502, 0.95063633])],\n",
       "  'ap_val_epoch': [0.8920139879748964,\n",
       "   0.9045733408248565,\n",
       "   0.8835509988829366,\n",
       "   0.9048038475546748,\n",
       "   0.912112646075331,\n",
       "   0.9121263328803373,\n",
       "   0.8929858850592621,\n",
       "   0.8924130442922422,\n",
       "   0.8505319508009659,\n",
       "   0.8894964220210351,\n",
       "   0.8781569539656546,\n",
       "   0.8759695864808683,\n",
       "   0.8683424943594807,\n",
       "   0.8825573448517441,\n",
       "   0.8596776302758667,\n",
       "   0.8370752840054843,\n",
       "   0.8823056313468972,\n",
       "   0.8283466075928392,\n",
       "   0.8471654648106185,\n",
       "   0.8478736273285876,\n",
       "   0.8632313887091417,\n",
       "   0.8664810423756646,\n",
       "   0.851698080086576,\n",
       "   0.8701548824358903,\n",
       "   0.8482007736189214,\n",
       "   0.8554343742238876,\n",
       "   0.8460179120188377,\n",
       "   0.8678616836051604,\n",
       "   0.8589347449491964,\n",
       "   0.8478712647565511,\n",
       "   0.8572934935872719,\n",
       "   0.8611864116183333,\n",
       "   0.8306522390194301,\n",
       "   0.8448162027627111,\n",
       "   0.8437760525486183,\n",
       "   0.8491087953047874,\n",
       "   0.8477172665337072,\n",
       "   0.8508856527017662,\n",
       "   0.8161574740628087,\n",
       "   0.8417474166462271,\n",
       "   0.8287566400785643,\n",
       "   0.8454462625015616,\n",
       "   0.8362466936114832,\n",
       "   0.8542381687289266,\n",
       "   0.8607789215874374,\n",
       "   0.8397927807599045,\n",
       "   0.8452102959005441,\n",
       "   0.853938397993764,\n",
       "   0.8402344691056058,\n",
       "   0.8572713098024166,\n",
       "   0.8563836048956427,\n",
       "   0.8544924310224615,\n",
       "   0.8554956299012231,\n",
       "   0.8450217579468703,\n",
       "   0.850581867819118,\n",
       "   0.8503029793099661,\n",
       "   0.8475059772578448,\n",
       "   0.8638018309562682,\n",
       "   0.8561267986963538,\n",
       "   0.8568674306458949,\n",
       "   0.8580977453944538,\n",
       "   0.8523767113077108,\n",
       "   0.8552886527312321,\n",
       "   0.8468008807810462,\n",
       "   0.8455313929573935,\n",
       "   0.8488564855601518,\n",
       "   0.8612273579601956,\n",
       "   0.8645431440295414,\n",
       "   0.8548096860263944,\n",
       "   0.8379413485316666,\n",
       "   0.8668420057086239,\n",
       "   0.8492479105837017,\n",
       "   0.8480565205170112,\n",
       "   0.8485867694964113,\n",
       "   0.845127281704091,\n",
       "   0.8507292367772733,\n",
       "   0.8441523257044662,\n",
       "   0.8465616072452924,\n",
       "   0.8526245700628505,\n",
       "   0.8562047495751797,\n",
       "   0.857284690146716,\n",
       "   0.8586438027218517,\n",
       "   0.853774211481836,\n",
       "   0.8534582565117597,\n",
       "   0.8683790782705874,\n",
       "   0.8591367556074523,\n",
       "   0.8559389621083906,\n",
       "   0.8370854758966376,\n",
       "   0.84470878672678,\n",
       "   0.8479312959114484,\n",
       "   0.8481188638734775,\n",
       "   0.8415824435301763,\n",
       "   0.8367028244647509,\n",
       "   0.8605832713414486,\n",
       "   0.8522352506132083,\n",
       "   0.8513625665900177,\n",
       "   0.8464774357837072,\n",
       "   0.8573122123677354,\n",
       "   0.8565770766101507,\n",
       "   0.8437657942314992,\n",
       "   0.8502167647843389,\n",
       "   0.8321239532042158,\n",
       "   0.8351547977520036,\n",
       "   0.8545395411762781,\n",
       "   0.8603695047955666,\n",
       "   0.8455408588766503,\n",
       "   0.843474676614311,\n",
       "   0.8518602361175012,\n",
       "   0.8536843283154204,\n",
       "   0.8565372887996696,\n",
       "   0.8518987679996528,\n",
       "   0.8533008864172066,\n",
       "   0.8068957298088153,\n",
       "   0.8200483542815894,\n",
       "   0.8441386569478835,\n",
       "   0.840658312699412,\n",
       "   0.8469981411763684,\n",
       "   0.8028292664242122,\n",
       "   0.8430400572057665,\n",
       "   0.8626676614988108,\n",
       "   0.8545606890316799,\n",
       "   0.8593521818048334,\n",
       "   0.8449958144896316,\n",
       "   0.8396430645583263,\n",
       "   0.8190744749961906,\n",
       "   0.8544722994909167,\n",
       "   0.8514073271249604,\n",
       "   0.851921359756162,\n",
       "   0.8542992607761151,\n",
       "   0.8534494288278189,\n",
       "   0.8503405779045489,\n",
       "   0.8438840715994044,\n",
       "   0.8447706544046356,\n",
       "   0.8516155582396475,\n",
       "   0.8661584561916238,\n",
       "   0.853864493066718,\n",
       "   0.8601054942582835,\n",
       "   0.8761530924307681,\n",
       "   0.8320134481549823,\n",
       "   0.8472795243196363,\n",
       "   0.8487092847870159,\n",
       "   0.8377198555471431,\n",
       "   0.8593772379604088,\n",
       "   0.8492577465142218,\n",
       "   0.8503993673424197,\n",
       "   0.8515832959326204,\n",
       "   0.851220313389859,\n",
       "   0.8381180856966595,\n",
       "   0.8189398624188934,\n",
       "   0.8478414877085003,\n",
       "   0.8421728422290452,\n",
       "   0.8448071549294752,\n",
       "   0.8388824704236766,\n",
       "   0.8436732405371906,\n",
       "   0.8417023375467055,\n",
       "   0.8352692557917363,\n",
       "   0.8355659476649387,\n",
       "   0.8531483259290672,\n",
       "   0.8291339603117257,\n",
       "   0.8603014707674901,\n",
       "   0.8563445881610139,\n",
       "   0.856267268409226,\n",
       "   0.8501512132195034,\n",
       "   0.8422568344890198,\n",
       "   0.8245082418452474,\n",
       "   0.8367830015446351,\n",
       "   0.8425583888372834,\n",
       "   0.8497497454936476,\n",
       "   0.8432336852455937,\n",
       "   0.8351977923919407,\n",
       "   0.848081802139505,\n",
       "   0.842195322956271,\n",
       "   0.8393956159630482,\n",
       "   0.8411396418898337,\n",
       "   0.8455378890124092,\n",
       "   0.8485407786251897,\n",
       "   0.8535452645427728,\n",
       "   0.8798761705947309,\n",
       "   0.85208849635958,\n",
       "   0.8669285187672352,\n",
       "   0.8604739975005614,\n",
       "   0.8130023914150633,\n",
       "   0.854456935639294,\n",
       "   0.8557463546301206,\n",
       "   0.8469666449042935,\n",
       "   0.8483561407044536,\n",
       "   0.842827697145345,\n",
       "   0.8417624412999852,\n",
       "   0.8409945697475555,\n",
       "   0.8404565981107477,\n",
       "   0.8066550035200228,\n",
       "   0.8433616764281476,\n",
       "   0.8147867435637937,\n",
       "   0.8464426722046161,\n",
       "   0.8454949353656602,\n",
       "   0.8208998776927766,\n",
       "   0.8219929833234583,\n",
       "   0.8235064628589952,\n",
       "   0.8285299107085362,\n",
       "   0.8475006606176599,\n",
       "   0.8486421960097122,\n",
       "   0.8512743589770613,\n",
       "   0.8537028989130916,\n",
       "   0.8484363892142529,\n",
       "   0.8398676842883956,\n",
       "   0.8448860120579392,\n",
       "   0.8462453344046492,\n",
       "   0.84631746854287,\n",
       "   0.8495971197706185,\n",
       "   0.8477878114430508,\n",
       "   0.8487407859192845,\n",
       "   0.8590813603885515,\n",
       "   0.8207686909333959,\n",
       "   0.850064488028247,\n",
       "   0.8549446867607756,\n",
       "   0.8520515419809667,\n",
       "   0.8512226836434488,\n",
       "   0.8412232560464333,\n",
       "   0.8226953351870003,\n",
       "   0.8332624288152022,\n",
       "   0.8390285619661336,\n",
       "   0.8423964435674649,\n",
       "   0.8457806108700746,\n",
       "   0.8388232045709368,\n",
       "   0.8190842049866075,\n",
       "   0.8406969863962535,\n",
       "   0.8356556361681539,\n",
       "   0.8419287920719678,\n",
       "   0.843143455501769,\n",
       "   0.8365587783978953,\n",
       "   0.8376192631940161,\n",
       "   0.7907374671509138,\n",
       "   0.8394708023571849,\n",
       "   0.8261535123319356,\n",
       "   0.8108291171646886,\n",
       "   0.8029715833841603,\n",
       "   0.8047299247430694,\n",
       "   0.8166658996929747,\n",
       "   0.8375101907637423,\n",
       "   0.8422091060679403,\n",
       "   0.8408492802609663,\n",
       "   0.8419135403759126,\n",
       "   0.8350609269394649,\n",
       "   0.837520323181135,\n",
       "   0.8118348305730385,\n",
       "   0.8503992829078005,\n",
       "   0.8358786115860933,\n",
       "   0.8422128961209466,\n",
       "   0.838946175800882,\n",
       "   0.8462917903179112,\n",
       "   0.8403718040428657,\n",
       "   0.8325171480281522,\n",
       "   0.8423444872541618,\n",
       "   0.8590361516740355,\n",
       "   0.8474988584066195,\n",
       "   0.8537058618618392,\n",
       "   0.8542283770094932,\n",
       "   0.8501027028168965,\n",
       "   0.8437797614468034,\n",
       "   0.8628786373703408,\n",
       "   0.8385958588304075,\n",
       "   0.8443201440951551,\n",
       "   0.8228008467987313,\n",
       "   0.839585308544038,\n",
       "   0.8376954509446578,\n",
       "   0.8365377752115785,\n",
       "   0.842666567487546,\n",
       "   0.8380777559110593,\n",
       "   0.8349024810775215,\n",
       "   0.8406369070577777,\n",
       "   0.7950341079236237,\n",
       "   0.8249349367969783,\n",
       "   0.8128602232731014,\n",
       "   0.8302415809154917,\n",
       "   0.8338930631912004,\n",
       "   0.8361147024313498,\n",
       "   0.8377536095282413,\n",
       "   0.8309533214347329,\n",
       "   0.8319764632620164,\n",
       "   0.8439033973171256,\n",
       "   0.8455526870704001,\n",
       "   0.8425511569203642,\n",
       "   0.8496439215383046,\n",
       "   0.844177066116545,\n",
       "   0.8470786289682879,\n",
       "   0.829703265246434,\n",
       "   0.8077551855765125,\n",
       "   0.8350833647509966,\n",
       "   0.8324823397579357,\n",
       "   0.8253104628431266,\n",
       "   0.8209825531598578,\n",
       "   0.8239123697221471,\n",
       "   0.828708891030563,\n",
       "   0.8249840144743704,\n",
       "   0.8315739072049588,\n",
       "   0.8186065818733859,\n",
       "   0.8272047292004467,\n",
       "   0.8315393065060083,\n",
       "   0.8228180469651974,\n",
       "   0.834962581881229,\n",
       "   array([0.80749808, 0.82273989, 0.83985433, 0.85872457, 0.8609963 ,\n",
       "          0.86892574, 0.86687471, 0.86497508, 0.86812295, 0.87129951,\n",
       "          0.86204069, 0.87523819, 0.87218589, 0.88048727, 0.87876505,\n",
       "          0.8727605 , 0.87670528, 0.875446  , 0.86724573, 0.8770505 ,\n",
       "          0.87408241, 0.85827695, 0.8474401 , 0.85998394, 0.85954765,\n",
       "          0.85340003, 0.86702919, 0.85972703, 0.85912059, 0.86593158,\n",
       "          0.85212195, 0.86264557, 0.86135227, 0.84618975, 0.8458472 ,\n",
       "          0.85286863, 0.78113828, 0.83625027, 0.84953107, 0.83914001,\n",
       "          0.82946183, 0.82441968, 0.83068897, 0.83968389, 0.82796142,\n",
       "          0.83755452, 0.83776139, 0.82672484, 0.80960471, 0.84040085,\n",
       "          0.81835219, 0.82061831, 0.80699028, 0.8031446 , 0.79315991,\n",
       "          0.80886531, 0.82406291, 0.79936912, 0.80417748, 0.78936163,\n",
       "          0.81297354, 0.80344256, 0.8088646 , 0.82183181, 0.7977033 ,\n",
       "          0.819162  , 0.81072574, 0.80697185, 0.80769413, 0.78564435,\n",
       "          0.7777096 , 0.78464399, 0.78986115, 0.7727863 , 0.79610328,\n",
       "          0.80353494, 0.80560951]),\n",
       "   array([0.85050836, 0.8945854 , 0.88183446, 0.89801133, 0.89269674,\n",
       "          0.90184882, 0.89944001, 0.88247158, 0.88257883, 0.86909664,\n",
       "          0.8591877 , 0.87320585, 0.84263356, 0.87131383, 0.86874536,\n",
       "          0.83008057, 0.8653818 , 0.82743907, 0.82818373, 0.84196858,\n",
       "          0.84121597, 0.83726988, 0.84545626, 0.83725168, 0.84267265,\n",
       "          0.85415635, 0.84703515, 0.84162391, 0.84350921, 0.81951351,\n",
       "          0.85460143, 0.84621165, 0.81761846, 0.85915317, 0.84574773,\n",
       "          0.87104808, 0.86801928, 0.85020394, 0.83393149, 0.85582325,\n",
       "          0.86037388, 0.84106825, 0.83793092, 0.84283481, 0.85788078,\n",
       "          0.83731512, 0.86203954, 0.84867985, 0.85112449, 0.84808385,\n",
       "          0.85016662, 0.84627421, 0.85275244, 0.83586347, 0.86034353,\n",
       "          0.84549155, 0.85456142, 0.84418285, 0.84315455, 0.8601637 ,\n",
       "          0.83457027, 0.83965531, 0.85968096, 0.83624994, 0.84494397,\n",
       "          0.84426439, 0.8388982 , 0.857293  , 0.80950108, 0.85130096,\n",
       "          0.8582238 , 0.85114032, 0.80034267, 0.84000275, 0.83610677,\n",
       "          0.85408375, 0.85710637, 0.83142345, 0.84621788, 0.84375859,\n",
       "          0.84245909, 0.82555204, 0.8270866 , 0.84053019, 0.85162987,\n",
       "          0.85039121, 0.8443589 , 0.83441654, 0.85542304, 0.84465845,\n",
       "          0.84878658, 0.8497215 , 0.82708483, 0.7849376 , 0.818447  ,\n",
       "          0.83066374, 0.81284678, 0.84869161, 0.84185435, 0.83917657,\n",
       "          0.84995473, 0.84632404, 0.84502257, 0.84947716, 0.86055234,\n",
       "          0.86506482, 0.85069075, 0.84327971, 0.83254231, 0.83757101,\n",
       "          0.85484188, 0.85110142, 0.85089663, 0.83960792, 0.85901149,\n",
       "          0.83844657, 0.84573977, 0.84703243, 0.84315764, 0.84907899,\n",
       "          0.84987261, 0.85456075, 0.85333846, 0.84750394, 0.84683416,\n",
       "          0.83822623, 0.84182448, 0.84982773, 0.8497264 , 0.84960615,\n",
       "          0.85034073, 0.85349308, 0.84442306, 0.84223841, 0.84590322,\n",
       "          0.83791998, 0.83785981, 0.83568521, 0.83979451, 0.83545439,\n",
       "          0.83934424, 0.83461652, 0.84106346, 0.84322977, 0.85312846,\n",
       "          0.8016013 , 0.76284224, 0.82245616, 0.82883044, 0.83144285,\n",
       "          0.83179948, 0.82838779, 0.79158514, 0.83818497, 0.84184283,\n",
       "          0.83708054, 0.83349152, 0.83527126, 0.83733483, 0.83871852,\n",
       "          0.83422018, 0.83261536, 0.8305857 , 0.8326111 , 0.83467415,\n",
       "          0.8293834 , 0.84068343, 0.83798975, 0.84622524, 0.83510796,\n",
       "          0.83773145, 0.86378898, 0.84978884, 0.84238768, 0.84090872,\n",
       "          0.84187274, 0.83842021, 0.76976561, 0.8527285 , 0.83963606,\n",
       "          0.83461521, 0.82813078, 0.82695319, 0.82888188, 0.82844103,\n",
       "          0.82587955, 0.83065228, 0.83153379, 0.82819265, 0.83138288,\n",
       "          0.82930079, 0.84093008, 0.85878471, 0.83618954, 0.83073271,\n",
       "          0.83132026, 0.82967757, 0.82434435, 0.82677537, 0.8244155 ,\n",
       "          0.82318605, 0.83991496, 0.84146596, 0.84727001, 0.86477869,\n",
       "          0.86255651, 0.85942831, 0.85202402, 0.83455717, 0.84173504,\n",
       "          0.83064071, 0.83402255, 0.83191817, 0.83274751, 0.829405  ,\n",
       "          0.82740245, 0.83823049, 0.83775123, 0.8365973 , 0.83768967,\n",
       "          0.84583619, 0.83635955, 0.83714467, 0.83517108, 0.83805684,\n",
       "          0.84472956, 0.85742723, 0.84489126, 0.84195345, 0.83988398,\n",
       "          0.84174784, 0.84269213, 0.84080495, 0.83771412, 0.83335816,\n",
       "          0.82811078, 0.84650997, 0.83434885, 0.84282457, 0.8629515 ,\n",
       "          0.85952241, 0.85081046, 0.84646927, 0.84350202, 0.84760059,\n",
       "          0.84909694, 0.8454442 , 0.84283845, 0.8404826 , 0.82854559,\n",
       "          0.82662613, 0.85936174, 0.84234821, 0.8361257 , 0.73773646,\n",
       "          0.85098348, 0.85113861, 0.85124649, 0.84128331, 0.83837236,\n",
       "          0.83922547, 0.84127975, 0.84399793, 0.84239191, 0.84029366,\n",
       "          0.83916371, 0.84172661, 0.84283436, 0.8404242 , 0.84318863,\n",
       "          0.84189314, 0.84360003, 0.84315891, 0.81484337, 0.84982282,\n",
       "          0.82926959, 0.84973852, 0.84297467, 0.84324114, 0.84484864,\n",
       "          0.84637786, 0.84450399, 0.84328693, 0.84191783, 0.84062989,\n",
       "          0.84002384, 0.83631173, 0.82976001, 0.84190572, 0.85322099,\n",
       "          0.83854927, 0.8397134 , 0.84029677, 0.84218707, 0.8427978 ,\n",
       "          0.84334529, 0.84015069, 0.83741259, 0.82952646, 0.83607146]),\n",
       "   array([0.88686258, 0.89644225, 0.90912654, 0.90636221, 0.91790807,\n",
       "          0.91492465, 0.91855374, 0.91547161, 0.92211304, 0.92423402,\n",
       "          0.91787963, 0.91550445, 0.90473532, 0.91033492, 0.92192227,\n",
       "          0.90733113, 0.90645004, 0.90466408, 0.88908191, 0.91219232,\n",
       "          0.88206361, 0.90643593, 0.91232262, 0.8938772 , 0.8960111 ,\n",
       "          0.90741513, 0.87486337, 0.89164426, 0.88943129, 0.87644781,\n",
       "          0.8817995 , 0.86451056, 0.83260455, 0.83627636, 0.87236768,\n",
       "          0.85954844, 0.86192305, 0.87540782, 0.8647801 , 0.84062044,\n",
       "          0.84989906, 0.86431299, 0.83503882, 0.83604738, 0.88184154,\n",
       "          0.88032716, 0.84060098, 0.84414986, 0.84226711, 0.87105674,\n",
       "          0.83647515, 0.8289433 , 0.84687117, 0.86581301, 0.86474859,\n",
       "          0.84574062, 0.83783697, 0.81659163, 0.83173475, 0.86320368,\n",
       "          0.83480406, 0.81261836, 0.86148434, 0.84888499, 0.86276126,\n",
       "          0.82903453, 0.84882894, 0.85222455, 0.84850319, 0.83608623,\n",
       "          0.86109771, 0.85396768, 0.85269093, 0.83627575, 0.85424267,\n",
       "          0.82929023, 0.8625986 ]),\n",
       "   array([0.85076218, 0.90102753, 0.89603683, 0.90608854, 0.9155926 ,\n",
       "          0.90155062, 0.89868784, 0.89521428, 0.89620818, 0.89997377,\n",
       "          0.87292768, 0.85436681, 0.88242827, 0.87059176, 0.88019159,\n",
       "          0.89627254, 0.84466012, 0.88899139, 0.87505601, 0.85368879,\n",
       "          0.83774187, 0.84788138, 0.89439267, 0.86638268, 0.87026565,\n",
       "          0.87205375, 0.87177749, 0.84729983, 0.86333409, 0.866917  ,\n",
       "          0.8794635 , 0.85389494, 0.86354285, 0.86348049, 0.84753135,\n",
       "          0.86860865, 0.84933595, 0.83371862, 0.87531175, 0.86575665,\n",
       "          0.85742644, 0.85926381, 0.85088588, 0.83706351, 0.86342047,\n",
       "          0.87184459, 0.861228  , 0.88236956, 0.86758774, 0.86521599,\n",
       "          0.86638992, 0.86502121, 0.86876158, 0.86734454, 0.86101143,\n",
       "          0.85941723, 0.85330539, 0.84474752, 0.84242621, 0.84223235,\n",
       "          0.87119575, 0.86422995, 0.85270515, 0.84561925, 0.86372236,\n",
       "          0.8572354 , 0.84716512, 0.85011591, 0.84849573, 0.84936358,\n",
       "          0.8449143 , 0.8740322 , 0.85490176, 0.85254814, 0.85906013,\n",
       "          0.85136731, 0.84831179]),\n",
       "   array([0.89201399, 0.90457334, 0.883551  , 0.90480385, 0.91211265,\n",
       "          0.91212633, 0.89298589, 0.89241304, 0.85053195, 0.88949642,\n",
       "          0.87815695, 0.87596959, 0.86834249, 0.88255734, 0.85967763,\n",
       "          0.83707528, 0.88230563, 0.82834661, 0.84716546, 0.84787363,\n",
       "          0.86323139, 0.86648104, 0.85169808, 0.87015488, 0.84820077,\n",
       "          0.85543437, 0.84601791, 0.86786168, 0.85893474, 0.84787126,\n",
       "          0.85729349, 0.86118641, 0.83065224, 0.8448162 , 0.84377605,\n",
       "          0.8491088 , 0.84771727, 0.85088565, 0.81615747, 0.84174742,\n",
       "          0.82875664, 0.84544626, 0.83624669, 0.85423817, 0.86077892,\n",
       "          0.83979278, 0.8452103 , 0.8539384 , 0.84023447, 0.85727131,\n",
       "          0.8563836 , 0.85449243, 0.85549563, 0.84502176, 0.85058187,\n",
       "          0.85030298, 0.84750598, 0.86380183, 0.8561268 , 0.85686743,\n",
       "          0.85809775, 0.85237671, 0.85528865, 0.84680088, 0.84553139,\n",
       "          0.84885649, 0.86122736, 0.86454314, 0.85480969, 0.83794135,\n",
       "          0.86684201, 0.84924791, 0.84805652, 0.84858677, 0.84512728,\n",
       "          0.85072924, 0.84415233, 0.84656161, 0.85262457, 0.85620475,\n",
       "          0.85728469, 0.8586438 , 0.85377421, 0.85345826, 0.86837908,\n",
       "          0.85913676, 0.85593896, 0.83708548, 0.84470879, 0.8479313 ,\n",
       "          0.84811886, 0.84158244, 0.83670282, 0.86058327, 0.85223525,\n",
       "          0.85136257, 0.84647744, 0.85731221, 0.85657708, 0.84376579,\n",
       "          0.85021676, 0.83212395, 0.8351548 , 0.85453954, 0.8603695 ,\n",
       "          0.84554086, 0.84347468, 0.85186024, 0.85368433, 0.85653729,\n",
       "          0.85189877, 0.85330089, 0.80689573, 0.82004835, 0.84413866,\n",
       "          0.84065831, 0.84699814, 0.80282927, 0.84304006, 0.86266766,\n",
       "          0.85456069, 0.85935218, 0.84499581, 0.83964306, 0.81907447,\n",
       "          0.8544723 , 0.85140733, 0.85192136, 0.85429926, 0.85344943,\n",
       "          0.85034058, 0.84388407, 0.84477065, 0.85161556, 0.86615846,\n",
       "          0.85386449, 0.86010549, 0.87615309, 0.83201345, 0.84727952,\n",
       "          0.84870928, 0.83771986, 0.85937724, 0.84925775, 0.85039937,\n",
       "          0.8515833 , 0.85122031, 0.83811809, 0.81893986, 0.84784149,\n",
       "          0.84217284, 0.84480715, 0.83888247, 0.84367324, 0.84170234,\n",
       "          0.83526926, 0.83556595, 0.85314833, 0.82913396, 0.86030147,\n",
       "          0.85634459, 0.85626727, 0.85015121, 0.84225683, 0.82450824,\n",
       "          0.836783  , 0.84255839, 0.84974975, 0.84323369, 0.83519779,\n",
       "          0.8480818 , 0.84219532, 0.83939562, 0.84113964, 0.84553789,\n",
       "          0.84854078, 0.85354526, 0.87987617, 0.8520885 , 0.86692852,\n",
       "          0.860474  , 0.81300239, 0.85445694, 0.85574635, 0.84696664,\n",
       "          0.84835614, 0.8428277 , 0.84176244, 0.84099457, 0.8404566 ,\n",
       "          0.806655  , 0.84336168, 0.81478674, 0.84644267, 0.84549494,\n",
       "          0.82089988, 0.82199298, 0.82350646, 0.82852991, 0.84750066,\n",
       "          0.8486422 , 0.85127436, 0.8537029 , 0.84843639, 0.83986768,\n",
       "          0.84488601, 0.84624533, 0.84631747, 0.84959712, 0.84778781,\n",
       "          0.84874079, 0.85908136, 0.82076869, 0.85006449, 0.85494469,\n",
       "          0.85205154, 0.85122268, 0.84122326, 0.82269534, 0.83326243,\n",
       "          0.83902856, 0.84239644, 0.84578061, 0.8388232 , 0.8190842 ,\n",
       "          0.84069699, 0.83565564, 0.84192879, 0.84314346, 0.83655878,\n",
       "          0.83761926, 0.79073747, 0.8394708 , 0.82615351, 0.81082912,\n",
       "          0.80297158, 0.80472992, 0.8166659 , 0.83751019, 0.84220911,\n",
       "          0.84084928, 0.84191354, 0.83506093, 0.83752032, 0.81183483,\n",
       "          0.85039928, 0.83587861, 0.8422129 , 0.83894618, 0.84629179,\n",
       "          0.8403718 , 0.83251715, 0.84234449, 0.85903615, 0.84749886,\n",
       "          0.85370586, 0.85422838, 0.8501027 , 0.84377976, 0.86287864,\n",
       "          0.83859586, 0.84432014, 0.82280085, 0.83958531, 0.83769545,\n",
       "          0.83653778, 0.84266657, 0.83807776, 0.83490248, 0.84063691,\n",
       "          0.79503411, 0.82493494, 0.81286022, 0.83024158, 0.83389306,\n",
       "          0.8361147 , 0.83775361, 0.83095332, 0.83197646, 0.8439034 ,\n",
       "          0.84555269, 0.84255116, 0.84964392, 0.84417707, 0.84707863,\n",
       "          0.82970327, 0.80775519, 0.83508336, 0.83248234, 0.82531046,\n",
       "          0.82098255, 0.82391237, 0.82870889, 0.82498401, 0.83157391,\n",
       "          0.81860658, 0.82720473, 0.83153931, 0.82281805, 0.83496258])]}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputvals_contd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.0095215], dtype=float32),\n",
       " array([0.00503225], dtype=float32),\n",
       " array([0.00431049], dtype=float32),\n",
       " array([0.00384206], dtype=float32),\n",
       " array([0.00327061], dtype=float32),\n",
       " array([0.00284683], dtype=float32),\n",
       " array([0.00240302], dtype=float32),\n",
       " array([0.00213781], dtype=float32),\n",
       " array([0.00199263], dtype=float32),\n",
       " array([0.00175735], dtype=float32),\n",
       " array([0.00147505], dtype=float32),\n",
       " array([0.00134845], dtype=float32),\n",
       " array([0.00122727], dtype=float32),\n",
       " array([0.00112665], dtype=float32),\n",
       " array([0.00115904], dtype=float32),\n",
       " array([0.00138941], dtype=float32),\n",
       " array([0.00120968], dtype=float32),\n",
       " array([0.00105451], dtype=float32),\n",
       " array([0.00106463], dtype=float32),\n",
       " array([0.000921], dtype=float32),\n",
       " array([0.00098539], dtype=float32),\n",
       " array([0.00100998], dtype=float32),\n",
       " array([0.00056352], dtype=float32),\n",
       " array([0.00085802], dtype=float32),\n",
       " array([0.0003999], dtype=float32),\n",
       " array([0.00045191], dtype=float32),\n",
       " array([0.00048713], dtype=float32),\n",
       " array([0.00052725], dtype=float32),\n",
       " array([0.00065487], dtype=float32),\n",
       " array([0.00040087], dtype=float32),\n",
       " array([0.00048624], dtype=float32),\n",
       " array([0.00085401], dtype=float32),\n",
       " array([0.00036478], dtype=float32),\n",
       " array([0.00028083], dtype=float32),\n",
       " array([0.00040782], dtype=float32),\n",
       " array([0.00059448], dtype=float32),\n",
       " array([0.0002393], dtype=float32),\n",
       " array([0.00028559], dtype=float32),\n",
       " array([0.00034388], dtype=float32),\n",
       " array([0.00045985], dtype=float32),\n",
       " array([0.00033655], dtype=float32),\n",
       " array([0.00017728], dtype=float32),\n",
       " array([0.00049376], dtype=float32),\n",
       " array([0.00043621], dtype=float32),\n",
       " array([0.00020561], dtype=float32),\n",
       " array([0.0003959], dtype=float32),\n",
       " array([0.00030522], dtype=float32),\n",
       " array([0.00033478], dtype=float32),\n",
       " array([0.00021355], dtype=float32),\n",
       " array([0.00033244], dtype=float32),\n",
       " array([0.00018328], dtype=float32),\n",
       " array([0.00032121], dtype=float32),\n",
       " array([0.00034473], dtype=float32),\n",
       " array([0.00021301], dtype=float32),\n",
       " array([9.708891e-05], dtype=float32),\n",
       " array([9.477453e-05], dtype=float32),\n",
       " array([0.0004677], dtype=float32),\n",
       " array([0.00057659], dtype=float32),\n",
       " array([0.00044137], dtype=float32),\n",
       " array([0.00016033], dtype=float32),\n",
       " array([8.9425936e-05], dtype=float32),\n",
       " array([8.883979e-05], dtype=float32),\n",
       " array([9.01739e-05], dtype=float32),\n",
       " array([0.00046173], dtype=float32),\n",
       " array([0.00045723], dtype=float32),\n",
       " array([0.00037591], dtype=float32),\n",
       " array([0.00017954], dtype=float32),\n",
       " array([0.00025155], dtype=float32),\n",
       " array([0.00019139], dtype=float32),\n",
       " array([0.00028827], dtype=float32),\n",
       " array([0.00027893], dtype=float32),\n",
       " array([9.866117e-05], dtype=float32),\n",
       " array([9.039249e-05], dtype=float32),\n",
       " array([8.998805e-05], dtype=float32),\n",
       " array([9.728803e-05], dtype=float32),\n",
       " array([0.00068237], dtype=float32),\n",
       " array([0.00030328], dtype=float32),\n",
       " array([0.00013114], dtype=float32),\n",
       " array([9.4955634e-05], dtype=float32),\n",
       " array([9.68523e-05], dtype=float32),\n",
       " array([8.717062e-05], dtype=float32),\n",
       " array([0.00046846], dtype=float32),\n",
       " array([0.00032892], dtype=float32),\n",
       " array([0.00032195], dtype=float32),\n",
       " array([0.00018322], dtype=float32),\n",
       " array([8.537915e-05], dtype=float32),\n",
       " array([0.00027233], dtype=float32),\n",
       " array([0.0001858], dtype=float32),\n",
       " array([0.00019575], dtype=float32),\n",
       " array([9.174855e-05], dtype=float32),\n",
       " array([8.792445e-05], dtype=float32),\n",
       " array([0.00041992], dtype=float32),\n",
       " array([0.00057888], dtype=float32),\n",
       " array([0.00019234], dtype=float32),\n",
       " array([9.1626614e-05], dtype=float32),\n",
       " array([9.128757e-05], dtype=float32),\n",
       " array([8.813628e-05], dtype=float32),\n",
       " array([8.315936e-05], dtype=float32),\n",
       " array([9.4790514e-05], dtype=float32),\n",
       " array([0.00035602], dtype=float32),\n",
       " array([0.00057983], dtype=float32),\n",
       " array([0.00022865], dtype=float32),\n",
       " array([0.00020132], dtype=float32),\n",
       " array([0.00020445], dtype=float32),\n",
       " array([0.0001803], dtype=float32),\n",
       " array([0.00034422], dtype=float32),\n",
       " array([0.00013704], dtype=float32),\n",
       " array([8.827323e-05], dtype=float32),\n",
       " array([9.0870744e-05], dtype=float32),\n",
       " array([9.221383e-05], dtype=float32),\n",
       " array([9.2023925e-05], dtype=float32),\n",
       " array([8.6806576e-05], dtype=float32),\n",
       " array([0.0001011], dtype=float32),\n",
       " array([0.00082137], dtype=float32),\n",
       " array([0.00025996], dtype=float32),\n",
       " array([9.729927e-05], dtype=float32),\n",
       " array([0.00027509], dtype=float32),\n",
       " array([0.00044693], dtype=float32),\n",
       " array([0.00071697], dtype=float32),\n",
       " array([0.00061436], dtype=float32),\n",
       " array([0.00021165], dtype=float32),\n",
       " array([0.00015292], dtype=float32),\n",
       " array([9.012702e-05], dtype=float32),\n",
       " array([8.4250314e-05], dtype=float32),\n",
       " array([0.00013682], dtype=float32),\n",
       " array([0.00053448], dtype=float32),\n",
       " array([9.14275e-05], dtype=float32),\n",
       " array([8.722103e-05], dtype=float32),\n",
       " array([8.678796e-05], dtype=float32),\n",
       " array([8.6893444e-05], dtype=float32),\n",
       " array([8.873664e-05], dtype=float32),\n",
       " array([8.818633e-05], dtype=float32),\n",
       " array([9.970537e-05], dtype=float32),\n",
       " array([0.00108782], dtype=float32),\n",
       " array([0.00013303], dtype=float32),\n",
       " array([9.368278e-05], dtype=float32),\n",
       " array([8.54575e-05], dtype=float32),\n",
       " array([0.00021623], dtype=float32),\n",
       " array([0.00042335], dtype=float32),\n",
       " array([0.00013393], dtype=float32),\n",
       " array([0.00026407], dtype=float32),\n",
       " array([0.00015766], dtype=float32),\n",
       " array([0.00054196], dtype=float32),\n",
       " array([0.00020154], dtype=float32),\n",
       " array([0.00018819], dtype=float32),\n",
       " array([0.00020972], dtype=float32),\n",
       " array([0.00019792], dtype=float32),\n",
       " array([0.00018945], dtype=float32),\n",
       " array([0.00030168], dtype=float32),\n",
       " array([0.0006822], dtype=float32),\n",
       " array([0.00028231], dtype=float32),\n",
       " array([0.00021806], dtype=float32),\n",
       " array([0.0002106], dtype=float32),\n",
       " array([0.00021102], dtype=float32),\n",
       " array([0.00020909], dtype=float32),\n",
       " array([0.00021357], dtype=float32),\n",
       " array([0.00021286], dtype=float32),\n",
       " array([0.00092626], dtype=float32),\n",
       " array([0.00020507], dtype=float32),\n",
       " array([0.00023236], dtype=float32),\n",
       " array([8.88784e-05], dtype=float32),\n",
       " array([8.098816e-05], dtype=float32),\n",
       " array([0.0001616], dtype=float32),\n",
       " array([0.00021391], dtype=float32),\n",
       " array([0.00047705], dtype=float32),\n",
       " array([0.00019604], dtype=float32),\n",
       " array([0.00015238], dtype=float32),\n",
       " array([0.0001494], dtype=float32),\n",
       " array([9.814515e-05], dtype=float32),\n",
       " array([0.0001683], dtype=float32),\n",
       " array([0.00013691], dtype=float32),\n",
       " array([0.00037584], dtype=float32),\n",
       " array([0.00019721], dtype=float32),\n",
       " array([0.00019116], dtype=float32),\n",
       " array([0.00018385], dtype=float32),\n",
       " array([0.00017423], dtype=float32),\n",
       " array([0.0003197], dtype=float32),\n",
       " array([0.00037452], dtype=float32),\n",
       " array([0.00019211], dtype=float32),\n",
       " array([0.00013899], dtype=float32),\n",
       " array([0.00013113], dtype=float32),\n",
       " array([0.00027175], dtype=float32),\n",
       " array([0.00041329], dtype=float32),\n",
       " array([0.00028389], dtype=float32),\n",
       " array([0.00039731], dtype=float32),\n",
       " array([0.0001488], dtype=float32),\n",
       " array([0.00013185], dtype=float32),\n",
       " array([0.00012832], dtype=float32),\n",
       " array([0.00013777], dtype=float32),\n",
       " array([0.000132], dtype=float32),\n",
       " array([0.00026434], dtype=float32),\n",
       " array([0.00019312], dtype=float32),\n",
       " array([0.00030071], dtype=float32),\n",
       " array([0.00011159], dtype=float32),\n",
       " array([0.00016236], dtype=float32),\n",
       " array([0.00034385], dtype=float32),\n",
       " array([0.00015939], dtype=float32),\n",
       " array([0.00043147], dtype=float32),\n",
       " array([0.00022379], dtype=float32),\n",
       " array([0.00021313], dtype=float32),\n",
       " array([0.0001346], dtype=float32),\n",
       " array([0.00012559], dtype=float32),\n",
       " array([0.00012747], dtype=float32),\n",
       " array([0.0001316], dtype=float32),\n",
       " array([0.00058235], dtype=float32),\n",
       " array([0.00010995], dtype=float32),\n",
       " array([0.00010508], dtype=float32),\n",
       " array([0.00011202], dtype=float32),\n",
       " array([0.00034103], dtype=float32),\n",
       " array([0.00017161], dtype=float32),\n",
       " array([0.00017965], dtype=float32),\n",
       " array([8.787254e-05], dtype=float32),\n",
       " array([0.00018265], dtype=float32),\n",
       " array([0.00015705], dtype=float32),\n",
       " array([9.101959e-05], dtype=float32),\n",
       " array([8.180408e-05], dtype=float32),\n",
       " array([8.7868524e-05], dtype=float32),\n",
       " array([0.0002909], dtype=float32),\n",
       " array([9.5109746e-05], dtype=float32),\n",
       " array([0.00033085], dtype=float32),\n",
       " array([0.00013833], dtype=float32),\n",
       " array([0.00010466], dtype=float32),\n",
       " array([0.00010322], dtype=float32),\n",
       " array([0.00011741], dtype=float32),\n",
       " array([0.00025429], dtype=float32),\n",
       " array([0.00046185], dtype=float32),\n",
       " array([8.670516e-05], dtype=float32),\n",
       " array([8.604908e-05], dtype=float32),\n",
       " array([7.9898404e-05], dtype=float32),\n",
       " array([0.0001062], dtype=float32),\n",
       " array([0.00029357], dtype=float32),\n",
       " array([0.00022836], dtype=float32),\n",
       " array([0.00044564], dtype=float32),\n",
       " array([0.00018744], dtype=float32),\n",
       " array([0.00012488], dtype=float32),\n",
       " array([0.00010196], dtype=float32),\n",
       " array([0.00012189], dtype=float32),\n",
       " array([0.00053553], dtype=float32),\n",
       " array([0.00043625], dtype=float32),\n",
       " array([0.00018743], dtype=float32),\n",
       " array([0.00016409], dtype=float32),\n",
       " array([0.00017254], dtype=float32),\n",
       " array([0.00015181], dtype=float32),\n",
       " array([0.00018897], dtype=float32),\n",
       " array([0.00015805], dtype=float32),\n",
       " array([0.0004576], dtype=float32),\n",
       " array([0.00017706], dtype=float32),\n",
       " array([0.00070505], dtype=float32),\n",
       " array([0.00080209], dtype=float32),\n",
       " array([0.00089532], dtype=float32),\n",
       " array([0.00023597], dtype=float32),\n",
       " array([0.00014424], dtype=float32),\n",
       " array([0.00040504], dtype=float32),\n",
       " array([0.00040548], dtype=float32),\n",
       " array([0.00045441], dtype=float32),\n",
       " array([0.000386], dtype=float32),\n",
       " array([0.00029828], dtype=float32),\n",
       " array([0.00025503], dtype=float32),\n",
       " array([0.00036545], dtype=float32),\n",
       " array([0.00018193], dtype=float32),\n",
       " array([0.00019201], dtype=float32),\n",
       " array([0.00024342], dtype=float32),\n",
       " array([0.00030325], dtype=float32),\n",
       " array([0.00012884], dtype=float32),\n",
       " array([0.00016786], dtype=float32),\n",
       " array([0.00014124], dtype=float32),\n",
       " array([0.00024988], dtype=float32),\n",
       " array([0.00018299], dtype=float32),\n",
       " array([0.0001173], dtype=float32),\n",
       " array([0.00012178], dtype=float32),\n",
       " array([0.00027691], dtype=float32),\n",
       " array([0.00027307], dtype=float32),\n",
       " array([0.00038264], dtype=float32),\n",
       " array([0.00017524], dtype=float32),\n",
       " array([8.769739e-05], dtype=float32),\n",
       " array([0.00010119], dtype=float32),\n",
       " array([0.00010163], dtype=float32),\n",
       " array([0.00057921], dtype=float32),\n",
       " array([9.5935655e-05], dtype=float32),\n",
       " array([9.861149e-05], dtype=float32),\n",
       " array([7.83083e-05], dtype=float32),\n",
       " array([8.081281e-05], dtype=float32),\n",
       " array([7.6994336e-05], dtype=float32),\n",
       " array([8.785709e-05], dtype=float32),\n",
       " array([8.539379e-05], dtype=float32),\n",
       " array([0.00015632], dtype=float32),\n",
       " array([0.00145919], dtype=float32),\n",
       " array([0.00083791], dtype=float32),\n",
       " array([0.00059577], dtype=float32),\n",
       " array([0.00041528], dtype=float32),\n",
       " array([0.00041756], dtype=float32),\n",
       " array([0.00042003], dtype=float32),\n",
       " array([0.00039715], dtype=float32),\n",
       " array([0.00039285], dtype=float32),\n",
       " array([0.00039505], dtype=float32),\n",
       " array([0.00040475], dtype=float32),\n",
       " array([0.00057399], dtype=float32),\n",
       " array([0.00029214], dtype=float32),\n",
       " array([0.0004517], dtype=float32),\n",
       " array([0.00023036], dtype=float32)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputvals['exonet_CV4_201_mirror_Big']['loss_train_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.0095215], dtype=float32),\n",
       " array([0.00503225], dtype=float32),\n",
       " array([0.00431049], dtype=float32),\n",
       " array([0.00384206], dtype=float32),\n",
       " array([0.00327061], dtype=float32),\n",
       " array([0.00284683], dtype=float32),\n",
       " array([0.00240302], dtype=float32),\n",
       " array([0.00213781], dtype=float32),\n",
       " array([0.00199263], dtype=float32),\n",
       " array([0.00175735], dtype=float32),\n",
       " array([0.00147505], dtype=float32),\n",
       " array([0.00134845], dtype=float32),\n",
       " array([0.00122727], dtype=float32),\n",
       " array([0.00112665], dtype=float32),\n",
       " array([0.00115904], dtype=float32),\n",
       " array([0.00138941], dtype=float32),\n",
       " array([0.00120968], dtype=float32),\n",
       " array([0.00105451], dtype=float32),\n",
       " array([0.00106463], dtype=float32),\n",
       " array([0.000921], dtype=float32),\n",
       " array([0.00098539], dtype=float32),\n",
       " array([0.00100998], dtype=float32),\n",
       " array([0.00056352], dtype=float32),\n",
       " array([0.00085802], dtype=float32),\n",
       " array([0.0003999], dtype=float32),\n",
       " array([0.00045191], dtype=float32),\n",
       " array([0.00048713], dtype=float32),\n",
       " array([0.00052725], dtype=float32),\n",
       " array([0.00065487], dtype=float32),\n",
       " array([0.00040087], dtype=float32),\n",
       " array([0.00048624], dtype=float32),\n",
       " array([0.00085401], dtype=float32),\n",
       " array([0.00036478], dtype=float32),\n",
       " array([0.00028083], dtype=float32),\n",
       " array([0.00040782], dtype=float32),\n",
       " array([0.00059448], dtype=float32),\n",
       " array([0.0002393], dtype=float32),\n",
       " array([0.00028559], dtype=float32),\n",
       " array([0.00034388], dtype=float32),\n",
       " array([0.00045985], dtype=float32),\n",
       " array([0.00033655], dtype=float32),\n",
       " array([0.00017728], dtype=float32),\n",
       " array([0.00049376], dtype=float32),\n",
       " array([0.00043621], dtype=float32),\n",
       " array([0.00020561], dtype=float32),\n",
       " array([0.0003959], dtype=float32),\n",
       " array([0.00030522], dtype=float32),\n",
       " array([0.00033478], dtype=float32),\n",
       " array([0.00021355], dtype=float32),\n",
       " array([0.00033244], dtype=float32),\n",
       " array([0.00018328], dtype=float32),\n",
       " array([0.00032121], dtype=float32),\n",
       " array([0.00034473], dtype=float32),\n",
       " array([0.00021301], dtype=float32),\n",
       " array([9.708891e-05], dtype=float32),\n",
       " array([9.477453e-05], dtype=float32),\n",
       " array([0.0004677], dtype=float32),\n",
       " array([0.00057659], dtype=float32),\n",
       " array([0.00044137], dtype=float32),\n",
       " array([0.00016033], dtype=float32),\n",
       " array([8.9425936e-05], dtype=float32),\n",
       " array([8.883979e-05], dtype=float32),\n",
       " array([9.01739e-05], dtype=float32),\n",
       " array([0.00046173], dtype=float32),\n",
       " array([0.00045723], dtype=float32),\n",
       " array([0.00037591], dtype=float32),\n",
       " array([0.00017954], dtype=float32),\n",
       " array([0.00025155], dtype=float32),\n",
       " array([0.00019139], dtype=float32),\n",
       " array([0.00028827], dtype=float32),\n",
       " array([0.00027893], dtype=float32),\n",
       " array([9.866117e-05], dtype=float32),\n",
       " array([9.039249e-05], dtype=float32),\n",
       " array([8.998805e-05], dtype=float32),\n",
       " array([9.728803e-05], dtype=float32),\n",
       " array([0.00068237], dtype=float32),\n",
       " array([0.00030328], dtype=float32),\n",
       " array([0.00013114], dtype=float32),\n",
       " array([9.4955634e-05], dtype=float32),\n",
       " array([9.68523e-05], dtype=float32),\n",
       " array([8.717062e-05], dtype=float32),\n",
       " array([0.00046846], dtype=float32),\n",
       " array([0.00032892], dtype=float32),\n",
       " array([0.00032195], dtype=float32),\n",
       " array([0.00018322], dtype=float32),\n",
       " array([8.537915e-05], dtype=float32),\n",
       " array([0.00027233], dtype=float32),\n",
       " array([0.0001858], dtype=float32),\n",
       " array([0.00019575], dtype=float32),\n",
       " array([9.174855e-05], dtype=float32),\n",
       " array([8.792445e-05], dtype=float32),\n",
       " array([0.00041992], dtype=float32),\n",
       " array([0.00057888], dtype=float32),\n",
       " array([0.00019234], dtype=float32),\n",
       " array([9.1626614e-05], dtype=float32),\n",
       " array([9.128757e-05], dtype=float32),\n",
       " array([8.813628e-05], dtype=float32),\n",
       " array([8.315936e-05], dtype=float32),\n",
       " array([9.4790514e-05], dtype=float32),\n",
       " array([0.00035602], dtype=float32),\n",
       " array([0.00057983], dtype=float32),\n",
       " array([0.00022865], dtype=float32),\n",
       " array([0.00020132], dtype=float32),\n",
       " array([0.00020445], dtype=float32),\n",
       " array([0.0001803], dtype=float32),\n",
       " array([0.00034422], dtype=float32),\n",
       " array([0.00013704], dtype=float32),\n",
       " array([8.827323e-05], dtype=float32),\n",
       " array([9.0870744e-05], dtype=float32),\n",
       " array([9.221383e-05], dtype=float32),\n",
       " array([9.2023925e-05], dtype=float32),\n",
       " array([8.6806576e-05], dtype=float32),\n",
       " array([0.0001011], dtype=float32),\n",
       " array([0.00082137], dtype=float32),\n",
       " array([0.00025996], dtype=float32),\n",
       " array([9.729927e-05], dtype=float32),\n",
       " array([0.00027509], dtype=float32),\n",
       " array([0.00044693], dtype=float32),\n",
       " array([0.00071697], dtype=float32),\n",
       " array([0.00061436], dtype=float32),\n",
       " array([0.00021165], dtype=float32),\n",
       " array([0.00015292], dtype=float32),\n",
       " array([9.012702e-05], dtype=float32),\n",
       " array([8.4250314e-05], dtype=float32),\n",
       " array([0.00013682], dtype=float32),\n",
       " array([0.00053448], dtype=float32),\n",
       " array([9.14275e-05], dtype=float32),\n",
       " array([8.722103e-05], dtype=float32),\n",
       " array([8.678796e-05], dtype=float32),\n",
       " array([8.6893444e-05], dtype=float32),\n",
       " array([8.873664e-05], dtype=float32),\n",
       " array([8.818633e-05], dtype=float32),\n",
       " array([9.970537e-05], dtype=float32),\n",
       " array([0.00108782], dtype=float32),\n",
       " array([0.00013303], dtype=float32),\n",
       " array([9.368278e-05], dtype=float32),\n",
       " array([8.54575e-05], dtype=float32),\n",
       " array([0.00021623], dtype=float32),\n",
       " array([0.00042335], dtype=float32),\n",
       " array([0.00013393], dtype=float32),\n",
       " array([0.00026407], dtype=float32),\n",
       " array([0.00015766], dtype=float32),\n",
       " array([0.00054196], dtype=float32),\n",
       " array([0.00020154], dtype=float32),\n",
       " array([0.00018819], dtype=float32),\n",
       " array([0.00020972], dtype=float32),\n",
       " array([0.00019792], dtype=float32),\n",
       " array([0.00018945], dtype=float32),\n",
       " array([0.00030168], dtype=float32),\n",
       " array([0.0006822], dtype=float32),\n",
       " array([0.00028231], dtype=float32),\n",
       " array([0.00021806], dtype=float32),\n",
       " array([0.0002106], dtype=float32),\n",
       " array([0.00021102], dtype=float32),\n",
       " array([0.00020909], dtype=float32),\n",
       " array([0.00021357], dtype=float32),\n",
       " array([0.00021286], dtype=float32),\n",
       " array([0.00092626], dtype=float32),\n",
       " array([0.00020507], dtype=float32),\n",
       " array([0.00023236], dtype=float32),\n",
       " array([8.88784e-05], dtype=float32),\n",
       " array([8.098816e-05], dtype=float32),\n",
       " array([0.0001616], dtype=float32),\n",
       " array([0.00021391], dtype=float32),\n",
       " array([0.00047705], dtype=float32),\n",
       " array([0.00019604], dtype=float32),\n",
       " array([0.00015238], dtype=float32),\n",
       " array([0.0001494], dtype=float32),\n",
       " array([9.814515e-05], dtype=float32),\n",
       " array([0.0001683], dtype=float32),\n",
       " array([0.00013691], dtype=float32),\n",
       " array([0.00037584], dtype=float32),\n",
       " array([0.00019721], dtype=float32),\n",
       " array([0.00019116], dtype=float32),\n",
       " array([0.00018385], dtype=float32),\n",
       " array([0.00017423], dtype=float32),\n",
       " array([0.0003197], dtype=float32),\n",
       " array([0.00037452], dtype=float32),\n",
       " array([0.00019211], dtype=float32),\n",
       " array([0.00013899], dtype=float32),\n",
       " array([0.00013113], dtype=float32),\n",
       " array([0.00027175], dtype=float32),\n",
       " array([0.00041329], dtype=float32),\n",
       " array([0.00028389], dtype=float32),\n",
       " array([0.00039731], dtype=float32),\n",
       " array([0.0001488], dtype=float32),\n",
       " array([0.00013185], dtype=float32),\n",
       " array([0.00012832], dtype=float32),\n",
       " array([0.00013777], dtype=float32),\n",
       " array([0.000132], dtype=float32),\n",
       " array([0.00026434], dtype=float32),\n",
       " array([0.00019312], dtype=float32),\n",
       " array([0.00030071], dtype=float32),\n",
       " array([0.00011159], dtype=float32),\n",
       " array([0.00016236], dtype=float32),\n",
       " array([0.00034385], dtype=float32),\n",
       " array([0.00015939], dtype=float32),\n",
       " array([0.00043147], dtype=float32),\n",
       " array([0.00022379], dtype=float32),\n",
       " array([0.00021313], dtype=float32),\n",
       " array([0.0001346], dtype=float32),\n",
       " array([0.00012559], dtype=float32),\n",
       " array([0.00012747], dtype=float32),\n",
       " array([0.0001316], dtype=float32),\n",
       " array([0.00058235], dtype=float32),\n",
       " array([0.00010995], dtype=float32),\n",
       " array([0.00010508], dtype=float32),\n",
       " array([0.00011202], dtype=float32),\n",
       " array([0.00034103], dtype=float32),\n",
       " array([0.00017161], dtype=float32),\n",
       " array([0.00017965], dtype=float32),\n",
       " array([8.787254e-05], dtype=float32),\n",
       " array([0.00018265], dtype=float32),\n",
       " array([0.00015705], dtype=float32),\n",
       " array([9.101959e-05], dtype=float32),\n",
       " array([8.180408e-05], dtype=float32),\n",
       " array([8.7868524e-05], dtype=float32),\n",
       " array([0.0002909], dtype=float32),\n",
       " array([9.5109746e-05], dtype=float32),\n",
       " array([0.00033085], dtype=float32),\n",
       " array([0.00013833], dtype=float32),\n",
       " array([0.00010466], dtype=float32),\n",
       " array([0.00010322], dtype=float32),\n",
       " array([0.00011741], dtype=float32),\n",
       " array([0.00025429], dtype=float32),\n",
       " array([0.00046185], dtype=float32),\n",
       " array([8.670516e-05], dtype=float32),\n",
       " array([8.604908e-05], dtype=float32),\n",
       " array([7.9898404e-05], dtype=float32),\n",
       " array([0.0001062], dtype=float32),\n",
       " array([0.00029357], dtype=float32),\n",
       " array([0.00022836], dtype=float32),\n",
       " array([0.00044564], dtype=float32),\n",
       " array([0.00018744], dtype=float32),\n",
       " array([0.00012488], dtype=float32),\n",
       " array([0.00010196], dtype=float32),\n",
       " array([0.00012189], dtype=float32),\n",
       " array([0.00053553], dtype=float32),\n",
       " array([0.00043625], dtype=float32),\n",
       " array([0.00018743], dtype=float32),\n",
       " array([0.00016409], dtype=float32),\n",
       " array([0.00017254], dtype=float32),\n",
       " array([0.00015181], dtype=float32),\n",
       " array([0.00018897], dtype=float32),\n",
       " array([0.00015805], dtype=float32),\n",
       " array([0.0004576], dtype=float32),\n",
       " array([0.00017706], dtype=float32),\n",
       " array([0.00070505], dtype=float32),\n",
       " array([0.00080209], dtype=float32),\n",
       " array([0.00089532], dtype=float32),\n",
       " array([0.00023597], dtype=float32),\n",
       " array([0.00014424], dtype=float32),\n",
       " array([0.00040504], dtype=float32),\n",
       " array([0.00040548], dtype=float32),\n",
       " array([0.00045441], dtype=float32),\n",
       " array([0.000386], dtype=float32),\n",
       " array([0.00029828], dtype=float32),\n",
       " array([0.00025503], dtype=float32),\n",
       " array([0.00036545], dtype=float32),\n",
       " array([0.00018193], dtype=float32),\n",
       " array([0.00019201], dtype=float32),\n",
       " array([0.00024342], dtype=float32),\n",
       " array([0.00030325], dtype=float32),\n",
       " array([0.00012884], dtype=float32),\n",
       " array([0.00016786], dtype=float32),\n",
       " array([0.00014124], dtype=float32),\n",
       " array([0.00024988], dtype=float32),\n",
       " array([0.00018299], dtype=float32),\n",
       " array([0.0001173], dtype=float32),\n",
       " array([0.00012178], dtype=float32),\n",
       " array([0.00027691], dtype=float32),\n",
       " array([0.00027307], dtype=float32),\n",
       " array([0.00038264], dtype=float32),\n",
       " array([0.00017524], dtype=float32),\n",
       " array([8.769739e-05], dtype=float32),\n",
       " array([0.00010119], dtype=float32),\n",
       " array([0.00010163], dtype=float32),\n",
       " array([0.00057921], dtype=float32),\n",
       " array([9.5935655e-05], dtype=float32),\n",
       " array([9.861149e-05], dtype=float32),\n",
       " array([7.83083e-05], dtype=float32),\n",
       " array([8.081281e-05], dtype=float32),\n",
       " array([7.6994336e-05], dtype=float32),\n",
       " array([8.785709e-05], dtype=float32),\n",
       " array([8.539379e-05], dtype=float32),\n",
       " array([0.00015632], dtype=float32),\n",
       " array([0.00145919], dtype=float32),\n",
       " array([0.00083791], dtype=float32),\n",
       " array([0.00059577], dtype=float32),\n",
       " array([0.00041528], dtype=float32),\n",
       " array([0.00041756], dtype=float32),\n",
       " array([0.00042003], dtype=float32),\n",
       " array([0.00039715], dtype=float32),\n",
       " array([0.00039285], dtype=float32),\n",
       " array([0.00039505], dtype=float32),\n",
       " array([0.00040475], dtype=float32),\n",
       " array([0.00057399], dtype=float32),\n",
       " array([0.00029214], dtype=float32),\n",
       " array([0.0004517], dtype=float32),\n",
       " array([0.00023036], dtype=float32),\n",
       " array([0.0095215], dtype=float32),\n",
       " array([0.00503225], dtype=float32),\n",
       " array([0.00431049], dtype=float32),\n",
       " array([0.00384206], dtype=float32),\n",
       " array([0.00327061], dtype=float32),\n",
       " array([0.00284683], dtype=float32),\n",
       " array([0.00240302], dtype=float32),\n",
       " array([0.00213781], dtype=float32),\n",
       " array([0.00199263], dtype=float32),\n",
       " array([0.00175735], dtype=float32),\n",
       " array([0.00147505], dtype=float32),\n",
       " array([0.00134845], dtype=float32),\n",
       " array([0.00122727], dtype=float32),\n",
       " array([0.00112665], dtype=float32),\n",
       " array([0.00115904], dtype=float32),\n",
       " array([0.00138941], dtype=float32),\n",
       " array([0.00120968], dtype=float32),\n",
       " array([0.00105451], dtype=float32),\n",
       " array([0.00106463], dtype=float32),\n",
       " array([0.000921], dtype=float32),\n",
       " array([0.00098539], dtype=float32),\n",
       " array([0.00100998], dtype=float32),\n",
       " array([0.00056352], dtype=float32),\n",
       " array([0.00085802], dtype=float32),\n",
       " array([0.0003999], dtype=float32),\n",
       " array([0.00045191], dtype=float32),\n",
       " array([0.00048713], dtype=float32),\n",
       " array([0.00052725], dtype=float32),\n",
       " array([0.00065487], dtype=float32),\n",
       " array([0.00040087], dtype=float32),\n",
       " array([0.00048624], dtype=float32),\n",
       " array([0.00085401], dtype=float32),\n",
       " array([0.00036478], dtype=float32),\n",
       " array([0.00028083], dtype=float32),\n",
       " array([0.00040782], dtype=float32),\n",
       " array([0.00059448], dtype=float32),\n",
       " array([0.0002393], dtype=float32),\n",
       " array([0.00028559], dtype=float32),\n",
       " array([0.00034388], dtype=float32),\n",
       " array([0.00045985], dtype=float32),\n",
       " array([0.00033655], dtype=float32),\n",
       " array([0.00017728], dtype=float32),\n",
       " array([0.00049376], dtype=float32),\n",
       " array([0.00043621], dtype=float32),\n",
       " array([0.00020561], dtype=float32),\n",
       " array([0.0003959], dtype=float32),\n",
       " array([0.00030522], dtype=float32),\n",
       " array([0.00033478], dtype=float32),\n",
       " array([0.00021355], dtype=float32),\n",
       " array([0.00033244], dtype=float32),\n",
       " array([0.00018328], dtype=float32),\n",
       " array([0.00032121], dtype=float32),\n",
       " array([0.00034473], dtype=float32),\n",
       " array([0.00021301], dtype=float32),\n",
       " array([9.708891e-05], dtype=float32),\n",
       " array([9.477453e-05], dtype=float32),\n",
       " array([0.0004677], dtype=float32),\n",
       " array([0.00057659], dtype=float32),\n",
       " array([0.00044137], dtype=float32),\n",
       " array([0.00016033], dtype=float32),\n",
       " array([8.9425936e-05], dtype=float32),\n",
       " array([8.883979e-05], dtype=float32),\n",
       " array([9.01739e-05], dtype=float32),\n",
       " array([0.00046173], dtype=float32),\n",
       " array([0.00045723], dtype=float32),\n",
       " array([0.00037591], dtype=float32),\n",
       " array([0.00017954], dtype=float32),\n",
       " array([0.00025155], dtype=float32),\n",
       " array([0.00019139], dtype=float32),\n",
       " array([0.00028827], dtype=float32),\n",
       " array([0.00027893], dtype=float32),\n",
       " array([9.866117e-05], dtype=float32),\n",
       " array([9.039249e-05], dtype=float32),\n",
       " array([8.998805e-05], dtype=float32),\n",
       " array([9.728803e-05], dtype=float32),\n",
       " array([0.00068237], dtype=float32),\n",
       " array([0.00030328], dtype=float32),\n",
       " array([0.00013114], dtype=float32),\n",
       " array([9.4955634e-05], dtype=float32),\n",
       " array([9.68523e-05], dtype=float32),\n",
       " array([8.717062e-05], dtype=float32),\n",
       " array([0.00046846], dtype=float32),\n",
       " array([0.00032892], dtype=float32),\n",
       " array([0.00032195], dtype=float32),\n",
       " array([0.00018322], dtype=float32),\n",
       " array([8.537915e-05], dtype=float32),\n",
       " array([0.00027233], dtype=float32),\n",
       " array([0.0001858], dtype=float32),\n",
       " array([0.00019575], dtype=float32),\n",
       " array([9.174855e-05], dtype=float32),\n",
       " array([8.792445e-05], dtype=float32),\n",
       " array([0.00041992], dtype=float32),\n",
       " array([0.00057888], dtype=float32),\n",
       " array([0.00019234], dtype=float32),\n",
       " array([9.1626614e-05], dtype=float32),\n",
       " array([9.128757e-05], dtype=float32),\n",
       " array([8.813628e-05], dtype=float32),\n",
       " array([8.315936e-05], dtype=float32),\n",
       " array([9.4790514e-05], dtype=float32),\n",
       " array([0.00035602], dtype=float32),\n",
       " array([0.00057983], dtype=float32),\n",
       " array([0.00022865], dtype=float32),\n",
       " array([0.00020132], dtype=float32),\n",
       " array([0.00020445], dtype=float32),\n",
       " array([0.0001803], dtype=float32),\n",
       " array([0.00034422], dtype=float32),\n",
       " array([0.00013704], dtype=float32),\n",
       " array([8.827323e-05], dtype=float32),\n",
       " array([9.0870744e-05], dtype=float32),\n",
       " array([9.221383e-05], dtype=float32),\n",
       " array([9.2023925e-05], dtype=float32),\n",
       " array([8.6806576e-05], dtype=float32),\n",
       " array([0.0001011], dtype=float32),\n",
       " array([0.00082137], dtype=float32),\n",
       " array([0.00025996], dtype=float32),\n",
       " array([9.729927e-05], dtype=float32),\n",
       " array([0.00027509], dtype=float32),\n",
       " array([0.00044693], dtype=float32),\n",
       " array([0.00071697], dtype=float32),\n",
       " array([0.00061436], dtype=float32),\n",
       " array([0.00021165], dtype=float32),\n",
       " array([0.00015292], dtype=float32),\n",
       " array([9.012702e-05], dtype=float32),\n",
       " array([8.4250314e-05], dtype=float32),\n",
       " array([0.00013682], dtype=float32),\n",
       " array([0.00053448], dtype=float32),\n",
       " array([9.14275e-05], dtype=float32),\n",
       " array([8.722103e-05], dtype=float32),\n",
       " array([8.678796e-05], dtype=float32),\n",
       " array([8.6893444e-05], dtype=float32),\n",
       " array([8.873664e-05], dtype=float32),\n",
       " array([8.818633e-05], dtype=float32),\n",
       " array([9.970537e-05], dtype=float32),\n",
       " array([0.00108782], dtype=float32),\n",
       " array([0.00013303], dtype=float32),\n",
       " array([9.368278e-05], dtype=float32),\n",
       " array([8.54575e-05], dtype=float32),\n",
       " array([0.00021623], dtype=float32),\n",
       " array([0.00042335], dtype=float32),\n",
       " array([0.00013393], dtype=float32),\n",
       " array([0.00026407], dtype=float32),\n",
       " array([0.00015766], dtype=float32),\n",
       " array([0.00054196], dtype=float32),\n",
       " array([0.00020154], dtype=float32),\n",
       " array([0.00018819], dtype=float32),\n",
       " array([0.00020972], dtype=float32),\n",
       " array([0.00019792], dtype=float32),\n",
       " array([0.00018945], dtype=float32),\n",
       " array([0.00030168], dtype=float32),\n",
       " array([0.0006822], dtype=float32),\n",
       " array([0.00028231], dtype=float32),\n",
       " array([0.00021806], dtype=float32),\n",
       " array([0.0002106], dtype=float32),\n",
       " array([0.00021102], dtype=float32),\n",
       " array([0.00020909], dtype=float32),\n",
       " array([0.00021357], dtype=float32),\n",
       " array([0.00021286], dtype=float32),\n",
       " array([0.00092626], dtype=float32),\n",
       " array([0.00020507], dtype=float32),\n",
       " array([0.00023236], dtype=float32),\n",
       " array([8.88784e-05], dtype=float32),\n",
       " array([8.098816e-05], dtype=float32),\n",
       " array([0.0001616], dtype=float32),\n",
       " array([0.00021391], dtype=float32),\n",
       " array([0.00047705], dtype=float32),\n",
       " array([0.00019604], dtype=float32),\n",
       " array([0.00015238], dtype=float32),\n",
       " array([0.0001494], dtype=float32),\n",
       " array([9.814515e-05], dtype=float32),\n",
       " array([0.0001683], dtype=float32),\n",
       " array([0.00013691], dtype=float32),\n",
       " array([0.00037584], dtype=float32),\n",
       " array([0.00019721], dtype=float32),\n",
       " array([0.00019116], dtype=float32),\n",
       " array([0.00018385], dtype=float32),\n",
       " array([0.00017423], dtype=float32),\n",
       " array([0.0003197], dtype=float32),\n",
       " array([0.00037452], dtype=float32),\n",
       " array([0.00019211], dtype=float32),\n",
       " array([0.00013899], dtype=float32),\n",
       " array([0.00013113], dtype=float32),\n",
       " array([0.00027175], dtype=float32),\n",
       " array([0.00041329], dtype=float32),\n",
       " array([0.00028389], dtype=float32),\n",
       " array([0.00039731], dtype=float32),\n",
       " array([0.0001488], dtype=float32),\n",
       " array([0.00013185], dtype=float32),\n",
       " array([0.00012832], dtype=float32),\n",
       " array([0.00013777], dtype=float32),\n",
       " array([0.000132], dtype=float32),\n",
       " array([0.00026434], dtype=float32),\n",
       " array([0.00019312], dtype=float32),\n",
       " array([0.00030071], dtype=float32),\n",
       " array([0.00011159], dtype=float32),\n",
       " array([0.00016236], dtype=float32),\n",
       " array([0.00034385], dtype=float32),\n",
       " array([0.00015939], dtype=float32),\n",
       " array([0.00043147], dtype=float32),\n",
       " array([0.00022379], dtype=float32),\n",
       " array([0.00021313], dtype=float32),\n",
       " array([0.0001346], dtype=float32),\n",
       " array([0.00012559], dtype=float32),\n",
       " array([0.00012747], dtype=float32),\n",
       " array([0.0001316], dtype=float32),\n",
       " array([0.00058235], dtype=float32),\n",
       " array([0.00010995], dtype=float32),\n",
       " array([0.00010508], dtype=float32),\n",
       " array([0.00011202], dtype=float32),\n",
       " array([0.00034103], dtype=float32),\n",
       " array([0.00017161], dtype=float32),\n",
       " array([0.00017965], dtype=float32),\n",
       " array([8.787254e-05], dtype=float32),\n",
       " array([0.00018265], dtype=float32),\n",
       " array([0.00015705], dtype=float32),\n",
       " array([9.101959e-05], dtype=float32),\n",
       " array([8.180408e-05], dtype=float32),\n",
       " array([8.7868524e-05], dtype=float32),\n",
       " array([0.0002909], dtype=float32),\n",
       " array([9.5109746e-05], dtype=float32),\n",
       " array([0.00033085], dtype=float32),\n",
       " array([0.00013833], dtype=float32),\n",
       " array([0.00010466], dtype=float32),\n",
       " array([0.00010322], dtype=float32),\n",
       " array([0.00011741], dtype=float32),\n",
       " array([0.00025429], dtype=float32),\n",
       " array([0.00046185], dtype=float32),\n",
       " array([8.670516e-05], dtype=float32),\n",
       " array([8.604908e-05], dtype=float32),\n",
       " array([7.9898404e-05], dtype=float32),\n",
       " array([0.0001062], dtype=float32),\n",
       " array([0.00029357], dtype=float32),\n",
       " array([0.00022836], dtype=float32),\n",
       " array([0.00044564], dtype=float32),\n",
       " array([0.00018744], dtype=float32),\n",
       " array([0.00012488], dtype=float32),\n",
       " array([0.00010196], dtype=float32),\n",
       " array([0.00012189], dtype=float32),\n",
       " array([0.00053553], dtype=float32),\n",
       " array([0.00043625], dtype=float32),\n",
       " array([0.00018743], dtype=float32),\n",
       " array([0.00016409], dtype=float32),\n",
       " array([0.00017254], dtype=float32),\n",
       " array([0.00015181], dtype=float32),\n",
       " array([0.00018897], dtype=float32),\n",
       " array([0.00015805], dtype=float32),\n",
       " array([0.0004576], dtype=float32),\n",
       " array([0.00017706], dtype=float32),\n",
       " array([0.00070505], dtype=float32),\n",
       " array([0.00080209], dtype=float32),\n",
       " array([0.00089532], dtype=float32),\n",
       " array([0.00023597], dtype=float32),\n",
       " array([0.00014424], dtype=float32),\n",
       " array([0.00040504], dtype=float32),\n",
       " array([0.00040548], dtype=float32),\n",
       " array([0.00045441], dtype=float32),\n",
       " array([0.000386], dtype=float32),\n",
       " array([0.00029828], dtype=float32),\n",
       " array([0.00025503], dtype=float32),\n",
       " array([0.00036545], dtype=float32),\n",
       " array([0.00018193], dtype=float32),\n",
       " array([0.00019201], dtype=float32),\n",
       " array([0.00024342], dtype=float32),\n",
       " array([0.00030325], dtype=float32),\n",
       " array([0.00012884], dtype=float32),\n",
       " array([0.00016786], dtype=float32),\n",
       " array([0.00014124], dtype=float32),\n",
       " array([0.00024988], dtype=float32),\n",
       " array([0.00018299], dtype=float32),\n",
       " array([0.0001173], dtype=float32),\n",
       " array([0.00012178], dtype=float32),\n",
       " array([0.00027691], dtype=float32),\n",
       " array([0.00027307], dtype=float32),\n",
       " array([0.00038264], dtype=float32),\n",
       " array([0.00017524], dtype=float32),\n",
       " array([8.769739e-05], dtype=float32),\n",
       " array([0.00010119], dtype=float32),\n",
       " array([0.00010163], dtype=float32),\n",
       " array([0.00057921], dtype=float32),\n",
       " array([9.5935655e-05], dtype=float32),\n",
       " array([9.861149e-05], dtype=float32),\n",
       " array([7.83083e-05], dtype=float32),\n",
       " array([8.081281e-05], dtype=float32),\n",
       " array([7.6994336e-05], dtype=float32),\n",
       " array([8.785709e-05], dtype=float32),\n",
       " array([8.539379e-05], dtype=float32),\n",
       " array([0.00015632], dtype=float32),\n",
       " array([0.00145919], dtype=float32),\n",
       " array([0.00083791], dtype=float32),\n",
       " array([0.00059577], dtype=float32),\n",
       " array([0.00041528], dtype=float32),\n",
       " array([0.00041756], dtype=float32),\n",
       " array([0.00042003], dtype=float32),\n",
       " array([0.00039715], dtype=float32),\n",
       " array([0.00039285], dtype=float32),\n",
       " array([0.00039505], dtype=float32),\n",
       " array([0.00040475], dtype=float32),\n",
       " array([0.00057399], dtype=float32),\n",
       " array([0.00029214], dtype=float32),\n",
       " array([0.0004517], dtype=float32),\n",
       " array([0.00023036], dtype=float32)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputvals['exonet_CV4_201_mirror_Big']['loss_train_epoch']+loss_train_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exonet_CV4_101_all_XS'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.10421488], dtype=float32),\n",
       " array([0.10420503], dtype=float32),\n",
       " array([0.10462302], dtype=float32)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputvals['exonet_CV4_101_all_XS']['loss_train_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(outputvals,open(path.join(foldname,savedicname+'.pickle'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-53aaa13f0f81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred_val_final'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt_val_final'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda36/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   1590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m         warnings.warn(\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\n\u001b[1;32m   1594\u001b[0m                       \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "crit=nn.BCELoss()\n",
    "crit(outputvals[key]['pred_val_final'],outputvals[key]['gt_val_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_val_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aa409daa0b40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepoch_val_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_val_loss' is not defined"
     ]
    }
   ],
   "source": [
    "epoch_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BCELoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , ..., 0.        , 0.12614669,\n",
       "       0.11938311], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputvals[key]['pred_val_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputvals[key]['gt_val_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5187,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(gt_val_final[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced_2class_nocents_101_dv_auxdata_tess_dataset.pickle\r\n",
      "balanced_2class_nocents_101_dv_tess_dataset.pickle\r\n",
      "balanced_2class_nocents_101_tess_dataset.pickle\r\n",
      "balanced_2class_nocents_201_dv_tess_dataset.pickle\r\n",
      "balanced_2class_nocents_tess_dataset.pickle\r\n",
      "balanced_2class_tess_dataset.pickle\r\n",
      "exonet_101_none_XS.pickle\r\n",
      "exonet_101.pickle\r\n",
      "exonet_201.pickle\r\n",
      "exonet_3class_101.pickle\r\n",
      "exonet_3class_201.pickle\r\n",
      "output_dict_multi3_exonet.pickle\r\n"
     ]
    }
   ],
   "source": [
    "ls *.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output_dict_binary_e'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-139f674a0008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutputvals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output_dict_binary_e'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output_dict_binary_e'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "outputvals=pickle.load(open('output_dict_binary_e','rb'))\n",
    "from IPython.display import Image\n",
    "import IPython.display as dp\n",
    "\n",
    "images = []\n",
    "for runname in outputvals:\n",
    "    images.append(dp.Image(filename=runname+\".png\", format='png'))\n",
    "\n",
    "# display all images\n",
    "for n in range(len(images)):\n",
    "    ea=images[n]\n",
    "    dp.display_png(ea)\n",
    "    print(list(outputvals.keys())[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize model; cuda puts it on GPU\n",
    "model = ExtranetModel_101().cuda()\n",
    "\n",
    "### optimizer for learning\n",
    "lr = 1.5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "### loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "### batch size\n",
    "batch_size = 64\n",
    "\n",
    "### number of epochs\n",
    "n_epochs = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### grab data\n",
    "kepler_train_data = KeplerDataLoader(filepath='/home/hugh/processed_dv_101/train')\n",
    "kepler_val_data = KeplerDataLoader(filepath='/home/hugh/processed_dv_101/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import pickle\n",
    "fname='balanced_2class_nocents_101_dv_tess_dataset.pickle'\n",
    "if path.exists(fname):\n",
    "    kepler_batch_sampler = pickle.load(open(fname,'rb'))\n",
    "else:\n",
    "    kepler_batch_sampler = BalancedBatchSampler(batch_size, kepler_train_data, 2) #batch_size, dataset, n_classes\n",
    "    pickle.dump(kepler_batch_sampler, open(fname,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data\n",
    "kepler_data_loader = DataLoader(kepler_train_data, batch_sampler = kepler_batch_sampler, num_workers=4)\n",
    "kepler_val_loader = DataLoader(kepler_val_data, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_train_data, y_train in kepler_data_loader:\n",
    "\n",
    "    ### get local view, global view, and label for training\n",
    "    x_train_local, x_train_global, x_train_star = x_train_data\n",
    "\n",
    "    x_train_local = Variable(x_train_local).type(torch.FloatTensor).cuda()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 101)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(local_stds[:roundn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3104949 , 0.19332148, 0.16571361, 0.30263424, 0.2568697 ,\n",
       "       0.23017159, 0.22420129, 0.28905296, 0.18179056, 0.19634628,\n",
       "       0.21672797, 0.21053435, 0.253703  , 0.24512766, 0.18526301,\n",
       "       0.19354528, 0.20515218, 0.21828526, 0.25526178, 0.2778623 ,\n",
       "       0.22231571, 0.23471211, 0.24181873, 0.17698386, 0.2223023 ,\n",
       "       0.21134433, 0.22418943, 0.20619434, 0.23044309, 0.2860098 ,\n",
       "       0.24011365, 0.23271301, 0.23681004, 0.21058494, 0.27523062,\n",
       "       0.29131085, 0.2735954 , 0.32620978, 0.2716096 , 0.31779388,\n",
       "       0.22407229, 0.29636604, 0.2558426 , 0.32408857, 0.25012487,\n",
       "       0.24762379, 0.27158225, 0.24765004, 0.26510918, 0.301274  ,\n",
       "       0.3238664 , 0.23465028, 0.24234408, 0.33868286, 0.21578245,\n",
       "       0.17494325, 0.1778552 , 0.20382433, 0.30648482, 0.26350126,\n",
       "       0.2828583 , 0.2029524 , 0.2636463 , 0.20098051, 0.23086424,\n",
       "       0.23520589, 0.2701445 , 0.30773053, 0.27167425, 0.24339308],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roundn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-df62ac85e611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_stds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mroundn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "np.shape(local_stds[:roundn,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train model\n",
    "loss_train_epoch, loss_val_epoch, acc_val_epoch, ap_val_epoch, pred_val_final, gt_val_final  = train_model(n_epochs, kepler_data_loader, kepler_val_loader, model, criterion, optimizer)\n",
    "\n",
    "### transform from loss per sample to loss per batch (multiple by batch size to compare to Chris')\n",
    "loss_train_batch = [x.item()* batch_size for x in loss_train_epoch]\n",
    "loss_val_batch = [x.item()* batch_size for x in loss_val_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in kepler_data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(14,14))\n",
    "for n in range(64):\n",
    "    print(np.sum(np.isnan(data[0][0][n].cpu().numpy())),np.sum(np.isnan(data[0][1][n].cpu().numpy())),np.sum(np.isnan(data[0][2][n].cpu().numpy())))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(n+data[0][0][n].cpu().numpy(),'.',color='C'+str(int(data[1][n])),alpha=0.66)\n",
    "    plt.subplot(122)\n",
    "    plt.plot(n+data[0][1][n].cpu().numpy(),'.',color='C'+str(int(data[1][n])),alpha=0.66)\n",
    "for n in range(1,3):\n",
    "    plt.subplot(1,2,n)\n",
    "    plt.ylim(-2,65)\n",
    "plt.tight_layout()\n",
    "plt.savefig('InputExamples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.subplot(211)\n",
    "plt.plot(loss_val_batch[1:])\n",
    "plt.subplot(212)\n",
    "plt.plot(loss_train_batch[1:])\n",
    "plt.xlim(0,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.subplot(211)\n",
    "plt.plot(loss_val_batch)\n",
    "#plt.ylim(0.7,0.703)\n",
    "plt.subplot(212)\n",
    "plt.plot(loss_train_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### setup output of run for ensembling\n",
    "run = 0\n",
    "path = '/data_sata1/ensembling/extranet_xs_ensembling'\n",
    "\n",
    "### output predictions & ground truth\n",
    "pt_fname = os.path.join(path, 'r' + str(run).zfill(2) + '-i' + str(n_epochs) + '-lr' + str(lr) + '-pt.csv')\n",
    "while os.path.isfile(pt_fname):\n",
    "    run +=1\n",
    "    pt_fname = os.path.join(path, 'r' + str(run).zfill(2) + '-i' + str(n_epochs) + '-lr' + str(lr) + '-pt.csv')\n",
    "df = pd.DataFrame({\"gt\" : gt_val_final, \"pred\" : pred_val_final})\n",
    "df.to_csv(pt_fname, index=False)\n",
    "\n",
    "### output per-iteration values\n",
    "epochs_fname = os.path.join(path, 'r' + str(run).zfill(2) + '-i' + str(n_epochs) + '-lr' + str(lr) + '-epoch.csv')\n",
    "df = pd.DataFrame({\"loss_train\":loss_train_batch, \"loss_val\":loss_val_batch, \"acc_val\":acc_val_epoch, \"ap_val\":ap_val_epoch})\n",
    "df.to_csv(epochs_fname, index=False)\n",
    "\n",
    "### save model\n",
    "model_fname = os.path.join(path, 'r' + str(run).zfill(2) + '-i' + str(n_epochs) + '-lr' + str(lr) + '-model.pth')\n",
    "torch.save(model.state_dict(), model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate average precision + precision-recall curves\n",
    "P, R, _ = precision_recall_curve(gt_val_final, pred_val_final)\n",
    "AP = average_precision_score(gt_val_final, pred_val_final, average=None)\n",
    "print(\"average precision = {0:0.4f}\".format(AP))\n",
    "  \n",
    "### convert prediction to bytes based on threshold\n",
    "thresh = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "prec_thresh, recall_thresh = np.zeros(len(thresh)), np.zeros(len(thresh))\n",
    "for n, nval in enumerate(thresh):\n",
    "    pred_byte = np.zeros(len(pred_val_final))\n",
    "    for i, val in enumerate(pred_val_final):\n",
    "        if val > nval:\n",
    "            pred_byte[i] = 1.0\n",
    "        else:\n",
    "            pred_byte[i] = 0.0\n",
    "    prec_thresh[n] = precision_score(gt_val_final, pred_byte)\n",
    "    recall_thresh[n] = recall_score(gt_val_final, pred_byte)\n",
    "    print(\"thresh = {0:0.2f}, precision = {1:0.2f}, recall = {2:0.2f}\".format(thresh[n], prec_thresh[n], recall_thresh[n]))\n",
    "    tn, fp, fn, tp = confusion_matrix(gt_val_final, pred_byte).ravel()\n",
    "    print(\"   TN = {0:0}, FP = {1:0}, FN = {2:0}, TP = {3:0}\".format(tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### plot values\n",
    "p1 = figure(plot_width=400, plot_height=300, title='Precision vs. Recall, AP={0:0.3f}'.format(AP))\n",
    "p1.step(R, P, color=\"blue\", line_width=2, mode=\"after\")\n",
    "\n",
    "p2 = figure(plot_width=400, plot_height=300, title='Loss per Epoch')\n",
    "p2.line(np.arange(len(loss_train_batch)), loss_train_batch, color=\"blue\", line_width=2)\n",
    "p2.line(np.arange(len(loss_val_batch)), loss_val_batch, color=\"orange\", line_width=1)\n",
    "show(row(p1, p2))\n",
    "\n",
    "p3 = figure(plot_width=400, plot_height=300, title='Average Precision per Epoch')\n",
    "p3.line(np.arange(len(ap_val_epoch)), ap_val_epoch, color=\"orange\", line_width=2)\n",
    "p3.scatter(np.arange(len(ap_val_epoch)), ap_val_epoch, color=\"orange\", alpha=0.5)\n",
    "\n",
    "p4 = figure(plot_width=400, plot_height=300, title='Accuracy per Epoch')\n",
    "p4.line(np.arange(len(acc_val_epoch)), acc_val_epoch, color=\"orange\", line_width=2)\n",
    "p4.scatter(np.arange(len(acc_val_epoch)), acc_val_epoch, color=\"orange\", alpha=0.5)\n",
    "show(row(p3, p4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mall\u001b[0m/  \u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/  \u001b[01;34mval\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls /home/hugh/processed_dv_101/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('processed_101bins_noUNK/')\n",
    "os.mkdir('processed_101bins_noUNK/all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob('/home/hugh/processed_dv_101/all/*info.npy'):\n",
    "    info=np.load(file,encoding='latin1')\n",
    "    if info[6]!='UNK':\n",
    "        os.system('cp --preserve=links '+file+' processed_101bins_noUNK/all')\n",
    "        os.system('cp --preserve=links '+file.replace('_info','_loc')+' processed_101bins_noUNK/all')\n",
    "        os.system('cp --preserve=links '+file.replace('_info','_glob')+' processed_101bins_noUNK/all')\n",
    "        os.system('cp --preserve=links '+file.replace('_info','_sec')+' processed_101bins_noUNK/all')\n",
    "        os.system('cp --preserve=links '+file.replace('_info','_odd_even')+' processed_101bins_noUNK/all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
